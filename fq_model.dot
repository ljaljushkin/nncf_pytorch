strict digraph  {
"0 /nncf_model_input_0";
"1 /nncf_model_input_1";
"2 embed_tokens.weight";
"3 LlamaModel/Embedding[embed_tokens]/embedding_0";
"4 LlamaModel/__getitem___0";
"5 LlamaModel/eq_0";
"6 LlamaModel/__rmul___0";
"7 LlamaModel/__ne___0";
"8 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0";
"9 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/pow_0";
"10 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/mean_0";
"11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__add___0";
"12 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0";
"14 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1";
"15 layers.0.input_layernorm.weight";
"16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1";
"17 layers.0.self_attn.q_proj.weight";
"18 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"19 layers.0.self_attn.k_proj.weight";
"20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"21 layers.0.self_attn.v_proj.weight";
"22 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"23 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_0";
"24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0";
"25 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_1";
"26 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1";
"27 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_2";
"28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2";
"29 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"30 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"31 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"32 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"33 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0";
"37 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___0";
"38 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___1";
"39 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___0";
"40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0";
"41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1";
"42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0";
"43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2";
"44 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___2";
"45 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___3";
"46 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___1";
"47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1";
"48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3";
"49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1";
"50 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___4";
"51 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_0";
"52 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_0";
"53 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___5";
"54 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_1";
"55 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_1";
"56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"57 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_3";
"58 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_0";
"59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_3";
"60 layers.0.self_attn.o_proj.weight";
"61 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"62 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0";
"63 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"64 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"65 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"67 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"68 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"70 layers.0.post_attention_layernorm.weight";
"71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"72 layers.0.mlp.gate_proj.weight";
"73 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"74 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"75 layers.0.mlp.up_proj.weight";
"76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0";
"78 layers.0.mlp.down_proj.weight";
"79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1";
"81 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0";
"82 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/pow_0";
"83 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/mean_0";
"84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__add___0";
"85 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"86 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0";
"87 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1";
"88 layers.1.input_layernorm.weight";
"89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1";
"90 layers.1.self_attn.q_proj.weight";
"91 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"92 layers.1.self_attn.k_proj.weight";
"93 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"94 layers.1.self_attn.v_proj.weight";
"95 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"96 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_0";
"97 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0";
"98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_1";
"99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1";
"100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_2";
"101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2";
"102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0";
"110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___0";
"111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___1";
"112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___0";
"113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0";
"114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1";
"115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0";
"116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2";
"117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___2";
"118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___3";
"119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___1";
"120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1";
"121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3";
"122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1";
"123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___4";
"124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_0";
"125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_0";
"126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___5";
"127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_1";
"128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_1";
"129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_3";
"131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_0";
"132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_3";
"133 layers.1.self_attn.o_proj.weight";
"134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0";
"136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"143 layers.1.post_attention_layernorm.weight";
"144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"145 layers.1.mlp.gate_proj.weight";
"146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"148 layers.1.mlp.up_proj.weight";
"149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0";
"151 layers.1.mlp.down_proj.weight";
"152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1";
"154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0";
"155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/pow_0";
"156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/mean_0";
"157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__add___0";
"158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0";
"160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1";
"161 layers.2.input_layernorm.weight";
"162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1";
"163 layers.2.self_attn.q_proj.weight";
"164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"165 layers.2.self_attn.k_proj.weight";
"166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"167 layers.2.self_attn.v_proj.weight";
"168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_0";
"170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0";
"171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_1";
"172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1";
"173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_2";
"174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2";
"175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0";
"183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___0";
"184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___1";
"185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___0";
"186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0";
"187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1";
"188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0";
"189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2";
"190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___2";
"191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___3";
"192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___1";
"193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1";
"194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3";
"195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1";
"196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___4";
"197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_0";
"198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_0";
"199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___5";
"200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_1";
"201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_1";
"202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_3";
"204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_0";
"205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_3";
"206 layers.2.self_attn.o_proj.weight";
"207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0";
"209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"216 layers.2.post_attention_layernorm.weight";
"217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"218 layers.2.mlp.gate_proj.weight";
"219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"221 layers.2.mlp.up_proj.weight";
"222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0";
"224 layers.2.mlp.down_proj.weight";
"225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1";
"227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0";
"228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/pow_0";
"229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/mean_0";
"230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__add___0";
"231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0";
"233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1";
"234 layers.3.input_layernorm.weight";
"235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1";
"236 layers.3.self_attn.q_proj.weight";
"237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"238 layers.3.self_attn.k_proj.weight";
"239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"240 layers.3.self_attn.v_proj.weight";
"241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_0";
"243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0";
"244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_1";
"245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1";
"246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_2";
"247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2";
"248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0";
"256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___0";
"257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___1";
"258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___0";
"259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0";
"260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1";
"261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0";
"262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2";
"263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___2";
"264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___3";
"265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___1";
"266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1";
"267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3";
"268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1";
"269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___4";
"270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_0";
"271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_0";
"272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___5";
"273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_1";
"274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_1";
"275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_3";
"277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_0";
"278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_3";
"279 layers.3.self_attn.o_proj.weight";
"280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0";
"282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"289 layers.3.post_attention_layernorm.weight";
"290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"291 layers.3.mlp.gate_proj.weight";
"292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"294 layers.3.mlp.up_proj.weight";
"295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0";
"297 layers.3.mlp.down_proj.weight";
"298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1";
"300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0";
"301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/pow_0";
"302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/mean_0";
"303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__add___0";
"304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0";
"306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1";
"307 layers.4.input_layernorm.weight";
"308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1";
"309 layers.4.self_attn.q_proj.weight";
"310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"311 layers.4.self_attn.k_proj.weight";
"312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"313 layers.4.self_attn.v_proj.weight";
"314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_0";
"316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0";
"317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_1";
"318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1";
"319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_2";
"320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2";
"321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0";
"329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___0";
"330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___1";
"331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___0";
"332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0";
"333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1";
"334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0";
"335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2";
"336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___2";
"337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___3";
"338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___1";
"339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1";
"340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3";
"341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1";
"342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___4";
"343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_0";
"344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_0";
"345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___5";
"346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_1";
"347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_1";
"348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_3";
"350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_0";
"351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_3";
"352 layers.4.self_attn.o_proj.weight";
"353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0";
"355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"362 layers.4.post_attention_layernorm.weight";
"363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"364 layers.4.mlp.gate_proj.weight";
"365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"367 layers.4.mlp.up_proj.weight";
"368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0";
"370 layers.4.mlp.down_proj.weight";
"371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1";
"373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0";
"374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/pow_0";
"375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/mean_0";
"376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__add___0";
"377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0";
"379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1";
"380 layers.5.input_layernorm.weight";
"381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1";
"382 layers.5.self_attn.q_proj.weight";
"383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"384 layers.5.self_attn.k_proj.weight";
"385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"386 layers.5.self_attn.v_proj.weight";
"387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_0";
"389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0";
"390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_1";
"391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1";
"392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_2";
"393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2";
"394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0";
"402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___0";
"403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___1";
"404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___0";
"405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0";
"406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1";
"407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0";
"408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2";
"409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___2";
"410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___3";
"411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___1";
"412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1";
"413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3";
"414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1";
"415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___4";
"416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_0";
"417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_0";
"418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___5";
"419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_1";
"420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_1";
"421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_3";
"423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_0";
"424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_3";
"425 layers.5.self_attn.o_proj.weight";
"426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0";
"428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"435 layers.5.post_attention_layernorm.weight";
"436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"437 layers.5.mlp.gate_proj.weight";
"438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"440 layers.5.mlp.up_proj.weight";
"441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0";
"443 layers.5.mlp.down_proj.weight";
"444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1";
"446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0";
"447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/pow_0";
"448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/mean_0";
"449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__add___0";
"450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0";
"452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1";
"453 layers.6.input_layernorm.weight";
"454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1";
"455 layers.6.self_attn.q_proj.weight";
"456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"457 layers.6.self_attn.k_proj.weight";
"458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"459 layers.6.self_attn.v_proj.weight";
"460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_0";
"462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0";
"463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_1";
"464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1";
"465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_2";
"466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2";
"467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0";
"475 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___0";
"476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___1";
"477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___0";
"478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0";
"479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1";
"480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0";
"481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2";
"482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___2";
"483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___3";
"484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___1";
"485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1";
"486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3";
"487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1";
"488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___4";
"489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_0";
"490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_0";
"491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___5";
"492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_1";
"493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_1";
"494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_3";
"496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_0";
"497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_3";
"498 layers.6.self_attn.o_proj.weight";
"499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0";
"501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"508 layers.6.post_attention_layernorm.weight";
"509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"510 layers.6.mlp.gate_proj.weight";
"511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"513 layers.6.mlp.up_proj.weight";
"514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0";
"516 layers.6.mlp.down_proj.weight";
"517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1";
"519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0";
"520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/pow_0";
"521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/mean_0";
"522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__add___0";
"523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0";
"525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1";
"526 layers.7.input_layernorm.weight";
"527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1";
"528 layers.7.self_attn.q_proj.weight";
"529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"530 layers.7.self_attn.k_proj.weight";
"531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"532 layers.7.self_attn.v_proj.weight";
"533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_0";
"535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0";
"536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_1";
"537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1";
"538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_2";
"539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2";
"540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0";
"548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___0";
"549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___1";
"550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___0";
"551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0";
"552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1";
"553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0";
"554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2";
"555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___2";
"556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___3";
"557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___1";
"558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1";
"559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3";
"560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1";
"561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___4";
"562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_0";
"563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_0";
"564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___5";
"565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_1";
"566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_1";
"567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_3";
"569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_0";
"570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_3";
"571 layers.7.self_attn.o_proj.weight";
"572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0";
"574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"581 layers.7.post_attention_layernorm.weight";
"582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"583 layers.7.mlp.gate_proj.weight";
"584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"586 layers.7.mlp.up_proj.weight";
"587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0";
"589 layers.7.mlp.down_proj.weight";
"590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1";
"592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0";
"593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/pow_0";
"594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/mean_0";
"595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__add___0";
"596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0";
"598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1";
"599 layers.8.input_layernorm.weight";
"600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1";
"601 layers.8.self_attn.q_proj.weight";
"602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"603 layers.8.self_attn.k_proj.weight";
"604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"605 layers.8.self_attn.v_proj.weight";
"606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_0";
"608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0";
"609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_1";
"610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1";
"611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_2";
"612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2";
"613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0";
"621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___0";
"622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___1";
"623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___0";
"624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0";
"625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1";
"626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0";
"627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2";
"628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___2";
"629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___3";
"630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___1";
"631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1";
"632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3";
"633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1";
"634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___4";
"635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_0";
"636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_0";
"637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___5";
"638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_1";
"639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_1";
"640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_3";
"642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_0";
"643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_3";
"644 layers.8.self_attn.o_proj.weight";
"645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0";
"647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"654 layers.8.post_attention_layernorm.weight";
"655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"656 layers.8.mlp.gate_proj.weight";
"657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"659 layers.8.mlp.up_proj.weight";
"660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0";
"662 layers.8.mlp.down_proj.weight";
"663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1";
"665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0";
"666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/pow_0";
"667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/mean_0";
"668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__add___0";
"669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"670 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0";
"671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1";
"672 layers.9.input_layernorm.weight";
"673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1";
"674 layers.9.self_attn.q_proj.weight";
"675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"676 layers.9.self_attn.k_proj.weight";
"677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"678 layers.9.self_attn.v_proj.weight";
"679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_0";
"681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0";
"682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_1";
"683 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1";
"684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_2";
"685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2";
"686 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"687 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"689 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0";
"694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___0";
"695 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___1";
"696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___0";
"697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0";
"698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1";
"699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0";
"700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2";
"701 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___2";
"702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___3";
"703 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___1";
"704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1";
"705 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3";
"706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1";
"707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___4";
"708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_0";
"709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_0";
"710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___5";
"711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_1";
"712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_1";
"713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_3";
"715 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_0";
"716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_3";
"717 layers.9.self_attn.o_proj.weight";
"718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0";
"720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"727 layers.9.post_attention_layernorm.weight";
"728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"729 layers.9.mlp.gate_proj.weight";
"730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"732 layers.9.mlp.up_proj.weight";
"733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0";
"735 layers.9.mlp.down_proj.weight";
"736 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1";
"738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0";
"739 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/pow_0";
"740 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/mean_0";
"741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__add___0";
"742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0";
"744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1";
"745 layers.10.input_layernorm.weight";
"746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1";
"747 layers.10.self_attn.q_proj.weight";
"748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"749 layers.10.self_attn.k_proj.weight";
"750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"751 layers.10.self_attn.v_proj.weight";
"752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_0";
"754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0";
"755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_1";
"756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1";
"757 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_2";
"758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2";
"759 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"765 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0";
"767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___0";
"768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___1";
"769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___0";
"770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0";
"771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1";
"772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0";
"773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2";
"774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___2";
"775 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___3";
"776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___1";
"777 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1";
"778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3";
"779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1";
"780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___4";
"781 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_0";
"782 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_0";
"783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___5";
"784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_1";
"785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_1";
"786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_3";
"788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_0";
"789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_3";
"790 layers.10.self_attn.o_proj.weight";
"791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0";
"793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"800 layers.10.post_attention_layernorm.weight";
"801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"802 layers.10.mlp.gate_proj.weight";
"803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"805 layers.10.mlp.up_proj.weight";
"806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0";
"808 layers.10.mlp.down_proj.weight";
"809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1";
"811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0";
"812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/pow_0";
"813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/mean_0";
"814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__add___0";
"815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0";
"817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1";
"818 layers.11.input_layernorm.weight";
"819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1";
"820 layers.11.self_attn.q_proj.weight";
"821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"822 layers.11.self_attn.k_proj.weight";
"823 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"824 layers.11.self_attn.v_proj.weight";
"825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_0";
"827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0";
"828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_1";
"829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1";
"830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_2";
"831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2";
"832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"833 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"835 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"838 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0";
"840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___0";
"841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___1";
"842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___0";
"843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0";
"844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1";
"845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0";
"846 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2";
"847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___2";
"848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___3";
"849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___1";
"850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1";
"851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3";
"852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1";
"853 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___4";
"854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_0";
"855 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_0";
"856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___5";
"857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_1";
"858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_1";
"859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_3";
"861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_0";
"862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_3";
"863 layers.11.self_attn.o_proj.weight";
"864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0";
"866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"873 layers.11.post_attention_layernorm.weight";
"874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"875 layers.11.mlp.gate_proj.weight";
"876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"877 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"878 layers.11.mlp.up_proj.weight";
"879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0";
"881 layers.11.mlp.down_proj.weight";
"882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1";
"884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0";
"885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/pow_0";
"886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/mean_0";
"887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__add___0";
"888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0";
"890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1";
"891 layers.12.input_layernorm.weight";
"892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1";
"893 layers.12.self_attn.q_proj.weight";
"894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"895 layers.12.self_attn.k_proj.weight";
"896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"897 layers.12.self_attn.v_proj.weight";
"898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"899 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_0";
"900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0";
"901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_1";
"902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1";
"903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_2";
"904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2";
"905 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"909 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0";
"913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___0";
"914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___1";
"915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___0";
"916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0";
"917 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1";
"918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0";
"919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2";
"920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___2";
"921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___3";
"922 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___1";
"923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1";
"924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3";
"925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1";
"926 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___4";
"927 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_0";
"928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_0";
"929 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___5";
"930 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_1";
"931 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_1";
"932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"933 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_3";
"934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_0";
"935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_3";
"936 layers.12.self_attn.o_proj.weight";
"937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0";
"939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"941 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"945 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"946 layers.12.post_attention_layernorm.weight";
"947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"948 layers.12.mlp.gate_proj.weight";
"949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"950 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"951 layers.12.mlp.up_proj.weight";
"952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0";
"954 layers.12.mlp.down_proj.weight";
"955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1";
"957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0";
"958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/pow_0";
"959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/mean_0";
"960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__add___0";
"961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0";
"963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1";
"964 layers.13.input_layernorm.weight";
"965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1";
"966 layers.13.self_attn.q_proj.weight";
"967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"968 layers.13.self_attn.k_proj.weight";
"969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"970 layers.13.self_attn.v_proj.weight";
"971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"972 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_0";
"973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0";
"974 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_1";
"975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1";
"976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_2";
"977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2";
"978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"980 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"985 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0";
"986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___0";
"987 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___1";
"988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___0";
"989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0";
"990 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1";
"991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0";
"992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2";
"993 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___2";
"994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___3";
"995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___1";
"996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1";
"997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3";
"998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1";
"999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1000 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_0";
"1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_0";
"1002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_1";
"1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_1";
"1005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_3";
"1007 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_3";
"1009 layers.13.self_attn.o_proj.weight";
"1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0";
"1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1013 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1017 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1019 layers.13.post_attention_layernorm.weight";
"1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1021 layers.13.mlp.gate_proj.weight";
"1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1024 layers.13.mlp.up_proj.weight";
"1025 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0";
"1027 layers.13.mlp.down_proj.weight";
"1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1";
"1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0";
"1031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/pow_0";
"1032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/mean_0";
"1033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__add___0";
"1034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1036 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1";
"1037 layers.14.input_layernorm.weight";
"1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1039 layers.14.self_attn.q_proj.weight";
"1040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1041 layers.14.self_attn.k_proj.weight";
"1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1043 layers.14.self_attn.v_proj.weight";
"1044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1045 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_0";
"1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0";
"1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_1";
"1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1";
"1049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_2";
"1050 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2";
"1051 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0";
"1059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1061 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___0";
"1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0";
"1063 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1";
"1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0";
"1065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2";
"1066 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1067 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___1";
"1069 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1";
"1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3";
"1071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1";
"1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_0";
"1074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_0";
"1075 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_1";
"1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_1";
"1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_3";
"1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1081 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_3";
"1082 layers.14.self_attn.o_proj.weight";
"1083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0";
"1085 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1092 layers.14.post_attention_layernorm.weight";
"1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1094 layers.14.mlp.gate_proj.weight";
"1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1097 layers.14.mlp.up_proj.weight";
"1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0";
"1100 layers.14.mlp.down_proj.weight";
"1101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1";
"1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0";
"1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/pow_0";
"1105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/mean_0";
"1106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__add___0";
"1107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1";
"1110 layers.15.input_layernorm.weight";
"1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1112 layers.15.self_attn.q_proj.weight";
"1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1114 layers.15.self_attn.k_proj.weight";
"1115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1116 layers.15.self_attn.v_proj.weight";
"1117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_0";
"1119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0";
"1120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_1";
"1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1";
"1122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_2";
"1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2";
"1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0";
"1132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___0";
"1135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0";
"1136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1";
"1137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0";
"1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2";
"1139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___1";
"1142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1";
"1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3";
"1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1";
"1145 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_0";
"1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_0";
"1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_1";
"1150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_1";
"1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_3";
"1153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_3";
"1155 layers.15.self_attn.o_proj.weight";
"1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0";
"1158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1165 layers.15.post_attention_layernorm.weight";
"1166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1167 layers.15.mlp.gate_proj.weight";
"1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1170 layers.15.mlp.up_proj.weight";
"1171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0";
"1173 layers.15.mlp.down_proj.weight";
"1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1";
"1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0";
"1177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/pow_0";
"1178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/mean_0";
"1179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__add___0";
"1180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1";
"1183 layers.16.input_layernorm.weight";
"1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1185 layers.16.self_attn.q_proj.weight";
"1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1187 layers.16.self_attn.k_proj.weight";
"1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1189 layers.16.self_attn.v_proj.weight";
"1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_0";
"1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0";
"1193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_1";
"1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1";
"1195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_2";
"1196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2";
"1197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0";
"1205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___0";
"1208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0";
"1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1";
"1210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0";
"1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2";
"1212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___1";
"1215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1";
"1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3";
"1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1";
"1218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_0";
"1220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_0";
"1221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_1";
"1223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_1";
"1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_3";
"1226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_3";
"1228 layers.16.self_attn.o_proj.weight";
"1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0";
"1231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1238 layers.16.post_attention_layernorm.weight";
"1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1240 layers.16.mlp.gate_proj.weight";
"1241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1243 layers.16.mlp.up_proj.weight";
"1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0";
"1246 layers.16.mlp.down_proj.weight";
"1247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1";
"1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0";
"1250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/pow_0";
"1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/mean_0";
"1252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__add___0";
"1253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1";
"1256 layers.17.input_layernorm.weight";
"1257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1258 layers.17.self_attn.q_proj.weight";
"1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1260 layers.17.self_attn.k_proj.weight";
"1261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1262 layers.17.self_attn.v_proj.weight";
"1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_0";
"1265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0";
"1266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_1";
"1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1";
"1268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_2";
"1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2";
"1270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0";
"1278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___0";
"1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0";
"1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1";
"1283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0";
"1284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2";
"1285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___1";
"1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1";
"1289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3";
"1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1";
"1291 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_0";
"1293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_0";
"1294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_1";
"1296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_1";
"1297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_3";
"1299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_3";
"1301 layers.17.self_attn.o_proj.weight";
"1302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0";
"1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1307 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1309 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1311 layers.17.post_attention_layernorm.weight";
"1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1313 layers.17.mlp.gate_proj.weight";
"1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1316 layers.17.mlp.up_proj.weight";
"1317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0";
"1319 layers.17.mlp.down_proj.weight";
"1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1";
"1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0";
"1323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/pow_0";
"1324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/mean_0";
"1325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__add___0";
"1326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1";
"1329 layers.18.input_layernorm.weight";
"1330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1331 layers.18.self_attn.q_proj.weight";
"1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1333 layers.18.self_attn.k_proj.weight";
"1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1335 layers.18.self_attn.v_proj.weight";
"1336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_0";
"1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0";
"1339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_1";
"1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1";
"1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_2";
"1342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2";
"1343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0";
"1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___0";
"1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0";
"1355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1";
"1356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0";
"1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2";
"1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___1";
"1361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1";
"1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3";
"1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1";
"1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_0";
"1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_0";
"1367 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_1";
"1369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_1";
"1370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_3";
"1372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_3";
"1374 layers.18.self_attn.o_proj.weight";
"1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0";
"1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1384 layers.18.post_attention_layernorm.weight";
"1385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1386 layers.18.mlp.gate_proj.weight";
"1387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1389 layers.18.mlp.up_proj.weight";
"1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0";
"1392 layers.18.mlp.down_proj.weight";
"1393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1";
"1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0";
"1396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/pow_0";
"1397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/mean_0";
"1398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__add___0";
"1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1";
"1402 layers.19.input_layernorm.weight";
"1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1404 layers.19.self_attn.q_proj.weight";
"1405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1406 layers.19.self_attn.k_proj.weight";
"1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1408 layers.19.self_attn.v_proj.weight";
"1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_0";
"1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0";
"1412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_1";
"1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1";
"1414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_2";
"1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2";
"1416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0";
"1424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___0";
"1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0";
"1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1";
"1429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0";
"1430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2";
"1431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___1";
"1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1";
"1435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3";
"1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1";
"1437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_0";
"1439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_0";
"1440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_1";
"1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_1";
"1443 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_3";
"1445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_3";
"1447 layers.19.self_attn.o_proj.weight";
"1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0";
"1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1455 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1457 layers.19.post_attention_layernorm.weight";
"1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1459 layers.19.mlp.gate_proj.weight";
"1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1462 layers.19.mlp.up_proj.weight";
"1463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0";
"1465 layers.19.mlp.down_proj.weight";
"1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1";
"1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0";
"1469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/pow_0";
"1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/mean_0";
"1471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__add___0";
"1472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1";
"1475 layers.20.input_layernorm.weight";
"1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1477 layers.20.self_attn.q_proj.weight";
"1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1479 layers.20.self_attn.k_proj.weight";
"1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1481 layers.20.self_attn.v_proj.weight";
"1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_0";
"1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0";
"1485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_1";
"1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1";
"1487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_2";
"1488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2";
"1489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0";
"1497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___0";
"1500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0";
"1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1";
"1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0";
"1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2";
"1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___1";
"1507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1";
"1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3";
"1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1";
"1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_0";
"1512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_0";
"1513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_1";
"1515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_1";
"1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_3";
"1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_3";
"1520 layers.20.self_attn.o_proj.weight";
"1521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0";
"1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1530 layers.20.post_attention_layernorm.weight";
"1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1532 layers.20.mlp.gate_proj.weight";
"1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1535 layers.20.mlp.up_proj.weight";
"1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0";
"1538 layers.20.mlp.down_proj.weight";
"1539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1";
"1541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0";
"1542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/pow_0";
"1543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/mean_0";
"1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__add___0";
"1545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1";
"1548 layers.21.input_layernorm.weight";
"1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1550 layers.21.self_attn.q_proj.weight";
"1551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1552 layers.21.self_attn.k_proj.weight";
"1553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1554 layers.21.self_attn.v_proj.weight";
"1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_0";
"1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0";
"1558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_1";
"1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1";
"1560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_2";
"1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2";
"1562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0";
"1570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___0";
"1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0";
"1574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1";
"1575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0";
"1576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2";
"1577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___1";
"1580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1";
"1581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3";
"1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1";
"1583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_0";
"1585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_0";
"1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_1";
"1588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_1";
"1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_3";
"1591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_3";
"1593 layers.21.self_attn.o_proj.weight";
"1594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0";
"1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1603 layers.21.post_attention_layernorm.weight";
"1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1605 layers.21.mlp.gate_proj.weight";
"1606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1608 layers.21.mlp.up_proj.weight";
"1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0";
"1611 layers.21.mlp.down_proj.weight";
"1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1";
"1614 LlamaModel/LlamaRMSNorm[norm]/to_0";
"1615 LlamaModel/LlamaRMSNorm[norm]/pow_0";
"1616 LlamaModel/LlamaRMSNorm[norm]/mean_0";
"1617 LlamaModel/LlamaRMSNorm[norm]/__add___0";
"1618 LlamaModel/LlamaRMSNorm[norm]/rsqrt_0";
"1619 LlamaModel/LlamaRMSNorm[norm]/__mul___0";
"1620 LlamaModel/LlamaRMSNorm[norm]/to_1";
"1621 norm.weight";
"1622 LlamaModel/LlamaRMSNorm[norm]/__mul___1";
"1623 /nncf_model_output_0";
"1624 /nncf_model_output_1";
"1625 /nncf_model_output_2";
"1626 /nncf_model_output_3";
"1627 /nncf_model_output_4";
"1628 /nncf_model_output_5";
"1629 /nncf_model_output_6";
"1630 /nncf_model_output_7";
"1631 /nncf_model_output_8";
"1632 /nncf_model_output_9";
"1633 /nncf_model_output_10";
"1634 /nncf_model_output_11";
"1635 /nncf_model_output_12";
"1636 /nncf_model_output_13";
"1637 /nncf_model_output_14";
"1638 /nncf_model_output_15";
"1639 /nncf_model_output_16";
"1640 /nncf_model_output_17";
"1641 /nncf_model_output_18";
"1642 /nncf_model_output_19";
"1643 /nncf_model_output_20";
"1644 /nncf_model_output_21";
"1645 /nncf_model_output_22";
"1646 /nncf_model_output_23";
"1647 /nncf_model_output_24";
"1648 /nncf_model_output_25";
"1649 /nncf_model_output_26";
"1650 /nncf_model_output_27";
"1651 /nncf_model_output_28";
"1652 /nncf_model_output_29";
"1653 /nncf_model_output_30";
"1654 /nncf_model_output_31";
"1655 /nncf_model_output_32";
"1656 /nncf_model_output_33";
"1657 /nncf_model_output_34";
"1658 /nncf_model_output_35";
"1659 /nncf_model_output_36";
"1660 /nncf_model_output_37";
"1661 /nncf_model_output_38";
"1662 /nncf_model_output_39";
"1663 /nncf_model_output_40";
"1664 /nncf_model_output_41";
"1665 /nncf_model_output_42";
"1666 /nncf_model_output_43";
"1667 /nncf_model_output_44";
"0 /nncf_model_input_0" -> "3 LlamaModel/Embedding[embed_tokens]/embedding_0"  [label="(1, 21) \n0 -> 0", style=dashed];
"1 /nncf_model_input_1" -> "4 LlamaModel/__getitem___0"  [label="(1, 21) \n0 -> 0", style=dashed];
"1 /nncf_model_input_1" -> "7 LlamaModel/__ne___0"  [label="(1, 21) \n0 -> 0", style=dashed];
"2 embed_tokens.weight" -> "3 LlamaModel/Embedding[embed_tokens]/embedding_0"  [label="(32000, 2048) \n0 -> 1", style=solid];
"3 LlamaModel/Embedding[embed_tokens]/embedding_0" -> "8 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"4 LlamaModel/__getitem___0" -> "5 LlamaModel/eq_0"  [label="(1, 1, 1, 21) \n0 -> 0", style=dashed];
"5 LlamaModel/eq_0" -> "6 LlamaModel/__rmul___0"  [label="(1, 1, 1, 21) \n0 -> 0", style=dashed];
"8 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0" -> "9 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"8 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0" -> "13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"8 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0" -> "62 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"9 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/pow_0" -> "10 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"10 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/mean_0" -> "11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__add___0" -> "12 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"12 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "14 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"14 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1" -> "16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"15 layers.0.input_layernorm.weight" -> "16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "18 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "22 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"17 layers.0.self_attn.q_proj.weight" -> "18 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"18 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "23 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"19 layers.0.self_attn.k_proj.weight" -> "20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "25 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"21 layers.0.self_attn.v_proj.weight" -> "22 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"22 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "27 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"23 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_0" -> "24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0" -> "36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0" -> "37 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0" -> "38 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"25 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_1" -> "26 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"26 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1" -> "43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"26 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1" -> "44 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"26 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1" -> "45 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"27 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_2" -> "28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2" -> "53 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1624 /nncf_model_output_1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"29 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "30 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"29 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "31 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"30 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "32 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"31 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "33 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"32 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"33 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0" -> "42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"37 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"38 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "39 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"39 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___0" -> "40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0" -> "41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1" -> "42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0" -> "56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2" -> "49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"44 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"45 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "46 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"46 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___1" -> "47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1" -> "48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3" -> "49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1" -> "50 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1" -> "1623 /nncf_model_output_0"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"50 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "51 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"51 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_0" -> "52 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"52 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_0" -> "56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"53 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "54 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"54 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_1" -> "55 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"55 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_1" -> "56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "57 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"57 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_3" -> "58 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"58 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_3" -> "61 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"60 layers.0.self_attn.o_proj.weight" -> "61 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"61 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "62 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"62 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0" -> "63 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"63 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "64 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"63 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "68 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"63 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"64 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "65 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"65 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "67 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"67 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "68 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"68 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"70 layers.0.post_attention_layernorm.weight" -> "71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "73 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"72 layers.0.mlp.gate_proj.weight" -> "73 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"73 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "74 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"74 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"75 layers.0.mlp.up_proj.weight" -> "76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0" -> "79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"78 layers.0.mlp.down_proj.weight" -> "79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1" -> "81 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"81 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0" -> "82 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"81 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0" -> "86 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"81 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0" -> "135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"82 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/pow_0" -> "83 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"83 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/mean_0" -> "84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__add___0" -> "85 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"85 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "86 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"86 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "87 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"87 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1" -> "89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"88 layers.1.input_layernorm.weight" -> "89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "91 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "93 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "95 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"90 layers.1.self_attn.q_proj.weight" -> "91 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"91 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "96 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"92 layers.1.self_attn.k_proj.weight" -> "93 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"93 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"94 layers.1.self_attn.v_proj.weight" -> "95 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"95 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"96 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_0" -> "97 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"97 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0" -> "109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"97 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0" -> "110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"97 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0" -> "111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_1" -> "99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1" -> "116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1" -> "117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1" -> "118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_2" -> "101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2" -> "126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1626 /nncf_model_output_3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0" -> "115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___0" -> "113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0" -> "114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1" -> "115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0" -> "129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2" -> "122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___1" -> "120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1" -> "121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3" -> "122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1" -> "123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1" -> "1625 /nncf_model_output_2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_0" -> "125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_0" -> "129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_1" -> "128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_1" -> "129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_3" -> "131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_3" -> "134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"133 layers.1.self_attn.o_proj.weight" -> "134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0" -> "136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"143 layers.1.post_attention_layernorm.weight" -> "144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"145 layers.1.mlp.gate_proj.weight" -> "146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"148 layers.1.mlp.up_proj.weight" -> "149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0" -> "152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"151 layers.1.mlp.down_proj.weight" -> "152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1" -> "154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0" -> "155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0" -> "159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0" -> "208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/pow_0" -> "156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/mean_0" -> "157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__add___0" -> "158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1" -> "162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"161 layers.2.input_layernorm.weight" -> "162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"163 layers.2.self_attn.q_proj.weight" -> "164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"165 layers.2.self_attn.k_proj.weight" -> "166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"167 layers.2.self_attn.v_proj.weight" -> "168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_0" -> "170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0" -> "182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0" -> "183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0" -> "184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_1" -> "172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1" -> "189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1" -> "190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1" -> "191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_2" -> "174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2" -> "199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1628 /nncf_model_output_5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0" -> "188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___0" -> "186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0" -> "187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1" -> "188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0" -> "202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2" -> "195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___1" -> "193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1" -> "194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3" -> "195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1" -> "196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1" -> "1627 /nncf_model_output_4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_0" -> "198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_0" -> "202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_1" -> "201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_1" -> "202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_3" -> "204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_3" -> "207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"206 layers.2.self_attn.o_proj.weight" -> "207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0" -> "209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"216 layers.2.post_attention_layernorm.weight" -> "217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"218 layers.2.mlp.gate_proj.weight" -> "219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"221 layers.2.mlp.up_proj.weight" -> "222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0" -> "225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"224 layers.2.mlp.down_proj.weight" -> "225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1" -> "227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0" -> "228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0" -> "232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0" -> "281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/pow_0" -> "229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/mean_0" -> "230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__add___0" -> "231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1" -> "235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"234 layers.3.input_layernorm.weight" -> "235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"236 layers.3.self_attn.q_proj.weight" -> "237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"238 layers.3.self_attn.k_proj.weight" -> "239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"240 layers.3.self_attn.v_proj.weight" -> "241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_0" -> "243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0" -> "255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0" -> "256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0" -> "257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_1" -> "245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1" -> "262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1" -> "263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1" -> "264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_2" -> "247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2" -> "272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1630 /nncf_model_output_7"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0" -> "261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___0" -> "259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0" -> "260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1" -> "261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0" -> "275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2" -> "268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___1" -> "266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1" -> "267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3" -> "268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1" -> "269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1" -> "1629 /nncf_model_output_6"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_0" -> "271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_0" -> "275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_1" -> "274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_1" -> "275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_3" -> "277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_3" -> "280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"279 layers.3.self_attn.o_proj.weight" -> "280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0" -> "282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"289 layers.3.post_attention_layernorm.weight" -> "290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"291 layers.3.mlp.gate_proj.weight" -> "292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"294 layers.3.mlp.up_proj.weight" -> "295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0" -> "298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"297 layers.3.mlp.down_proj.weight" -> "298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1" -> "300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0" -> "301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0" -> "305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0" -> "354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/pow_0" -> "302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/mean_0" -> "303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__add___0" -> "304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1" -> "308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"307 layers.4.input_layernorm.weight" -> "308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"309 layers.4.self_attn.q_proj.weight" -> "310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"311 layers.4.self_attn.k_proj.weight" -> "312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"313 layers.4.self_attn.v_proj.weight" -> "314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_0" -> "316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0" -> "328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0" -> "329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0" -> "330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_1" -> "318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1" -> "335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1" -> "336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1" -> "337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_2" -> "320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2" -> "345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1632 /nncf_model_output_9"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0" -> "334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___0" -> "332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0" -> "333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1" -> "334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0" -> "348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2" -> "341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___1" -> "339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1" -> "340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3" -> "341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1" -> "342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1" -> "1631 /nncf_model_output_8"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_0" -> "344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_0" -> "348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_1" -> "347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_1" -> "348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_3" -> "350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_3" -> "353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"352 layers.4.self_attn.o_proj.weight" -> "353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0" -> "355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"362 layers.4.post_attention_layernorm.weight" -> "363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"364 layers.4.mlp.gate_proj.weight" -> "365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"367 layers.4.mlp.up_proj.weight" -> "368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0" -> "371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"370 layers.4.mlp.down_proj.weight" -> "371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1" -> "373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0" -> "374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0" -> "378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0" -> "427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/pow_0" -> "375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/mean_0" -> "376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__add___0" -> "377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1" -> "381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"380 layers.5.input_layernorm.weight" -> "381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"382 layers.5.self_attn.q_proj.weight" -> "383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"384 layers.5.self_attn.k_proj.weight" -> "385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"386 layers.5.self_attn.v_proj.weight" -> "387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_0" -> "389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0" -> "401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0" -> "402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0" -> "403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_1" -> "391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1" -> "408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1" -> "409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1" -> "410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_2" -> "393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2" -> "418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1634 /nncf_model_output_11"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0" -> "407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___0" -> "405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0" -> "406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1" -> "407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0" -> "421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2" -> "414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___1" -> "412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1" -> "413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3" -> "414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1" -> "415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1" -> "1633 /nncf_model_output_10"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_0" -> "417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_0" -> "421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_1" -> "420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_1" -> "421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_3" -> "423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_3" -> "426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"425 layers.5.self_attn.o_proj.weight" -> "426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0" -> "428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"435 layers.5.post_attention_layernorm.weight" -> "436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"437 layers.5.mlp.gate_proj.weight" -> "438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"440 layers.5.mlp.up_proj.weight" -> "441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0" -> "444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"443 layers.5.mlp.down_proj.weight" -> "444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1" -> "446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0" -> "447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0" -> "451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0" -> "500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/pow_0" -> "448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/mean_0" -> "449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__add___0" -> "450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1" -> "454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"453 layers.6.input_layernorm.weight" -> "454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"455 layers.6.self_attn.q_proj.weight" -> "456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"457 layers.6.self_attn.k_proj.weight" -> "458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"459 layers.6.self_attn.v_proj.weight" -> "460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_0" -> "462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0" -> "474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0" -> "475 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0" -> "476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_1" -> "464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1" -> "481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1" -> "482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1" -> "483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_2" -> "466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2" -> "491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1636 /nncf_model_output_13"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0" -> "480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"475 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___0" -> "478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0" -> "479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1" -> "480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0" -> "494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2" -> "487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___1" -> "485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1" -> "486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3" -> "487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1" -> "488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1" -> "1635 /nncf_model_output_12"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_0" -> "490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_0" -> "494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_1" -> "493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_1" -> "494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_3" -> "496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_3" -> "499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"498 layers.6.self_attn.o_proj.weight" -> "499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0" -> "501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"508 layers.6.post_attention_layernorm.weight" -> "509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"510 layers.6.mlp.gate_proj.weight" -> "511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"513 layers.6.mlp.up_proj.weight" -> "514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0" -> "517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"516 layers.6.mlp.down_proj.weight" -> "517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1" -> "519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0" -> "520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0" -> "524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0" -> "573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/pow_0" -> "521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/mean_0" -> "522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__add___0" -> "523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1" -> "527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"526 layers.7.input_layernorm.weight" -> "527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"528 layers.7.self_attn.q_proj.weight" -> "529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"530 layers.7.self_attn.k_proj.weight" -> "531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"532 layers.7.self_attn.v_proj.weight" -> "533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_0" -> "535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0" -> "547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0" -> "548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0" -> "549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_1" -> "537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1" -> "554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1" -> "555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1" -> "556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_2" -> "539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2" -> "564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1638 /nncf_model_output_15"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0" -> "553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___0" -> "551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0" -> "552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1" -> "553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0" -> "567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2" -> "560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___1" -> "558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1" -> "559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3" -> "560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1" -> "561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1" -> "1637 /nncf_model_output_14"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_0" -> "563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_0" -> "567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_1" -> "566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_1" -> "567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_3" -> "569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_3" -> "572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"571 layers.7.self_attn.o_proj.weight" -> "572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0" -> "574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"581 layers.7.post_attention_layernorm.weight" -> "582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"583 layers.7.mlp.gate_proj.weight" -> "584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"586 layers.7.mlp.up_proj.weight" -> "587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0" -> "590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"589 layers.7.mlp.down_proj.weight" -> "590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1" -> "592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0" -> "593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0" -> "597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0" -> "646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/pow_0" -> "594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/mean_0" -> "595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__add___0" -> "596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1" -> "600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"599 layers.8.input_layernorm.weight" -> "600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"601 layers.8.self_attn.q_proj.weight" -> "602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"603 layers.8.self_attn.k_proj.weight" -> "604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"605 layers.8.self_attn.v_proj.weight" -> "606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_0" -> "608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0" -> "620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0" -> "621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0" -> "622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_1" -> "610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1" -> "627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1" -> "628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1" -> "629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_2" -> "612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2" -> "637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1640 /nncf_model_output_17"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0" -> "626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___0" -> "624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0" -> "625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1" -> "626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0" -> "640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2" -> "633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___1" -> "631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1" -> "632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3" -> "633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1" -> "634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1" -> "1639 /nncf_model_output_16"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_0" -> "636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_0" -> "640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_1" -> "639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_1" -> "640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_3" -> "642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_3" -> "645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"644 layers.8.self_attn.o_proj.weight" -> "645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0" -> "647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"654 layers.8.post_attention_layernorm.weight" -> "655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"656 layers.8.mlp.gate_proj.weight" -> "657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"659 layers.8.mlp.up_proj.weight" -> "660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0" -> "663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"662 layers.8.mlp.down_proj.weight" -> "663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1" -> "665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0" -> "666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0" -> "670 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0" -> "719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/pow_0" -> "667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/mean_0" -> "668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__add___0" -> "669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "670 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"670 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1" -> "673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"672 layers.9.input_layernorm.weight" -> "673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"674 layers.9.self_attn.q_proj.weight" -> "675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"676 layers.9.self_attn.k_proj.weight" -> "677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"678 layers.9.self_attn.v_proj.weight" -> "679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_0" -> "681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0" -> "693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0" -> "694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0" -> "695 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_1" -> "683 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"683 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1" -> "700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"683 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1" -> "701 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"683 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1" -> "702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_2" -> "685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2" -> "710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1642 /nncf_model_output_19"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"686 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "687 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"686 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"687 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "689 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"689 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "705 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0" -> "699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"695 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___0" -> "697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0" -> "698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1" -> "699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0" -> "713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2" -> "706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"701 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "703 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"703 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___1" -> "704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1" -> "705 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"705 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3" -> "706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1" -> "707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1" -> "1641 /nncf_model_output_18"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_0" -> "709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_0" -> "713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_1" -> "712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_1" -> "713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_3" -> "715 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"715 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_3" -> "718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"717 layers.9.self_attn.o_proj.weight" -> "718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0" -> "720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"727 layers.9.post_attention_layernorm.weight" -> "728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"729 layers.9.mlp.gate_proj.weight" -> "730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"732 layers.9.mlp.up_proj.weight" -> "733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0" -> "736 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"735 layers.9.mlp.down_proj.weight" -> "736 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"736 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1" -> "738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0" -> "739 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0" -> "743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0" -> "792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"739 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/pow_0" -> "740 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"740 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/mean_0" -> "741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__add___0" -> "742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1" -> "746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"745 layers.10.input_layernorm.weight" -> "746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"747 layers.10.self_attn.q_proj.weight" -> "748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"749 layers.10.self_attn.k_proj.weight" -> "750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"751 layers.10.self_attn.v_proj.weight" -> "752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "757 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_0" -> "754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0" -> "766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0" -> "767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0" -> "768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_1" -> "756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1" -> "773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1" -> "774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1" -> "775 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"757 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_2" -> "758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2" -> "783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1644 /nncf_model_output_21"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"759 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"759 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "765 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"765 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"765 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0" -> "772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___0" -> "770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0" -> "771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1" -> "772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0" -> "786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2" -> "779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "777 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"775 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___1" -> "777 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"777 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1" -> "778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3" -> "779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1" -> "780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1" -> "1643 /nncf_model_output_20"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "781 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"781 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_0" -> "782 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"782 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_0" -> "786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_1" -> "785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_1" -> "786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_3" -> "788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_3" -> "791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"790 layers.10.self_attn.o_proj.weight" -> "791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0" -> "793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"800 layers.10.post_attention_layernorm.weight" -> "801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"802 layers.10.mlp.gate_proj.weight" -> "803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"805 layers.10.mlp.up_proj.weight" -> "806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0" -> "809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"808 layers.10.mlp.down_proj.weight" -> "809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1" -> "811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0" -> "812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0" -> "816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0" -> "865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/pow_0" -> "813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/mean_0" -> "814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__add___0" -> "815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1" -> "819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"818 layers.11.input_layernorm.weight" -> "819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "823 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"820 layers.11.self_attn.q_proj.weight" -> "821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"822 layers.11.self_attn.k_proj.weight" -> "823 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"823 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"824 layers.11.self_attn.v_proj.weight" -> "825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_0" -> "827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0" -> "839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0" -> "840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0" -> "841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_1" -> "829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1" -> "846 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1" -> "847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1" -> "848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_2" -> "831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2" -> "856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1646 /nncf_model_output_23"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "833 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"833 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "835 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"835 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "838 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "846 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"838 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"838 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0" -> "845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___0" -> "843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0" -> "844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1" -> "845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0" -> "859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"846 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2" -> "852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___1" -> "850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1" -> "851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3" -> "852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1" -> "853 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1" -> "1645 /nncf_model_output_22"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"853 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_0" -> "855 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"855 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_0" -> "859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_1" -> "858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_1" -> "859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_3" -> "861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_3" -> "864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"863 layers.11.self_attn.o_proj.weight" -> "864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0" -> "866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"873 layers.11.post_attention_layernorm.weight" -> "874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"875 layers.11.mlp.gate_proj.weight" -> "876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "877 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"877 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"878 layers.11.mlp.up_proj.weight" -> "879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0" -> "882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"881 layers.11.mlp.down_proj.weight" -> "882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1" -> "884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0" -> "885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0" -> "889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0" -> "938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/pow_0" -> "886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/mean_0" -> "887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__add___0" -> "888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1" -> "892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"891 layers.12.input_layernorm.weight" -> "892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"893 layers.12.self_attn.q_proj.weight" -> "894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "899 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"895 layers.12.self_attn.k_proj.weight" -> "896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"897 layers.12.self_attn.v_proj.weight" -> "898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"899 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_0" -> "900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0" -> "912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0" -> "913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0" -> "914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_1" -> "902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1" -> "919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1" -> "920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1" -> "921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_2" -> "904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2" -> "929 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1648 /nncf_model_output_25"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"905 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"905 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "909 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"909 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "917 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0" -> "918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___0" -> "916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0" -> "917 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"917 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1" -> "918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0" -> "932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2" -> "925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "922 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"922 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___1" -> "923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1" -> "924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3" -> "925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1" -> "926 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1" -> "1647 /nncf_model_output_24"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"926 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "927 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"927 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_0" -> "928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_0" -> "932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"929 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "930 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"930 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_1" -> "931 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"931 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_1" -> "932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "933 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"933 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_3" -> "934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_3" -> "937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"936 layers.12.self_attn.o_proj.weight" -> "937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0" -> "939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "941 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"941 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "945 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"945 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"946 layers.12.post_attention_layernorm.weight" -> "947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"948 layers.12.mlp.gate_proj.weight" -> "949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "950 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"950 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"951 layers.12.mlp.up_proj.weight" -> "952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0" -> "955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"954 layers.12.mlp.down_proj.weight" -> "955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1" -> "957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0" -> "958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0" -> "962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0" -> "1011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/pow_0" -> "959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/mean_0" -> "960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__add___0" -> "961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1" -> "965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"964 layers.13.input_layernorm.weight" -> "965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"966 layers.13.self_attn.q_proj.weight" -> "967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "972 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"968 layers.13.self_attn.k_proj.weight" -> "969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "974 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"970 layers.13.self_attn.v_proj.weight" -> "971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"972 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_0" -> "973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0" -> "985 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0" -> "986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0" -> "987 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"974 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_1" -> "975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1" -> "992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1" -> "993 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1" -> "994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_2" -> "977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1650 /nncf_model_output_27"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "980 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"980 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "985 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "990 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"985 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0" -> "991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"987 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___0" -> "989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0" -> "990 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"990 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1" -> "991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0" -> "1005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2" -> "998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"993 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___1" -> "996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1" -> "997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3" -> "998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1" -> "999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1" -> "1649 /nncf_model_output_26"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1000 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1000 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_0" -> "1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_1" -> "1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1007 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1007 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_3" -> "1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1009 layers.13.self_attn.o_proj.weight" -> "1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0" -> "1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1013 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1017 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1013 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1017 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1017 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1019 layers.13.post_attention_layernorm.weight" -> "1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1025 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1021 layers.13.mlp.gate_proj.weight" -> "1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1024 layers.13.mlp.up_proj.weight" -> "1025 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1025 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0" -> "1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1027 layers.13.mlp.down_proj.weight" -> "1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1" -> "1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0" -> "1031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0" -> "1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0" -> "1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1036 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1036 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1" -> "1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1037 layers.14.input_layernorm.weight" -> "1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1039 layers.14.self_attn.q_proj.weight" -> "1040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1045 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1041 layers.14.self_attn.k_proj.weight" -> "1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1043 layers.14.self_attn.v_proj.weight" -> "1044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1045 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_0" -> "1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_1" -> "1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1066 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1067 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_2" -> "1050 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1050 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1075 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1050 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1652 /nncf_model_output_29"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1051 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1051 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1063 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1061 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1061 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0" -> "1063 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1063 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0" -> "1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1066 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1069 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1067 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1069 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1069 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1" -> "1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1" -> "1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1" -> "1651 /nncf_model_output_28"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_0" -> "1074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1075 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_1" -> "1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1081 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1081 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_3" -> "1083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1082 layers.14.self_attn.o_proj.weight" -> "1083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0" -> "1085 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1085 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1085 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1085 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1092 layers.14.post_attention_layernorm.weight" -> "1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1094 layers.14.mlp.gate_proj.weight" -> "1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1097 layers.14.mlp.up_proj.weight" -> "1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0" -> "1101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1100 layers.14.mlp.down_proj.weight" -> "1101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1" -> "1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0" -> "1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0" -> "1108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0" -> "1157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1" -> "1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1110 layers.15.input_layernorm.weight" -> "1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1112 layers.15.self_attn.q_proj.weight" -> "1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1114 layers.15.self_attn.k_proj.weight" -> "1115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1116 layers.15.self_attn.v_proj.weight" -> "1117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_0" -> "1119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_1" -> "1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_2" -> "1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1654 /nncf_model_output_31"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0" -> "1136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0" -> "1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1" -> "1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1" -> "1145 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1" -> "1653 /nncf_model_output_30"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1145 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_0" -> "1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_1" -> "1150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_3" -> "1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1155 layers.15.self_attn.o_proj.weight" -> "1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0" -> "1158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1165 layers.15.post_attention_layernorm.weight" -> "1166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1167 layers.15.mlp.gate_proj.weight" -> "1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1170 layers.15.mlp.up_proj.weight" -> "1171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0" -> "1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1173 layers.15.mlp.down_proj.weight" -> "1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1" -> "1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0" -> "1177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0" -> "1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0" -> "1230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1" -> "1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1183 layers.16.input_layernorm.weight" -> "1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1185 layers.16.self_attn.q_proj.weight" -> "1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1187 layers.16.self_attn.k_proj.weight" -> "1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1189 layers.16.self_attn.v_proj.weight" -> "1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_0" -> "1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_1" -> "1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_2" -> "1196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1656 /nncf_model_output_33"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0" -> "1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0" -> "1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1" -> "1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1" -> "1218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1" -> "1655 /nncf_model_output_32"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_0" -> "1220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_1" -> "1223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_3" -> "1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1228 layers.16.self_attn.o_proj.weight" -> "1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0" -> "1231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1238 layers.16.post_attention_layernorm.weight" -> "1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1240 layers.16.mlp.gate_proj.weight" -> "1241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1243 layers.16.mlp.up_proj.weight" -> "1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0" -> "1247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1246 layers.16.mlp.down_proj.weight" -> "1247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1" -> "1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0" -> "1250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0" -> "1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0" -> "1303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1" -> "1257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1256 layers.17.input_layernorm.weight" -> "1257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1258 layers.17.self_attn.q_proj.weight" -> "1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1260 layers.17.self_attn.k_proj.weight" -> "1261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1262 layers.17.self_attn.v_proj.weight" -> "1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_0" -> "1265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_1" -> "1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_2" -> "1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1658 /nncf_model_output_35"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0" -> "1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0" -> "1297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1" -> "1289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1" -> "1291 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1" -> "1657 /nncf_model_output_34"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1291 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_0" -> "1293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_1" -> "1296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_3" -> "1302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1301 layers.17.self_attn.o_proj.weight" -> "1302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0" -> "1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1309 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1307 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1307 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1309 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1309 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1311 layers.17.post_attention_layernorm.weight" -> "1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1313 layers.17.mlp.gate_proj.weight" -> "1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1316 layers.17.mlp.up_proj.weight" -> "1317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0" -> "1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1319 layers.17.mlp.down_proj.weight" -> "1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1" -> "1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0" -> "1323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0" -> "1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0" -> "1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1" -> "1330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1329 layers.18.input_layernorm.weight" -> "1330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1331 layers.18.self_attn.q_proj.weight" -> "1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1333 layers.18.self_attn.k_proj.weight" -> "1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1335 layers.18.self_attn.v_proj.weight" -> "1336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_0" -> "1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_1" -> "1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_2" -> "1342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1367 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1660 /nncf_model_output_37"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0" -> "1355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0" -> "1370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1" -> "1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1" -> "1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1" -> "1659 /nncf_model_output_36"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_0" -> "1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1367 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_1" -> "1369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_3" -> "1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1374 layers.18.self_attn.o_proj.weight" -> "1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0" -> "1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1384 layers.18.post_attention_layernorm.weight" -> "1385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1386 layers.18.mlp.gate_proj.weight" -> "1387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1389 layers.18.mlp.up_proj.weight" -> "1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0" -> "1393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1392 layers.18.mlp.down_proj.weight" -> "1393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1" -> "1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0" -> "1396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0" -> "1400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0" -> "1449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1" -> "1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1402 layers.19.input_layernorm.weight" -> "1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1404 layers.19.self_attn.q_proj.weight" -> "1405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1406 layers.19.self_attn.k_proj.weight" -> "1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1408 layers.19.self_attn.v_proj.weight" -> "1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_0" -> "1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_1" -> "1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_2" -> "1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1662 /nncf_model_output_39"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0" -> "1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0" -> "1443 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1" -> "1435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1" -> "1437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1" -> "1661 /nncf_model_output_38"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_0" -> "1439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1443 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_1" -> "1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1443 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1443 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_3" -> "1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1447 layers.19.self_attn.o_proj.weight" -> "1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0" -> "1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1455 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1455 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1455 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1457 layers.19.post_attention_layernorm.weight" -> "1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1459 layers.19.mlp.gate_proj.weight" -> "1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1462 layers.19.mlp.up_proj.weight" -> "1463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0" -> "1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1465 layers.19.mlp.down_proj.weight" -> "1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1" -> "1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0" -> "1469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0" -> "1473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0" -> "1522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1" -> "1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1475 layers.20.input_layernorm.weight" -> "1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1477 layers.20.self_attn.q_proj.weight" -> "1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1479 layers.20.self_attn.k_proj.weight" -> "1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1481 layers.20.self_attn.v_proj.weight" -> "1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_0" -> "1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_1" -> "1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_2" -> "1488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1664 /nncf_model_output_41"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0" -> "1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0" -> "1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1" -> "1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1" -> "1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1" -> "1663 /nncf_model_output_40"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_0" -> "1512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_1" -> "1515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_3" -> "1521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1520 layers.20.self_attn.o_proj.weight" -> "1521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0" -> "1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1530 layers.20.post_attention_layernorm.weight" -> "1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1532 layers.20.mlp.gate_proj.weight" -> "1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1535 layers.20.mlp.up_proj.weight" -> "1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0" -> "1539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1538 layers.20.mlp.down_proj.weight" -> "1539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1" -> "1541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0" -> "1542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0" -> "1546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0" -> "1595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1" -> "1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1548 layers.21.input_layernorm.weight" -> "1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1550 layers.21.self_attn.q_proj.weight" -> "1551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1552 layers.21.self_attn.k_proj.weight" -> "1553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1554 layers.21.self_attn.v_proj.weight" -> "1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_0" -> "1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_1" -> "1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_2" -> "1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1666 /nncf_model_output_43"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0" -> "1574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0" -> "1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1" -> "1581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1" -> "1583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1" -> "1665 /nncf_model_output_42"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_0" -> "1585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_1" -> "1588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_3" -> "1594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1593 layers.21.self_attn.o_proj.weight" -> "1594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0" -> "1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1603 layers.21.post_attention_layernorm.weight" -> "1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1605 layers.21.mlp.gate_proj.weight" -> "1606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1608 layers.21.mlp.up_proj.weight" -> "1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0" -> "1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1611 layers.21.mlp.down_proj.weight" -> "1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1" -> "1614 LlamaModel/LlamaRMSNorm[norm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1614 LlamaModel/LlamaRMSNorm[norm]/to_0" -> "1615 LlamaModel/LlamaRMSNorm[norm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1614 LlamaModel/LlamaRMSNorm[norm]/to_0" -> "1619 LlamaModel/LlamaRMSNorm[norm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1615 LlamaModel/LlamaRMSNorm[norm]/pow_0" -> "1616 LlamaModel/LlamaRMSNorm[norm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1616 LlamaModel/LlamaRMSNorm[norm]/mean_0" -> "1617 LlamaModel/LlamaRMSNorm[norm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1617 LlamaModel/LlamaRMSNorm[norm]/__add___0" -> "1618 LlamaModel/LlamaRMSNorm[norm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1618 LlamaModel/LlamaRMSNorm[norm]/rsqrt_0" -> "1619 LlamaModel/LlamaRMSNorm[norm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1619 LlamaModel/LlamaRMSNorm[norm]/__mul___0" -> "1620 LlamaModel/LlamaRMSNorm[norm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1620 LlamaModel/LlamaRMSNorm[norm]/to_1" -> "1622 LlamaModel/LlamaRMSNorm[norm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1621 norm.weight" -> "1622 LlamaModel/LlamaRMSNorm[norm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1622 LlamaModel/LlamaRMSNorm[norm]/__mul___1" -> "1667 /nncf_model_output_44"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
}
