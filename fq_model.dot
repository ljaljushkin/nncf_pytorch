strict digraph  {
"0 /nncf_model_input_0";
"1 /nncf_model_input_1";
"2 embed_tokens.weight";
"3 LlamaModel/Embedding[embed_tokens]/embedding_0";
"4 LlamaModel/__getitem___0";
"5 LlamaModel/eq_0";
"6 LlamaModel/__rmul___0";
"7 LlamaModel/__ne___0";
"8 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0";
"9 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/pow_0";
"10 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/mean_0";
"11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__add___0";
"12 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0";
"14 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1";
"15 layers.0.input_layernorm.weight";
"16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1";
"17 layers.0.self_attn.q_proj.weight";
"18 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"19 layers.0.self_attn.k_proj.weight";
"20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"21 layers.0.self_attn.v_proj.weight";
"22 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"23 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_0";
"24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0";
"25 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_1";
"26 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1";
"27 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_2";
"28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2";
"29 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"30 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"31 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"32 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"33 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0";
"37 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___0";
"38 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___1";
"39 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___0";
"40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0";
"41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1";
"42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0";
"43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2";
"44 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___2";
"45 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___3";
"46 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___1";
"47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1";
"48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3";
"49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1";
"50 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___4";
"51 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_0";
"52 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_0";
"53 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___5";
"54 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_1";
"55 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_1";
"56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_0";
"57 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_1";
"58 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_2";
"59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"60 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_3";
"61 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_3";
"62 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_3";
"63 layers.0.self_attn.o_proj.weight";
"64 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"65 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0";
"66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"67 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"68 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"70 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"72 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"73 layers.0.post_attention_layernorm.weight";
"74 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"75 layers.0.mlp.gate_proj.weight";
"76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"78 layers.0.mlp.up_proj.weight";
"79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0";
"81 layers.0.mlp.down_proj.weight";
"82 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"83 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1";
"84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0";
"85 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/pow_0";
"86 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/mean_0";
"87 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__add___0";
"88 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0";
"90 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1";
"91 layers.1.input_layernorm.weight";
"92 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1";
"93 layers.1.self_attn.q_proj.weight";
"94 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"95 layers.1.self_attn.k_proj.weight";
"96 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"97 layers.1.self_attn.v_proj.weight";
"98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_0";
"100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0";
"101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_1";
"102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1";
"103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_2";
"104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2";
"105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0";
"113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___0";
"114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___1";
"115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___0";
"116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0";
"117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1";
"118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0";
"119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2";
"120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___2";
"121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___3";
"122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___1";
"123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1";
"124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3";
"125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1";
"126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___4";
"127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_0";
"128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_0";
"129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___5";
"130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_1";
"131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_1";
"132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_0";
"133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_1";
"134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_2";
"135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_3";
"137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_3";
"138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_3";
"139 layers.1.self_attn.o_proj.weight";
"140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0";
"142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"145 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"149 layers.1.post_attention_layernorm.weight";
"150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"151 layers.1.mlp.gate_proj.weight";
"152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"154 layers.1.mlp.up_proj.weight";
"155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0";
"157 layers.1.mlp.down_proj.weight";
"158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1";
"160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0";
"161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/pow_0";
"162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/mean_0";
"163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__add___0";
"164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0";
"166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1";
"167 layers.2.input_layernorm.weight";
"168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1";
"169 layers.2.self_attn.q_proj.weight";
"170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"171 layers.2.self_attn.k_proj.weight";
"172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"173 layers.2.self_attn.v_proj.weight";
"174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_0";
"176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0";
"177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_1";
"178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1";
"179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_2";
"180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2";
"181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0";
"189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___0";
"190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___1";
"191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___0";
"192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0";
"193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1";
"194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0";
"195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2";
"196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___2";
"197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___3";
"198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___1";
"199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1";
"200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3";
"201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1";
"202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___4";
"203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_0";
"204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_0";
"205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___5";
"206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_1";
"207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_1";
"208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_0";
"209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_1";
"210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_2";
"211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_3";
"213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_3";
"214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_3";
"215 layers.2.self_attn.o_proj.weight";
"216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0";
"218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"225 layers.2.post_attention_layernorm.weight";
"226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"227 layers.2.mlp.gate_proj.weight";
"228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"230 layers.2.mlp.up_proj.weight";
"231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0";
"233 layers.2.mlp.down_proj.weight";
"234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1";
"236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0";
"237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/pow_0";
"238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/mean_0";
"239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__add___0";
"240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0";
"242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1";
"243 layers.3.input_layernorm.weight";
"244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1";
"245 layers.3.self_attn.q_proj.weight";
"246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"247 layers.3.self_attn.k_proj.weight";
"248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"249 layers.3.self_attn.v_proj.weight";
"250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_0";
"252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0";
"253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_1";
"254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1";
"255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_2";
"256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2";
"257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0";
"265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___0";
"266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___1";
"267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___0";
"268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0";
"269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1";
"270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0";
"271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2";
"272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___2";
"273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___3";
"274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___1";
"275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1";
"276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3";
"277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1";
"278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___4";
"279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_0";
"280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_0";
"281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___5";
"282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_1";
"283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_1";
"284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_0";
"285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_1";
"286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_2";
"287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_3";
"289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_3";
"290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_3";
"291 layers.3.self_attn.o_proj.weight";
"292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0";
"294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"301 layers.3.post_attention_layernorm.weight";
"302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"303 layers.3.mlp.gate_proj.weight";
"304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"306 layers.3.mlp.up_proj.weight";
"307 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0";
"309 layers.3.mlp.down_proj.weight";
"310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"311 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1";
"312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0";
"313 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/pow_0";
"314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/mean_0";
"315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__add___0";
"316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0";
"318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1";
"319 layers.4.input_layernorm.weight";
"320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1";
"321 layers.4.self_attn.q_proj.weight";
"322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"323 layers.4.self_attn.k_proj.weight";
"324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"325 layers.4.self_attn.v_proj.weight";
"326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_0";
"328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0";
"329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_1";
"330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1";
"331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_2";
"332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2";
"333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0";
"341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___0";
"342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___1";
"343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___0";
"344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0";
"345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1";
"346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0";
"347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2";
"348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___2";
"349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___3";
"350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___1";
"351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1";
"352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3";
"353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1";
"354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___4";
"355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_0";
"356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_0";
"357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___5";
"358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_1";
"359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_1";
"360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_0";
"361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_1";
"362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_2";
"363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_3";
"365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_3";
"366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_3";
"367 layers.4.self_attn.o_proj.weight";
"368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0";
"370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"377 layers.4.post_attention_layernorm.weight";
"378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"379 layers.4.mlp.gate_proj.weight";
"380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"382 layers.4.mlp.up_proj.weight";
"383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0";
"385 layers.4.mlp.down_proj.weight";
"386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1";
"388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0";
"389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/pow_0";
"390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/mean_0";
"391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__add___0";
"392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0";
"394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1";
"395 layers.5.input_layernorm.weight";
"396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1";
"397 layers.5.self_attn.q_proj.weight";
"398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"399 layers.5.self_attn.k_proj.weight";
"400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"401 layers.5.self_attn.v_proj.weight";
"402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_0";
"404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0";
"405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_1";
"406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1";
"407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_2";
"408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2";
"409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0";
"417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___0";
"418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___1";
"419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___0";
"420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0";
"421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1";
"422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0";
"423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2";
"424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___2";
"425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___3";
"426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___1";
"427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1";
"428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3";
"429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1";
"430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___4";
"431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_0";
"432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_0";
"433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___5";
"434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_1";
"435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_1";
"436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_0";
"437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_1";
"438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_2";
"439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_3";
"441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_3";
"442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_3";
"443 layers.5.self_attn.o_proj.weight";
"444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0";
"446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"453 layers.5.post_attention_layernorm.weight";
"454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"455 layers.5.mlp.gate_proj.weight";
"456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"458 layers.5.mlp.up_proj.weight";
"459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0";
"461 layers.5.mlp.down_proj.weight";
"462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1";
"464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0";
"465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/pow_0";
"466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/mean_0";
"467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__add___0";
"468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0";
"470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1";
"471 layers.6.input_layernorm.weight";
"472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1";
"473 layers.6.self_attn.q_proj.weight";
"474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"475 layers.6.self_attn.k_proj.weight";
"476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"477 layers.6.self_attn.v_proj.weight";
"478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_0";
"480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0";
"481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_1";
"482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1";
"483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_2";
"484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2";
"485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0";
"493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___0";
"494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___1";
"495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___0";
"496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0";
"497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1";
"498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0";
"499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2";
"500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___2";
"501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___3";
"502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___1";
"503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1";
"504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3";
"505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1";
"506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___4";
"507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_0";
"508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_0";
"509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___5";
"510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_1";
"511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_1";
"512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_0";
"513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_1";
"514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_2";
"515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_3";
"517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_3";
"518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_3";
"519 layers.6.self_attn.o_proj.weight";
"520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0";
"522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"529 layers.6.post_attention_layernorm.weight";
"530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"531 layers.6.mlp.gate_proj.weight";
"532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"534 layers.6.mlp.up_proj.weight";
"535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0";
"537 layers.6.mlp.down_proj.weight";
"538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1";
"540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0";
"541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/pow_0";
"542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/mean_0";
"543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__add___0";
"544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0";
"546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1";
"547 layers.7.input_layernorm.weight";
"548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1";
"549 layers.7.self_attn.q_proj.weight";
"550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"551 layers.7.self_attn.k_proj.weight";
"552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"553 layers.7.self_attn.v_proj.weight";
"554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_0";
"556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0";
"557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_1";
"558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1";
"559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_2";
"560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2";
"561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0";
"569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___0";
"570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___1";
"571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___0";
"572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0";
"573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1";
"574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0";
"575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2";
"576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___2";
"577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___3";
"578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___1";
"579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1";
"580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3";
"581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1";
"582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___4";
"583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_0";
"584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_0";
"585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___5";
"586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_1";
"587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_1";
"588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_0";
"589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_1";
"590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_2";
"591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_3";
"593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_3";
"594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_3";
"595 layers.7.self_attn.o_proj.weight";
"596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0";
"598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"605 layers.7.post_attention_layernorm.weight";
"606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"607 layers.7.mlp.gate_proj.weight";
"608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"610 layers.7.mlp.up_proj.weight";
"611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0";
"613 layers.7.mlp.down_proj.weight";
"614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1";
"616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0";
"617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/pow_0";
"618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/mean_0";
"619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__add___0";
"620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0";
"622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1";
"623 layers.8.input_layernorm.weight";
"624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1";
"625 layers.8.self_attn.q_proj.weight";
"626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"627 layers.8.self_attn.k_proj.weight";
"628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"629 layers.8.self_attn.v_proj.weight";
"630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_0";
"632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0";
"633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_1";
"634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1";
"635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_2";
"636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2";
"637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0";
"645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___0";
"646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___1";
"647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___0";
"648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0";
"649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1";
"650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0";
"651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2";
"652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___2";
"653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___3";
"654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___1";
"655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1";
"656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3";
"657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1";
"658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___4";
"659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_0";
"660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_0";
"661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___5";
"662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_1";
"663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_1";
"664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_0";
"665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_1";
"666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_2";
"667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_3";
"669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_3";
"670 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_3";
"671 layers.8.self_attn.o_proj.weight";
"672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0";
"674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"678 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"681 layers.8.post_attention_layernorm.weight";
"682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"683 layers.8.mlp.gate_proj.weight";
"684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"686 layers.8.mlp.up_proj.weight";
"687 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0";
"689 layers.8.mlp.down_proj.weight";
"690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1";
"692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0";
"693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/pow_0";
"694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/mean_0";
"695 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__add___0";
"696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0";
"698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1";
"699 layers.9.input_layernorm.weight";
"700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1";
"701 layers.9.self_attn.q_proj.weight";
"702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"703 layers.9.self_attn.k_proj.weight";
"704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"705 layers.9.self_attn.v_proj.weight";
"706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_0";
"708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0";
"709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_1";
"710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1";
"711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_2";
"712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2";
"713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"715 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0";
"721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___0";
"722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___1";
"723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___0";
"724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0";
"725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1";
"726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0";
"727 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2";
"728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___2";
"729 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___3";
"730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___1";
"731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1";
"732 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3";
"733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1";
"734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___4";
"735 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_0";
"736 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_0";
"737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___5";
"738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_1";
"739 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_1";
"740 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_0";
"741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_1";
"742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_2";
"743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_3";
"745 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_3";
"746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_3";
"747 layers.9.self_attn.o_proj.weight";
"748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0";
"750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"751 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"757 layers.9.post_attention_layernorm.weight";
"758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"759 layers.9.mlp.gate_proj.weight";
"760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"762 layers.9.mlp.up_proj.weight";
"763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0";
"765 layers.9.mlp.down_proj.weight";
"766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1";
"768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0";
"769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/pow_0";
"770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/mean_0";
"771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__add___0";
"772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0";
"774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1";
"775 layers.10.input_layernorm.weight";
"776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1";
"777 layers.10.self_attn.q_proj.weight";
"778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"779 layers.10.self_attn.k_proj.weight";
"780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"781 layers.10.self_attn.v_proj.weight";
"782 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_0";
"784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0";
"785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_1";
"786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1";
"787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_2";
"788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2";
"789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"790 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0";
"797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___0";
"798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___1";
"799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___0";
"800 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0";
"801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1";
"802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0";
"803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2";
"804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___2";
"805 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___3";
"806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___1";
"807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1";
"808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3";
"809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1";
"810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___4";
"811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_0";
"812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_0";
"813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___5";
"814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_1";
"815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_1";
"816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_0";
"817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_1";
"818 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_2";
"819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"820 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_3";
"821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_3";
"822 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_3";
"823 layers.10.self_attn.o_proj.weight";
"824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0";
"826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"833 layers.10.post_attention_layernorm.weight";
"834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"835 layers.10.mlp.gate_proj.weight";
"836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"838 layers.10.mlp.up_proj.weight";
"839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0";
"841 layers.10.mlp.down_proj.weight";
"842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1";
"844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0";
"845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/pow_0";
"846 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/mean_0";
"847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__add___0";
"848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0";
"850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1";
"851 layers.11.input_layernorm.weight";
"852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1";
"853 layers.11.self_attn.q_proj.weight";
"854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"855 layers.11.self_attn.k_proj.weight";
"856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"857 layers.11.self_attn.v_proj.weight";
"858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_0";
"860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0";
"861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_1";
"862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1";
"863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_2";
"864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2";
"865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0";
"873 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___0";
"874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___1";
"875 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___0";
"876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0";
"877 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1";
"878 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0";
"879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2";
"880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___2";
"881 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___3";
"882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___1";
"883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1";
"884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3";
"885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1";
"886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___4";
"887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_0";
"888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_0";
"889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___5";
"890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_1";
"891 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_1";
"892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_0";
"893 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_1";
"894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_2";
"895 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_3";
"897 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_3";
"898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_3";
"899 layers.11.self_attn.o_proj.weight";
"900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0";
"902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"905 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"909 layers.11.post_attention_layernorm.weight";
"910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"911 layers.11.mlp.gate_proj.weight";
"912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"914 layers.11.mlp.up_proj.weight";
"915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0";
"917 layers.11.mlp.down_proj.weight";
"918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1";
"920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0";
"921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/pow_0";
"922 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/mean_0";
"923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__add___0";
"924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0";
"926 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1";
"927 layers.12.input_layernorm.weight";
"928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1";
"929 layers.12.self_attn.q_proj.weight";
"930 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"931 layers.12.self_attn.k_proj.weight";
"932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"933 layers.12.self_attn.v_proj.weight";
"934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_0";
"936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0";
"937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_1";
"938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1";
"939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_2";
"940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2";
"941 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"945 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"948 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0";
"949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___0";
"950 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___1";
"951 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___0";
"952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0";
"953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1";
"954 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0";
"955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2";
"956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___2";
"957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___3";
"958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___1";
"959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1";
"960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3";
"961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1";
"962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___4";
"963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_0";
"964 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_0";
"965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___5";
"966 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_1";
"967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_1";
"968 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_0";
"969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_1";
"970 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_2";
"971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"972 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_3";
"973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_3";
"974 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_3";
"975 layers.12.self_attn.o_proj.weight";
"976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0";
"978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"980 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"985 layers.12.post_attention_layernorm.weight";
"986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"987 layers.12.mlp.gate_proj.weight";
"988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"990 layers.12.mlp.up_proj.weight";
"991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0";
"993 layers.12.mlp.down_proj.weight";
"994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1";
"996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0";
"997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/pow_0";
"998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/mean_0";
"999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__add___0";
"1000 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1";
"1003 layers.13.input_layernorm.weight";
"1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1005 layers.13.self_attn.q_proj.weight";
"1006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1007 layers.13.self_attn.k_proj.weight";
"1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1009 layers.13.self_attn.v_proj.weight";
"1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_0";
"1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0";
"1013 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_1";
"1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1";
"1015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_2";
"1016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2";
"1017 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1021 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1024 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0";
"1025 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___0";
"1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0";
"1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1";
"1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0";
"1031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2";
"1032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___1";
"1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1";
"1036 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3";
"1037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1";
"1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_0";
"1040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_0";
"1041 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_1";
"1043 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_1";
"1044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1045 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_3";
"1049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1050 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_3";
"1051 layers.13.self_attn.o_proj.weight";
"1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0";
"1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1061 layers.13.post_attention_layernorm.weight";
"1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1063 layers.13.mlp.gate_proj.weight";
"1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1066 layers.13.mlp.up_proj.weight";
"1067 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0";
"1069 layers.13.mlp.down_proj.weight";
"1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1";
"1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0";
"1073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/pow_0";
"1074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/mean_0";
"1075 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__add___0";
"1076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1";
"1079 layers.14.input_layernorm.weight";
"1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1081 layers.14.self_attn.q_proj.weight";
"1082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1083 layers.14.self_attn.k_proj.weight";
"1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1085 layers.14.self_attn.v_proj.weight";
"1086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_0";
"1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0";
"1089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_1";
"1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1";
"1091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_2";
"1092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2";
"1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1094 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0";
"1101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___0";
"1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0";
"1105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1";
"1106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0";
"1107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2";
"1108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___1";
"1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1";
"1112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3";
"1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1";
"1114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_0";
"1116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_0";
"1117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_1";
"1119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_1";
"1120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_3";
"1125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_3";
"1127 layers.14.self_attn.o_proj.weight";
"1128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0";
"1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1137 layers.14.post_attention_layernorm.weight";
"1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1139 layers.14.mlp.gate_proj.weight";
"1140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1142 layers.14.mlp.up_proj.weight";
"1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0";
"1145 layers.14.mlp.down_proj.weight";
"1146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1";
"1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0";
"1149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/pow_0";
"1150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/mean_0";
"1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__add___0";
"1152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1";
"1155 layers.15.input_layernorm.weight";
"1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1157 layers.15.self_attn.q_proj.weight";
"1158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1159 layers.15.self_attn.k_proj.weight";
"1160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1161 layers.15.self_attn.v_proj.weight";
"1162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_0";
"1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0";
"1165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_1";
"1166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1";
"1167 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_2";
"1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2";
"1169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0";
"1177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___0";
"1180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0";
"1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1";
"1182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0";
"1183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2";
"1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___1";
"1187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1";
"1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3";
"1189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1";
"1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_0";
"1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_0";
"1193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_1";
"1195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_1";
"1196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_3";
"1201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_3";
"1203 layers.15.self_attn.o_proj.weight";
"1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0";
"1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1213 layers.15.post_attention_layernorm.weight";
"1214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1215 layers.15.mlp.gate_proj.weight";
"1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1218 layers.15.mlp.up_proj.weight";
"1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0";
"1221 layers.15.mlp.down_proj.weight";
"1222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1";
"1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0";
"1225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/pow_0";
"1226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/mean_0";
"1227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__add___0";
"1228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1";
"1231 layers.16.input_layernorm.weight";
"1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1233 layers.16.self_attn.q_proj.weight";
"1234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1235 layers.16.self_attn.k_proj.weight";
"1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1237 layers.16.self_attn.v_proj.weight";
"1238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_0";
"1240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0";
"1241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_1";
"1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1";
"1243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_2";
"1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2";
"1245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0";
"1253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___0";
"1256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0";
"1257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1";
"1258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0";
"1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2";
"1260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___1";
"1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1";
"1264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3";
"1265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1";
"1266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_0";
"1268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_0";
"1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_1";
"1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_1";
"1272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_3";
"1277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_3";
"1279 layers.16.self_attn.o_proj.weight";
"1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0";
"1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1289 layers.16.post_attention_layernorm.weight";
"1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1291 layers.16.mlp.gate_proj.weight";
"1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1294 layers.16.mlp.up_proj.weight";
"1295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0";
"1297 layers.16.mlp.down_proj.weight";
"1298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1";
"1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0";
"1301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/pow_0";
"1302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/mean_0";
"1303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__add___0";
"1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1";
"1307 layers.17.input_layernorm.weight";
"1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1309 layers.17.self_attn.q_proj.weight";
"1310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1311 layers.17.self_attn.k_proj.weight";
"1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1313 layers.17.self_attn.v_proj.weight";
"1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_0";
"1316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0";
"1317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_1";
"1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1";
"1319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_2";
"1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2";
"1321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0";
"1329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___0";
"1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0";
"1333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1";
"1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0";
"1335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2";
"1336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___1";
"1339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1";
"1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3";
"1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1";
"1342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_0";
"1344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_0";
"1345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_1";
"1347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_1";
"1348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_3";
"1353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_3";
"1355 layers.17.self_attn.o_proj.weight";
"1356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0";
"1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1365 layers.17.post_attention_layernorm.weight";
"1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1367 layers.17.mlp.gate_proj.weight";
"1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1370 layers.17.mlp.up_proj.weight";
"1371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0";
"1373 layers.17.mlp.down_proj.weight";
"1374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1";
"1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0";
"1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/pow_0";
"1378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/mean_0";
"1379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__add___0";
"1380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1";
"1383 layers.18.input_layernorm.weight";
"1384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1385 layers.18.self_attn.q_proj.weight";
"1386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1387 layers.18.self_attn.k_proj.weight";
"1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1389 layers.18.self_attn.v_proj.weight";
"1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_0";
"1392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0";
"1393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_1";
"1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1";
"1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_2";
"1396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2";
"1397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0";
"1405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___0";
"1408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0";
"1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1";
"1410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0";
"1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2";
"1412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___1";
"1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1";
"1416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3";
"1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1";
"1418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_0";
"1420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_0";
"1421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_1";
"1423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_1";
"1424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_3";
"1429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_3";
"1431 layers.18.self_attn.o_proj.weight";
"1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0";
"1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1441 layers.18.post_attention_layernorm.weight";
"1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1443 layers.18.mlp.gate_proj.weight";
"1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1446 layers.18.mlp.up_proj.weight";
"1447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0";
"1449 layers.18.mlp.down_proj.weight";
"1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1";
"1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0";
"1453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/pow_0";
"1454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/mean_0";
"1455 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__add___0";
"1456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1";
"1459 layers.19.input_layernorm.weight";
"1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1461 layers.19.self_attn.q_proj.weight";
"1462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1463 layers.19.self_attn.k_proj.weight";
"1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1465 layers.19.self_attn.v_proj.weight";
"1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_0";
"1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0";
"1469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_1";
"1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1";
"1471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_2";
"1472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2";
"1473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1475 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0";
"1481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___0";
"1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0";
"1485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1";
"1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0";
"1487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2";
"1488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___1";
"1491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1";
"1492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3";
"1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1";
"1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_0";
"1496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_0";
"1497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_1";
"1499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_1";
"1500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_3";
"1505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_3";
"1507 layers.19.self_attn.o_proj.weight";
"1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0";
"1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1517 layers.19.post_attention_layernorm.weight";
"1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1519 layers.19.mlp.gate_proj.weight";
"1520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1522 layers.19.mlp.up_proj.weight";
"1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0";
"1525 layers.19.mlp.down_proj.weight";
"1526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1";
"1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0";
"1529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/pow_0";
"1530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/mean_0";
"1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__add___0";
"1532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1";
"1535 layers.20.input_layernorm.weight";
"1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1537 layers.20.self_attn.q_proj.weight";
"1538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1539 layers.20.self_attn.k_proj.weight";
"1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1541 layers.20.self_attn.v_proj.weight";
"1542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_0";
"1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0";
"1545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_1";
"1546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1";
"1547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_2";
"1548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2";
"1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0";
"1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___0";
"1560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0";
"1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1";
"1562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0";
"1563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2";
"1564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___1";
"1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1";
"1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3";
"1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1";
"1570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_0";
"1572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_0";
"1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_1";
"1575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_1";
"1576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_3";
"1581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_3";
"1583 layers.20.self_attn.o_proj.weight";
"1584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0";
"1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1593 layers.20.post_attention_layernorm.weight";
"1594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1595 layers.20.mlp.gate_proj.weight";
"1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1598 layers.20.mlp.up_proj.weight";
"1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0";
"1601 layers.20.mlp.down_proj.weight";
"1602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1";
"1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0";
"1605 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/pow_0";
"1606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/mean_0";
"1607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__add___0";
"1608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1";
"1611 layers.21.input_layernorm.weight";
"1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1613 layers.21.self_attn.q_proj.weight";
"1614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1615 layers.21.self_attn.k_proj.weight";
"1616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1617 layers.21.self_attn.v_proj.weight";
"1618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/FQLora/__add___0";
"1619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_0";
"1621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0";
"1622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_1";
"1623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1";
"1624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_2";
"1625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2";
"1626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0";
"1634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___0";
"1637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0";
"1638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1";
"1639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0";
"1640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2";
"1641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___1";
"1644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1";
"1645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3";
"1646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1";
"1647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_0";
"1649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_0";
"1650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_1";
"1652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_1";
"1653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_3";
"1658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_3";
"1660 layers.21.self_attn.o_proj.weight";
"1661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0";
"1663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1670 layers.21.post_attention_layernorm.weight";
"1671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1672 layers.21.mlp.gate_proj.weight";
"1673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1675 layers.21.mlp.up_proj.weight";
"1676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0";
"1678 layers.21.mlp.down_proj.weight";
"1679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1";
"1681 LlamaModel/LlamaRMSNorm[norm]/to_0";
"1682 LlamaModel/LlamaRMSNorm[norm]/pow_0";
"1683 LlamaModel/LlamaRMSNorm[norm]/mean_0";
"1684 LlamaModel/LlamaRMSNorm[norm]/__add___0";
"1685 LlamaModel/LlamaRMSNorm[norm]/rsqrt_0";
"1686 LlamaModel/LlamaRMSNorm[norm]/__mul___0";
"1687 LlamaModel/LlamaRMSNorm[norm]/to_1";
"1688 norm.weight";
"1689 LlamaModel/LlamaRMSNorm[norm]/__mul___1";
"1690 /nncf_model_output_0";
"1691 /nncf_model_output_1";
"1692 /nncf_model_output_2";
"1693 /nncf_model_output_3";
"1694 /nncf_model_output_4";
"1695 /nncf_model_output_5";
"1696 /nncf_model_output_6";
"1697 /nncf_model_output_7";
"1698 /nncf_model_output_8";
"1699 /nncf_model_output_9";
"1700 /nncf_model_output_10";
"1701 /nncf_model_output_11";
"1702 /nncf_model_output_12";
"1703 /nncf_model_output_13";
"1704 /nncf_model_output_14";
"1705 /nncf_model_output_15";
"1706 /nncf_model_output_16";
"1707 /nncf_model_output_17";
"1708 /nncf_model_output_18";
"1709 /nncf_model_output_19";
"1710 /nncf_model_output_20";
"1711 /nncf_model_output_21";
"1712 /nncf_model_output_22";
"1713 /nncf_model_output_23";
"1714 /nncf_model_output_24";
"1715 /nncf_model_output_25";
"1716 /nncf_model_output_26";
"1717 /nncf_model_output_27";
"1718 /nncf_model_output_28";
"1719 /nncf_model_output_29";
"1720 /nncf_model_output_30";
"1721 /nncf_model_output_31";
"1722 /nncf_model_output_32";
"1723 /nncf_model_output_33";
"1724 /nncf_model_output_34";
"1725 /nncf_model_output_35";
"1726 /nncf_model_output_36";
"1727 /nncf_model_output_37";
"1728 /nncf_model_output_38";
"1729 /nncf_model_output_39";
"1730 /nncf_model_output_40";
"1731 /nncf_model_output_41";
"1732 /nncf_model_output_42";
"1733 /nncf_model_output_43";
"1734 /nncf_model_output_44";
"0 /nncf_model_input_0" -> "3 LlamaModel/Embedding[embed_tokens]/embedding_0"  [label="(1, 21) \n0 -> 0", style=dashed];
"1 /nncf_model_input_1" -> "4 LlamaModel/__getitem___0"  [label="(1, 21) \n0 -> 0", style=dashed];
"1 /nncf_model_input_1" -> "7 LlamaModel/__ne___0"  [label="(1, 21) \n0 -> 0", style=dashed];
"2 embed_tokens.weight" -> "3 LlamaModel/Embedding[embed_tokens]/embedding_0"  [label="(32000, 2048) \n0 -> 1", style=solid];
"3 LlamaModel/Embedding[embed_tokens]/embedding_0" -> "8 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"4 LlamaModel/__getitem___0" -> "5 LlamaModel/eq_0"  [label="(1, 1, 1, 21) \n0 -> 0", style=dashed];
"5 LlamaModel/eq_0" -> "6 LlamaModel/__rmul___0"  [label="(1, 1, 1, 21) \n0 -> 0", style=dashed];
"8 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0" -> "9 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"8 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0" -> "13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"8 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0" -> "65 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"9 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/pow_0" -> "10 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"10 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/mean_0" -> "11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__add___0" -> "12 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"12 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "14 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"14 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1" -> "16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"15 layers.0.input_layernorm.weight" -> "16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "18 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "22 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"17 layers.0.self_attn.q_proj.weight" -> "18 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"18 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "23 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"19 layers.0.self_attn.k_proj.weight" -> "20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "25 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"21 layers.0.self_attn.v_proj.weight" -> "22 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"22 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "27 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"23 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_0" -> "24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0" -> "36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0" -> "37 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0" -> "38 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"25 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_1" -> "26 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"26 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1" -> "43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"26 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1" -> "44 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"26 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1" -> "45 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"27 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_2" -> "28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2" -> "53 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1691 /nncf_model_output_1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"29 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "30 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"29 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "31 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"30 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "32 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"31 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "33 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"32 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"33 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0" -> "42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"37 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"38 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "39 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"39 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___0" -> "40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0" -> "41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1" -> "42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0" -> "56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2" -> "49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"44 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"45 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "46 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"46 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___1" -> "47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1" -> "48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3" -> "49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1" -> "50 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1" -> "1690 /nncf_model_output_0"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"50 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "51 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"51 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_0" -> "52 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"52 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_0" -> "57 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"53 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "54 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"54 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_1" -> "55 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"55 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_1" -> "58 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"57 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"58 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "60 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"60 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_3" -> "61 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"61 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "62 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"62 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_3" -> "64 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"63 layers.0.self_attn.o_proj.weight" -> "64 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"64 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "65 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"65 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0" -> "66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "67 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "83 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"67 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "68 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"68 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "70 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"70 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "72 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"72 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "74 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"73 layers.0.post_attention_layernorm.weight" -> "74 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"74 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"74 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"75 layers.0.mlp.gate_proj.weight" -> "76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"78 layers.0.mlp.up_proj.weight" -> "79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0" -> "82 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"81 layers.0.mlp.down_proj.weight" -> "82 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"82 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "83 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"83 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1" -> "84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0" -> "85 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0" -> "89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0" -> "141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"85 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/pow_0" -> "86 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"86 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/mean_0" -> "87 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"87 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__add___0" -> "88 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"88 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "90 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"90 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1" -> "92 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"91 layers.1.input_layernorm.weight" -> "92 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"92 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "94 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"92 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "96 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"92 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"93 layers.1.self_attn.q_proj.weight" -> "94 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"94 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"95 layers.1.self_attn.k_proj.weight" -> "96 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"96 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"97 layers.1.self_attn.v_proj.weight" -> "98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_0" -> "100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0" -> "112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0" -> "113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0" -> "114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_1" -> "102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1" -> "119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1" -> "120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1" -> "121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_2" -> "104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2" -> "129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1693 /nncf_model_output_3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0" -> "118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___0" -> "116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0" -> "117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1" -> "118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0" -> "132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2" -> "125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___1" -> "123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1" -> "124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3" -> "125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1" -> "126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1" -> "1692 /nncf_model_output_2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_0" -> "128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_0" -> "133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_1" -> "131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_1" -> "134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_3" -> "137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_3" -> "140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"139 layers.1.self_attn.o_proj.weight" -> "140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0" -> "142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "145 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"145 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"149 layers.1.post_attention_layernorm.weight" -> "150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"151 layers.1.mlp.gate_proj.weight" -> "152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"154 layers.1.mlp.up_proj.weight" -> "155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0" -> "158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"157 layers.1.mlp.down_proj.weight" -> "158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1" -> "160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0" -> "161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0" -> "165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0" -> "217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/pow_0" -> "162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/mean_0" -> "163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__add___0" -> "164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1" -> "168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"167 layers.2.input_layernorm.weight" -> "168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"169 layers.2.self_attn.q_proj.weight" -> "170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"171 layers.2.self_attn.k_proj.weight" -> "172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"173 layers.2.self_attn.v_proj.weight" -> "174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_0" -> "176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0" -> "188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0" -> "189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0" -> "190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_1" -> "178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1" -> "195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1" -> "196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1" -> "197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_2" -> "180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2" -> "205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1695 /nncf_model_output_5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0" -> "194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___0" -> "192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0" -> "193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1" -> "194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0" -> "208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2" -> "201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___1" -> "199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1" -> "200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3" -> "201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1" -> "202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1" -> "1694 /nncf_model_output_4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_0" -> "204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_0" -> "209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_1" -> "207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_1" -> "210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_3" -> "213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_3" -> "216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"215 layers.2.self_attn.o_proj.weight" -> "216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0" -> "218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"225 layers.2.post_attention_layernorm.weight" -> "226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"227 layers.2.mlp.gate_proj.weight" -> "228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"230 layers.2.mlp.up_proj.weight" -> "231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0" -> "234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"233 layers.2.mlp.down_proj.weight" -> "234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1" -> "236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0" -> "237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0" -> "241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0" -> "293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/pow_0" -> "238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/mean_0" -> "239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__add___0" -> "240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1" -> "244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"243 layers.3.input_layernorm.weight" -> "244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"245 layers.3.self_attn.q_proj.weight" -> "246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"247 layers.3.self_attn.k_proj.weight" -> "248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"249 layers.3.self_attn.v_proj.weight" -> "250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_0" -> "252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0" -> "264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0" -> "265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0" -> "266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_1" -> "254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1" -> "271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1" -> "272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1" -> "273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_2" -> "256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2" -> "281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1697 /nncf_model_output_7"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0" -> "270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___0" -> "268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0" -> "269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1" -> "270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0" -> "284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2" -> "277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___1" -> "275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1" -> "276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3" -> "277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1" -> "278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1" -> "1696 /nncf_model_output_6"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_0" -> "280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_0" -> "285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_1" -> "283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_1" -> "286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_3" -> "289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_3" -> "292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"291 layers.3.self_attn.o_proj.weight" -> "292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0" -> "294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "311 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"301 layers.3.post_attention_layernorm.weight" -> "302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "307 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"303 layers.3.mlp.gate_proj.weight" -> "304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"306 layers.3.mlp.up_proj.weight" -> "307 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"307 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0" -> "310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"309 layers.3.mlp.down_proj.weight" -> "310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "311 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"311 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1" -> "312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0" -> "313 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0" -> "317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0" -> "369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"313 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/pow_0" -> "314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/mean_0" -> "315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__add___0" -> "316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1" -> "320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"319 layers.4.input_layernorm.weight" -> "320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"321 layers.4.self_attn.q_proj.weight" -> "322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"323 layers.4.self_attn.k_proj.weight" -> "324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"325 layers.4.self_attn.v_proj.weight" -> "326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_0" -> "328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0" -> "340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0" -> "341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0" -> "342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_1" -> "330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1" -> "347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1" -> "348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1" -> "349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_2" -> "332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2" -> "357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1699 /nncf_model_output_9"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0" -> "346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___0" -> "344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0" -> "345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1" -> "346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0" -> "360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2" -> "353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___1" -> "351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1" -> "352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3" -> "353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1" -> "354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1" -> "1698 /nncf_model_output_8"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_0" -> "356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_0" -> "361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_1" -> "359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_1" -> "362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_3" -> "365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_3" -> "368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"367 layers.4.self_attn.o_proj.weight" -> "368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0" -> "370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"377 layers.4.post_attention_layernorm.weight" -> "378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"379 layers.4.mlp.gate_proj.weight" -> "380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"382 layers.4.mlp.up_proj.weight" -> "383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0" -> "386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"385 layers.4.mlp.down_proj.weight" -> "386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1" -> "388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0" -> "389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0" -> "393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0" -> "445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/pow_0" -> "390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/mean_0" -> "391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__add___0" -> "392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1" -> "396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"395 layers.5.input_layernorm.weight" -> "396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"397 layers.5.self_attn.q_proj.weight" -> "398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"399 layers.5.self_attn.k_proj.weight" -> "400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"401 layers.5.self_attn.v_proj.weight" -> "402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_0" -> "404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0" -> "416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0" -> "417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0" -> "418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_1" -> "406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1" -> "423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1" -> "424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1" -> "425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_2" -> "408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2" -> "433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1701 /nncf_model_output_11"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0" -> "422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___0" -> "420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0" -> "421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1" -> "422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0" -> "436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2" -> "429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___1" -> "427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1" -> "428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3" -> "429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1" -> "430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1" -> "1700 /nncf_model_output_10"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_0" -> "432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_0" -> "437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_1" -> "435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_1" -> "438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_3" -> "441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_3" -> "444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"443 layers.5.self_attn.o_proj.weight" -> "444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0" -> "446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"453 layers.5.post_attention_layernorm.weight" -> "454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"455 layers.5.mlp.gate_proj.weight" -> "456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"458 layers.5.mlp.up_proj.weight" -> "459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0" -> "462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"461 layers.5.mlp.down_proj.weight" -> "462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1" -> "464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0" -> "465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0" -> "469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0" -> "521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/pow_0" -> "466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/mean_0" -> "467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__add___0" -> "468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1" -> "472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"471 layers.6.input_layernorm.weight" -> "472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"473 layers.6.self_attn.q_proj.weight" -> "474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"475 layers.6.self_attn.k_proj.weight" -> "476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"477 layers.6.self_attn.v_proj.weight" -> "478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_0" -> "480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0" -> "492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0" -> "493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0" -> "494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_1" -> "482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1" -> "499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1" -> "500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1" -> "501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_2" -> "484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2" -> "509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1703 /nncf_model_output_13"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0" -> "498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___0" -> "496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0" -> "497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1" -> "498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0" -> "512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2" -> "505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___1" -> "503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1" -> "504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3" -> "505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1" -> "506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1" -> "1702 /nncf_model_output_12"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_0" -> "508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_0" -> "513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_1" -> "511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_1" -> "514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_3" -> "517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_3" -> "520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"519 layers.6.self_attn.o_proj.weight" -> "520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0" -> "522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"529 layers.6.post_attention_layernorm.weight" -> "530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"531 layers.6.mlp.gate_proj.weight" -> "532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"534 layers.6.mlp.up_proj.weight" -> "535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0" -> "538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"537 layers.6.mlp.down_proj.weight" -> "538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1" -> "540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0" -> "541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0" -> "545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0" -> "597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/pow_0" -> "542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/mean_0" -> "543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__add___0" -> "544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1" -> "548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"547 layers.7.input_layernorm.weight" -> "548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"549 layers.7.self_attn.q_proj.weight" -> "550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"551 layers.7.self_attn.k_proj.weight" -> "552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"553 layers.7.self_attn.v_proj.weight" -> "554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_0" -> "556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0" -> "568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0" -> "569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0" -> "570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_1" -> "558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1" -> "575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1" -> "576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1" -> "577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_2" -> "560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2" -> "585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1705 /nncf_model_output_15"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0" -> "574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___0" -> "572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0" -> "573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1" -> "574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0" -> "588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2" -> "581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___1" -> "579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1" -> "580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3" -> "581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1" -> "582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1" -> "1704 /nncf_model_output_14"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_0" -> "584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_0" -> "589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_1" -> "587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_1" -> "590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_3" -> "593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_3" -> "596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"595 layers.7.self_attn.o_proj.weight" -> "596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0" -> "598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"605 layers.7.post_attention_layernorm.weight" -> "606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"607 layers.7.mlp.gate_proj.weight" -> "608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"610 layers.7.mlp.up_proj.weight" -> "611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0" -> "614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"613 layers.7.mlp.down_proj.weight" -> "614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1" -> "616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0" -> "617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0" -> "621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0" -> "673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/pow_0" -> "618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/mean_0" -> "619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__add___0" -> "620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1" -> "624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"623 layers.8.input_layernorm.weight" -> "624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"625 layers.8.self_attn.q_proj.weight" -> "626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"627 layers.8.self_attn.k_proj.weight" -> "628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"629 layers.8.self_attn.v_proj.weight" -> "630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_0" -> "632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0" -> "644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0" -> "645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0" -> "646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_1" -> "634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1" -> "651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1" -> "652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1" -> "653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_2" -> "636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2" -> "661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1707 /nncf_model_output_17"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0" -> "650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___0" -> "648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0" -> "649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1" -> "650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0" -> "664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2" -> "657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___1" -> "655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1" -> "656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3" -> "657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1" -> "658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1" -> "1706 /nncf_model_output_16"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_0" -> "660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_0" -> "665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_1" -> "663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_1" -> "666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_3" -> "669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "670 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"670 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_3" -> "672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"671 layers.8.self_attn.o_proj.weight" -> "672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0" -> "674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "678 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"678 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"681 layers.8.post_attention_layernorm.weight" -> "682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "687 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"683 layers.8.mlp.gate_proj.weight" -> "684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"686 layers.8.mlp.up_proj.weight" -> "687 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"687 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0" -> "690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"689 layers.8.mlp.down_proj.weight" -> "690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1" -> "692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0" -> "693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0" -> "697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0" -> "749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/pow_0" -> "694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/mean_0" -> "695 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"695 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__add___0" -> "696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1" -> "700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"699 layers.9.input_layernorm.weight" -> "700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"701 layers.9.self_attn.q_proj.weight" -> "702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"703 layers.9.self_attn.k_proj.weight" -> "704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"705 layers.9.self_attn.v_proj.weight" -> "706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_0" -> "708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0" -> "720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0" -> "721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0" -> "722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_1" -> "710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1" -> "727 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1" -> "728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1" -> "729 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_2" -> "712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2" -> "737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1709 /nncf_model_output_19"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "715 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"715 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "727 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "732 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0" -> "726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___0" -> "724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0" -> "725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1" -> "726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0" -> "740 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"727 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2" -> "733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"729 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___1" -> "731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1" -> "732 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"732 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3" -> "733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1" -> "734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1" -> "1708 /nncf_model_output_18"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "735 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"735 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_0" -> "736 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"736 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_0" -> "741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_1" -> "739 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"739 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_1" -> "742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"740 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_3" -> "745 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"745 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_3" -> "748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"747 layers.9.self_attn.o_proj.weight" -> "748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0" -> "750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "751 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"751 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"757 layers.9.post_attention_layernorm.weight" -> "758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"759 layers.9.mlp.gate_proj.weight" -> "760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"762 layers.9.mlp.up_proj.weight" -> "763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0" -> "766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"765 layers.9.mlp.down_proj.weight" -> "766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1" -> "768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0" -> "769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0" -> "773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0" -> "825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/pow_0" -> "770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/mean_0" -> "771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__add___0" -> "772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1" -> "776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"775 layers.10.input_layernorm.weight" -> "776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "782 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"777 layers.10.self_attn.q_proj.weight" -> "778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"779 layers.10.self_attn.k_proj.weight" -> "780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"781 layers.10.self_attn.v_proj.weight" -> "782 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"782 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_0" -> "784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0" -> "796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0" -> "797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0" -> "798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_1" -> "786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1" -> "803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1" -> "804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1" -> "805 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_2" -> "788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2" -> "813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1711 /nncf_model_output_21"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "790 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"790 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0" -> "802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "800 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___0" -> "800 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"800 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0" -> "801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1" -> "802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0" -> "816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2" -> "809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"805 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___1" -> "807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1" -> "808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3" -> "809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1" -> "810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1" -> "1710 /nncf_model_output_20"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_0" -> "812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_0" -> "817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_1" -> "815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_1" -> "818 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"818 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "820 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"820 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_3" -> "821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "822 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"822 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_3" -> "824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"823 layers.10.self_attn.o_proj.weight" -> "824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0" -> "826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"833 layers.10.post_attention_layernorm.weight" -> "834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"835 layers.10.mlp.gate_proj.weight" -> "836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"838 layers.10.mlp.up_proj.weight" -> "839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0" -> "842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"841 layers.10.mlp.down_proj.weight" -> "842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1" -> "844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0" -> "845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0" -> "849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0" -> "901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/pow_0" -> "846 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"846 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/mean_0" -> "847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__add___0" -> "848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1" -> "852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"851 layers.11.input_layernorm.weight" -> "852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"853 layers.11.self_attn.q_proj.weight" -> "854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"855 layers.11.self_attn.k_proj.weight" -> "856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"857 layers.11.self_attn.v_proj.weight" -> "858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_0" -> "860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0" -> "872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0" -> "873 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0" -> "874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_1" -> "862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1" -> "879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1" -> "880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1" -> "881 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_2" -> "864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2" -> "889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1713 /nncf_model_output_23"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "877 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0" -> "878 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"873 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "875 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"875 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___0" -> "876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0" -> "877 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"877 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1" -> "878 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"878 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0" -> "892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2" -> "885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"881 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___1" -> "883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1" -> "884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3" -> "885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1" -> "886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1" -> "1712 /nncf_model_output_22"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_0" -> "888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_0" -> "893 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_1" -> "891 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"891 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_1" -> "894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "895 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"893 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "895 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "895 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"895 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_3" -> "897 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"897 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_3" -> "900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"899 layers.11.self_attn.o_proj.weight" -> "900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0" -> "902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "905 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"905 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"909 layers.11.post_attention_layernorm.weight" -> "910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"911 layers.11.mlp.gate_proj.weight" -> "912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"914 layers.11.mlp.up_proj.weight" -> "915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0" -> "918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"917 layers.11.mlp.down_proj.weight" -> "918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1" -> "920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0" -> "921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0" -> "925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0" -> "977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/pow_0" -> "922 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"922 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/mean_0" -> "923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__add___0" -> "924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "926 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"926 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1" -> "928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"927 layers.12.input_layernorm.weight" -> "928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "930 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"929 layers.12.self_attn.q_proj.weight" -> "930 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"930 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"931 layers.12.self_attn.k_proj.weight" -> "932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"933 layers.12.self_attn.v_proj.weight" -> "934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_0" -> "936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0" -> "948 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0" -> "949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0" -> "950 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_1" -> "938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1" -> "955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1" -> "956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1" -> "957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_2" -> "940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2" -> "965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1715 /nncf_model_output_25"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"941 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"941 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "945 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"945 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "948 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"948 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0" -> "954 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"950 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "951 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"951 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___0" -> "952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0" -> "953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1" -> "954 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"954 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0" -> "968 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2" -> "961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___1" -> "959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1" -> "960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3" -> "961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1" -> "962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1" -> "1714 /nncf_model_output_24"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_0" -> "964 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"964 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_0" -> "969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "966 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"966 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_1" -> "967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_1" -> "970 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"968 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"970 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "972 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"972 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_3" -> "973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "974 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"974 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_3" -> "976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"975 layers.12.self_attn.o_proj.weight" -> "976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0" -> "978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "980 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"980 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"985 layers.12.post_attention_layernorm.weight" -> "986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"987 layers.12.mlp.gate_proj.weight" -> "988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"990 layers.12.mlp.up_proj.weight" -> "991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0" -> "994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"993 layers.12.mlp.down_proj.weight" -> "994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1" -> "996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0" -> "997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0" -> "1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0" -> "1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/pow_0" -> "998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/mean_0" -> "999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1000 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1000 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1" -> "1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1003 layers.13.input_layernorm.weight" -> "1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1005 layers.13.self_attn.q_proj.weight" -> "1006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1007 layers.13.self_attn.k_proj.weight" -> "1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1013 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1009 layers.13.self_attn.v_proj.weight" -> "1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_0" -> "1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1024 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1025 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1013 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_1" -> "1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_2" -> "1016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1041 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1717 /nncf_model_output_27"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1017 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1017 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1021 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1021 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1024 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1036 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1024 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1025 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0" -> "1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0" -> "1044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1" -> "1036 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1036 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1" -> "1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1" -> "1716 /nncf_model_output_26"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_0" -> "1040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1045 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1041 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_1" -> "1043 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1043 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1045 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1050 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1050 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_3" -> "1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1051 layers.13.self_attn.o_proj.weight" -> "1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0" -> "1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1061 layers.13.post_attention_layernorm.weight" -> "1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1067 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1063 layers.13.mlp.gate_proj.weight" -> "1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1066 layers.13.mlp.up_proj.weight" -> "1067 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1067 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0" -> "1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1069 layers.13.mlp.down_proj.weight" -> "1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1" -> "1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0" -> "1073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0" -> "1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0" -> "1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1075 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1075 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1" -> "1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1079 layers.14.input_layernorm.weight" -> "1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1081 layers.14.self_attn.q_proj.weight" -> "1082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1083 layers.14.self_attn.k_proj.weight" -> "1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1085 layers.14.self_attn.v_proj.weight" -> "1086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_0" -> "1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_1" -> "1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_2" -> "1092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1719 /nncf_model_output_29"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1094 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1094 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0" -> "1105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0" -> "1120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1" -> "1112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1" -> "1114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1" -> "1718 /nncf_model_output_28"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_0" -> "1116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_1" -> "1119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_3" -> "1128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1127 layers.14.self_attn.o_proj.weight" -> "1128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0" -> "1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1137 layers.14.post_attention_layernorm.weight" -> "1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1139 layers.14.mlp.gate_proj.weight" -> "1140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1142 layers.14.mlp.up_proj.weight" -> "1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0" -> "1146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1145 layers.14.mlp.down_proj.weight" -> "1146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1" -> "1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0" -> "1149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0" -> "1153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0" -> "1205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1" -> "1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1155 layers.15.input_layernorm.weight" -> "1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1157 layers.15.self_attn.q_proj.weight" -> "1158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1159 layers.15.self_attn.k_proj.weight" -> "1160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1161 layers.15.self_attn.v_proj.weight" -> "1162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1167 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_0" -> "1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_1" -> "1166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1167 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_2" -> "1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1721 /nncf_model_output_31"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0" -> "1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0" -> "1196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1" -> "1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1" -> "1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1" -> "1720 /nncf_model_output_30"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_0" -> "1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_1" -> "1195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_3" -> "1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1203 layers.15.self_attn.o_proj.weight" -> "1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0" -> "1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1213 layers.15.post_attention_layernorm.weight" -> "1214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1215 layers.15.mlp.gate_proj.weight" -> "1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1218 layers.15.mlp.up_proj.weight" -> "1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0" -> "1222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1221 layers.15.mlp.down_proj.weight" -> "1222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1" -> "1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0" -> "1225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0" -> "1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0" -> "1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1" -> "1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1231 layers.16.input_layernorm.weight" -> "1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1233 layers.16.self_attn.q_proj.weight" -> "1234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1235 layers.16.self_attn.k_proj.weight" -> "1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1237 layers.16.self_attn.v_proj.weight" -> "1238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_0" -> "1240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_1" -> "1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_2" -> "1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1723 /nncf_model_output_33"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0" -> "1257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0" -> "1272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1" -> "1264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1" -> "1266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1" -> "1722 /nncf_model_output_32"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_0" -> "1268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_1" -> "1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_3" -> "1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1279 layers.16.self_attn.o_proj.weight" -> "1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0" -> "1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1289 layers.16.post_attention_layernorm.weight" -> "1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1291 layers.16.mlp.gate_proj.weight" -> "1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1294 layers.16.mlp.up_proj.weight" -> "1295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0" -> "1298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1297 layers.16.mlp.down_proj.weight" -> "1298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1" -> "1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0" -> "1301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0" -> "1305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0" -> "1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1" -> "1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1307 layers.17.input_layernorm.weight" -> "1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1309 layers.17.self_attn.q_proj.weight" -> "1310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1311 layers.17.self_attn.k_proj.weight" -> "1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1313 layers.17.self_attn.v_proj.weight" -> "1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_0" -> "1316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_1" -> "1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_2" -> "1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1725 /nncf_model_output_35"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0" -> "1333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0" -> "1348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1" -> "1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1" -> "1342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1" -> "1724 /nncf_model_output_34"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_0" -> "1344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_1" -> "1347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_3" -> "1356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1355 layers.17.self_attn.o_proj.weight" -> "1356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0" -> "1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1365 layers.17.post_attention_layernorm.weight" -> "1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1367 layers.17.mlp.gate_proj.weight" -> "1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1370 layers.17.mlp.up_proj.weight" -> "1371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0" -> "1374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1373 layers.17.mlp.down_proj.weight" -> "1374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1" -> "1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0" -> "1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0" -> "1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0" -> "1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1" -> "1384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1383 layers.18.input_layernorm.weight" -> "1384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1385 layers.18.self_attn.q_proj.weight" -> "1386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1387 layers.18.self_attn.k_proj.weight" -> "1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1389 layers.18.self_attn.v_proj.weight" -> "1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_0" -> "1392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_1" -> "1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_2" -> "1396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1727 /nncf_model_output_37"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0" -> "1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0" -> "1424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1" -> "1416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1" -> "1418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1" -> "1726 /nncf_model_output_36"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_0" -> "1420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_1" -> "1423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_3" -> "1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1431 layers.18.self_attn.o_proj.weight" -> "1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0" -> "1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1441 layers.18.post_attention_layernorm.weight" -> "1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1443 layers.18.mlp.gate_proj.weight" -> "1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1446 layers.18.mlp.up_proj.weight" -> "1447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0" -> "1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1449 layers.18.mlp.down_proj.weight" -> "1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1" -> "1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0" -> "1453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0" -> "1457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0" -> "1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1455 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1455 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1" -> "1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1459 layers.19.input_layernorm.weight" -> "1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1461 layers.19.self_attn.q_proj.weight" -> "1462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1463 layers.19.self_attn.k_proj.weight" -> "1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1465 layers.19.self_attn.v_proj.weight" -> "1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_0" -> "1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_1" -> "1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_2" -> "1472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1729 /nncf_model_output_39"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1475 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1475 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0" -> "1485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0" -> "1500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1" -> "1492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1" -> "1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1" -> "1728 /nncf_model_output_38"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_0" -> "1496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_1" -> "1499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_3" -> "1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1507 layers.19.self_attn.o_proj.weight" -> "1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0" -> "1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1517 layers.19.post_attention_layernorm.weight" -> "1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1519 layers.19.mlp.gate_proj.weight" -> "1520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1522 layers.19.mlp.up_proj.weight" -> "1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0" -> "1526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1525 layers.19.mlp.down_proj.weight" -> "1526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1" -> "1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0" -> "1529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0" -> "1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0" -> "1585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1" -> "1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1535 layers.20.input_layernorm.weight" -> "1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1537 layers.20.self_attn.q_proj.weight" -> "1538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1539 layers.20.self_attn.k_proj.weight" -> "1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1541 layers.20.self_attn.v_proj.weight" -> "1542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_0" -> "1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_1" -> "1546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_2" -> "1548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1731 /nncf_model_output_41"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0" -> "1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0" -> "1576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1" -> "1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1" -> "1570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1" -> "1730 /nncf_model_output_40"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_0" -> "1572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_1" -> "1575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_3" -> "1584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1583 layers.20.self_attn.o_proj.weight" -> "1584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0" -> "1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1593 layers.20.post_attention_layernorm.weight" -> "1594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1595 layers.20.mlp.gate_proj.weight" -> "1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1598 layers.20.mlp.up_proj.weight" -> "1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0" -> "1602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1601 layers.20.mlp.down_proj.weight" -> "1602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1" -> "1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0" -> "1605 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0" -> "1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0" -> "1662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1605 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1" -> "1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1611 layers.21.input_layernorm.weight" -> "1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1613 layers.21.self_attn.q_proj.weight" -> "1614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1615 layers.21.self_attn.k_proj.weight" -> "1616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1617 layers.21.self_attn.v_proj.weight" -> "1618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/FQLora/__add___0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/FQLora/__add___0" -> "1619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_0" -> "1621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_1" -> "1623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_2" -> "1625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1733 /nncf_model_output_43"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0" -> "1638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0" -> "1653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1" -> "1645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1" -> "1647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1" -> "1732 /nncf_model_output_42"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_0" -> "1649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_1" -> "1652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_3" -> "1661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1660 layers.21.self_attn.o_proj.weight" -> "1661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0" -> "1663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1670 layers.21.post_attention_layernorm.weight" -> "1671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1672 layers.21.mlp.gate_proj.weight" -> "1673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1675 layers.21.mlp.up_proj.weight" -> "1676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0" -> "1679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1678 layers.21.mlp.down_proj.weight" -> "1679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1" -> "1681 LlamaModel/LlamaRMSNorm[norm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1681 LlamaModel/LlamaRMSNorm[norm]/to_0" -> "1682 LlamaModel/LlamaRMSNorm[norm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1681 LlamaModel/LlamaRMSNorm[norm]/to_0" -> "1686 LlamaModel/LlamaRMSNorm[norm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1682 LlamaModel/LlamaRMSNorm[norm]/pow_0" -> "1683 LlamaModel/LlamaRMSNorm[norm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1683 LlamaModel/LlamaRMSNorm[norm]/mean_0" -> "1684 LlamaModel/LlamaRMSNorm[norm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1684 LlamaModel/LlamaRMSNorm[norm]/__add___0" -> "1685 LlamaModel/LlamaRMSNorm[norm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1685 LlamaModel/LlamaRMSNorm[norm]/rsqrt_0" -> "1686 LlamaModel/LlamaRMSNorm[norm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1686 LlamaModel/LlamaRMSNorm[norm]/__mul___0" -> "1687 LlamaModel/LlamaRMSNorm[norm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1687 LlamaModel/LlamaRMSNorm[norm]/to_1" -> "1689 LlamaModel/LlamaRMSNorm[norm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1688 norm.weight" -> "1689 LlamaModel/LlamaRMSNorm[norm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1689 LlamaModel/LlamaRMSNorm[norm]/__mul___1" -> "1734 /nncf_model_output_44"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
}
