strict digraph  {
"0 /nncf_model_input_0";
"1 /nncf_model_input_1";
"2 LlamaModel/Embedding[embed_tokens]/to_0";
"3 embed_tokens.weight";
"4 LlamaModel/Embedding[embed_tokens]/embedding_0";
"5 LlamaModel/__getitem___0";
"6 LlamaModel/eq_0";
"7 LlamaModel/__rmul___0";
"8 LlamaModel/__ne___0";
"9 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/to_0";
"10 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0";
"11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1";
"12 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/pow_0";
"13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/mean_0";
"14 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__add___0";
"15 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0";
"17 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_2";
"18 layers.0.input_layernorm.weight";
"19 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1";
"20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/to_0";
"21 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"22 layers.0.self_attn.q_proj.weight";
"23 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"25 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"26 layers.0.self_attn.k_proj.weight";
"27 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"29 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"30 layers.0.self_attn.v_proj.weight";
"31 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"32 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"33 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_0";
"34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0";
"35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_1";
"36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1";
"37 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_2";
"38 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2";
"39 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"44 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"45 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"46 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0";
"47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___0";
"48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___1";
"49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___0";
"50 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0";
"51 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1";
"52 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0";
"53 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2";
"54 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___2";
"55 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___3";
"56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___1";
"57 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1";
"58 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3";
"59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1";
"60 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___4";
"61 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_0";
"62 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_0";
"63 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___5";
"64 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_1";
"65 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_1";
"66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_0";
"67 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_1";
"68 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_2";
"69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"70 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_3";
"71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_3";
"72 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_3";
"73 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"74 layers.0.self_attn.o_proj.weight";
"75 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0";
"78 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"81 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"82 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"83 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"85 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"86 layers.0.post_attention_layernorm.weight";
"87 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"88 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/to_0";
"89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"90 layers.0.mlp.gate_proj.weight";
"91 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"92 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"93 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"94 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"95 layers.0.mlp.up_proj.weight";
"96 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"97 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0";
"99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"100 layers.0.mlp.down_proj.weight";
"101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1";
"104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/to_0";
"105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0";
"106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1";
"107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/pow_0";
"108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/mean_0";
"109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__add___0";
"110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0";
"112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_2";
"113 layers.1.input_layernorm.weight";
"114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1";
"115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/to_0";
"116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"117 layers.1.self_attn.q_proj.weight";
"118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"121 layers.1.self_attn.k_proj.weight";
"122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"125 layers.1.self_attn.v_proj.weight";
"126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_0";
"129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0";
"130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_1";
"131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1";
"132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_2";
"133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2";
"134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0";
"142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___0";
"143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___1";
"144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___0";
"145 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0";
"146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1";
"147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0";
"148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2";
"149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___2";
"150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___3";
"151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___1";
"152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1";
"153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3";
"154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1";
"155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___4";
"156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_0";
"157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_0";
"158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___5";
"159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_1";
"160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_1";
"161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_0";
"162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_1";
"163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_2";
"164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_3";
"166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_3";
"167 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_3";
"168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"169 layers.1.self_attn.o_proj.weight";
"170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0";
"173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"181 layers.1.post_attention_layernorm.weight";
"182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/to_0";
"184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"185 layers.1.mlp.gate_proj.weight";
"186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"190 layers.1.mlp.up_proj.weight";
"191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0";
"194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"195 layers.1.mlp.down_proj.weight";
"196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1";
"199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/to_0";
"200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0";
"201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1";
"202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/pow_0";
"203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/mean_0";
"204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__add___0";
"205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0";
"207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_2";
"208 layers.2.input_layernorm.weight";
"209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1";
"210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/to_0";
"211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"212 layers.2.self_attn.q_proj.weight";
"213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"216 layers.2.self_attn.k_proj.weight";
"217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"220 layers.2.self_attn.v_proj.weight";
"221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_0";
"224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0";
"225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_1";
"226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1";
"227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_2";
"228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2";
"229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0";
"237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___0";
"238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___1";
"239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___0";
"240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0";
"241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1";
"242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0";
"243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2";
"244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___2";
"245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___3";
"246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___1";
"247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1";
"248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3";
"249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1";
"250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___4";
"251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_0";
"252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_0";
"253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___5";
"254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_1";
"255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_1";
"256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_0";
"257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_1";
"258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_2";
"259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_3";
"261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_3";
"262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_3";
"263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"264 layers.2.self_attn.o_proj.weight";
"265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0";
"268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"276 layers.2.post_attention_layernorm.weight";
"277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/to_0";
"279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"280 layers.2.mlp.gate_proj.weight";
"281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"285 layers.2.mlp.up_proj.weight";
"286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0";
"289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"290 layers.2.mlp.down_proj.weight";
"291 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1";
"294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/to_0";
"295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0";
"296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1";
"297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/pow_0";
"298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/mean_0";
"299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__add___0";
"300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0";
"302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_2";
"303 layers.3.input_layernorm.weight";
"304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1";
"305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/to_0";
"306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"307 layers.3.self_attn.q_proj.weight";
"308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"309 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"311 layers.3.self_attn.k_proj.weight";
"312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"313 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"315 layers.3.self_attn.v_proj.weight";
"316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_0";
"319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0";
"320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_1";
"321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1";
"322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_2";
"323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2";
"324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0";
"332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___0";
"333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___1";
"334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___0";
"335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0";
"336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1";
"337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0";
"338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2";
"339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___2";
"340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___3";
"341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___1";
"342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1";
"343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3";
"344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1";
"345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___4";
"346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_0";
"347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_0";
"348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___5";
"349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_1";
"350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_1";
"351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_0";
"352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_1";
"353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_2";
"354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_3";
"356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_3";
"357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_3";
"358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"359 layers.3.self_attn.o_proj.weight";
"360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0";
"363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"367 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"371 layers.3.post_attention_layernorm.weight";
"372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/to_0";
"374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"375 layers.3.mlp.gate_proj.weight";
"376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"380 layers.3.mlp.up_proj.weight";
"381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0";
"384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"385 layers.3.mlp.down_proj.weight";
"386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1";
"389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/to_0";
"390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0";
"391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1";
"392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/pow_0";
"393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/mean_0";
"394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__add___0";
"395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0";
"397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_2";
"398 layers.4.input_layernorm.weight";
"399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1";
"400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/to_0";
"401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"402 layers.4.self_attn.q_proj.weight";
"403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"406 layers.4.self_attn.k_proj.weight";
"407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"410 layers.4.self_attn.v_proj.weight";
"411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_0";
"414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0";
"415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_1";
"416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1";
"417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_2";
"418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2";
"419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0";
"427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___0";
"428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___1";
"429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___0";
"430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0";
"431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1";
"432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0";
"433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2";
"434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___2";
"435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___3";
"436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___1";
"437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1";
"438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3";
"439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1";
"440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___4";
"441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_0";
"442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_0";
"443 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___5";
"444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_1";
"445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_1";
"446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_0";
"447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_1";
"448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_2";
"449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_3";
"451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_3";
"452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_3";
"453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"454 layers.4.self_attn.o_proj.weight";
"455 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0";
"458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"466 layers.4.post_attention_layernorm.weight";
"467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/to_0";
"469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"470 layers.4.mlp.gate_proj.weight";
"471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"475 layers.4.mlp.up_proj.weight";
"476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0";
"479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"480 layers.4.mlp.down_proj.weight";
"481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1";
"484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/to_0";
"485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0";
"486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1";
"487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/pow_0";
"488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/mean_0";
"489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__add___0";
"490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0";
"492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_2";
"493 layers.5.input_layernorm.weight";
"494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1";
"495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/to_0";
"496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"497 layers.5.self_attn.q_proj.weight";
"498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"501 layers.5.self_attn.k_proj.weight";
"502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"505 layers.5.self_attn.v_proj.weight";
"506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_0";
"509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0";
"510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_1";
"511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1";
"512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_2";
"513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2";
"514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0";
"522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___0";
"523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___1";
"524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___0";
"525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0";
"526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1";
"527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0";
"528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2";
"529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___2";
"530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___3";
"531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___1";
"532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1";
"533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3";
"534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1";
"535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___4";
"536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_0";
"537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_0";
"538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___5";
"539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_1";
"540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_1";
"541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_0";
"542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_1";
"543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_2";
"544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_3";
"546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_3";
"547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_3";
"548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"549 layers.5.self_attn.o_proj.weight";
"550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0";
"553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"561 layers.5.post_attention_layernorm.weight";
"562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/to_0";
"564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"565 layers.5.mlp.gate_proj.weight";
"566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"570 layers.5.mlp.up_proj.weight";
"571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0";
"574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"575 layers.5.mlp.down_proj.weight";
"576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1";
"579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/to_0";
"580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0";
"581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1";
"582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/pow_0";
"583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/mean_0";
"584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__add___0";
"585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0";
"587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_2";
"588 layers.6.input_layernorm.weight";
"589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1";
"590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/to_0";
"591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"592 layers.6.self_attn.q_proj.weight";
"593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"596 layers.6.self_attn.k_proj.weight";
"597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"600 layers.6.self_attn.v_proj.weight";
"601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_0";
"604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0";
"605 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_1";
"606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1";
"607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_2";
"608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2";
"609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0";
"617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___0";
"618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___1";
"619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___0";
"620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0";
"621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1";
"622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0";
"623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2";
"624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___2";
"625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___3";
"626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___1";
"627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1";
"628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3";
"629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1";
"630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___4";
"631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_0";
"632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_0";
"633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___5";
"634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_1";
"635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_1";
"636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_0";
"637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_1";
"638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_2";
"639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_3";
"641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_3";
"642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_3";
"643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"644 layers.6.self_attn.o_proj.weight";
"645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0";
"648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"656 layers.6.post_attention_layernorm.weight";
"657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/to_0";
"659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"660 layers.6.mlp.gate_proj.weight";
"661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"665 layers.6.mlp.up_proj.weight";
"666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0";
"669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"670 layers.6.mlp.down_proj.weight";
"671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1";
"674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/to_0";
"675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0";
"676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1";
"677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/pow_0";
"678 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/mean_0";
"679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__add___0";
"680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0";
"682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_2";
"683 layers.7.input_layernorm.weight";
"684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1";
"685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/to_0";
"686 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"687 layers.7.self_attn.q_proj.weight";
"688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"689 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"691 layers.7.self_attn.k_proj.weight";
"692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"695 layers.7.self_attn.v_proj.weight";
"696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_0";
"699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0";
"700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_1";
"701 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1";
"702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_2";
"703 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2";
"704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"705 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0";
"712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___0";
"713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___1";
"714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___0";
"715 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0";
"716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1";
"717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0";
"718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2";
"719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___2";
"720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___3";
"721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___1";
"722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1";
"723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3";
"724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1";
"725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___4";
"726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_0";
"727 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_0";
"728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___5";
"729 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_1";
"730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_1";
"731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_0";
"732 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_1";
"733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_2";
"734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"735 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_3";
"736 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_3";
"737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_3";
"738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"739 layers.7.self_attn.o_proj.weight";
"740 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0";
"743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"745 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"747 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"751 layers.7.post_attention_layernorm.weight";
"752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/to_0";
"754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"755 layers.7.mlp.gate_proj.weight";
"756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"757 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"759 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"760 layers.7.mlp.up_proj.weight";
"761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0";
"764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"765 layers.7.mlp.down_proj.weight";
"766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1";
"769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/to_0";
"770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0";
"771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1";
"772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/pow_0";
"773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/mean_0";
"774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__add___0";
"775 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0";
"777 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_2";
"778 layers.8.input_layernorm.weight";
"779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1";
"780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/to_0";
"781 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"782 layers.8.self_attn.q_proj.weight";
"783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"786 layers.8.self_attn.k_proj.weight";
"787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"790 layers.8.self_attn.v_proj.weight";
"791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_0";
"794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0";
"795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_1";
"796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1";
"797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_2";
"798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2";
"799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"800 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"805 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0";
"807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___0";
"808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___1";
"809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___0";
"810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0";
"811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1";
"812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0";
"813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2";
"814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___2";
"815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___3";
"816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___1";
"817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1";
"818 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3";
"819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1";
"820 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___4";
"821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_0";
"822 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_0";
"823 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___5";
"824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_1";
"825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_1";
"826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_0";
"827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_1";
"828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_2";
"829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_3";
"831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_3";
"832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_3";
"833 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"834 layers.8.self_attn.o_proj.weight";
"835 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0";
"838 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"846 layers.8.post_attention_layernorm.weight";
"847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/to_0";
"849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"850 layers.8.mlp.gate_proj.weight";
"851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"853 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"855 layers.8.mlp.up_proj.weight";
"856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0";
"859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"860 layers.8.mlp.down_proj.weight";
"861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1";
"864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/to_0";
"865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0";
"866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1";
"867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/pow_0";
"868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/mean_0";
"869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__add___0";
"870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0";
"872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_2";
"873 layers.9.input_layernorm.weight";
"874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1";
"875 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/to_0";
"876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"877 layers.9.self_attn.q_proj.weight";
"878 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"881 layers.9.self_attn.k_proj.weight";
"882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"885 layers.9.self_attn.v_proj.weight";
"886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_0";
"889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0";
"890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_1";
"891 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1";
"892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_2";
"893 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2";
"894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"895 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"897 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"899 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0";
"902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___0";
"903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___1";
"904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___0";
"905 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0";
"906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1";
"907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0";
"908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2";
"909 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___2";
"910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___3";
"911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___1";
"912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1";
"913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3";
"914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1";
"915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___4";
"916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_0";
"917 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_0";
"918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___5";
"919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_1";
"920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_1";
"921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_0";
"922 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_1";
"923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_2";
"924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_3";
"926 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_3";
"927 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_3";
"928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"929 layers.9.self_attn.o_proj.weight";
"930 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"931 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0";
"933 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"941 layers.9.post_attention_layernorm.weight";
"942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/to_0";
"944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"945 layers.9.mlp.gate_proj.weight";
"946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"948 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"950 layers.9.mlp.up_proj.weight";
"951 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0";
"954 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"955 layers.9.mlp.down_proj.weight";
"956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1";
"959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/to_0";
"960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0";
"961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1";
"962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/pow_0";
"963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/mean_0";
"964 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__add___0";
"965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"966 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0";
"967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_2";
"968 layers.10.input_layernorm.weight";
"969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1";
"970 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/to_0";
"971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"972 layers.10.self_attn.q_proj.weight";
"973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"974 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"976 layers.10.self_attn.k_proj.weight";
"977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"980 layers.10.self_attn.v_proj.weight";
"981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_0";
"984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0";
"985 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_1";
"986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1";
"987 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_2";
"988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2";
"989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"990 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"993 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0";
"997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___0";
"998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___1";
"999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___0";
"1000 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0";
"1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1";
"1002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0";
"1003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2";
"1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___1";
"1007 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1";
"1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3";
"1009 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1";
"1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_0";
"1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_0";
"1013 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_1";
"1015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_1";
"1016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1017 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_3";
"1021 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_3";
"1023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1024 layers.10.self_attn.o_proj.weight";
"1025 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0";
"1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1036 layers.10.post_attention_layernorm.weight";
"1037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/to_0";
"1039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1040 layers.10.mlp.gate_proj.weight";
"1041 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1043 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1045 layers.10.mlp.up_proj.weight";
"1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0";
"1049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"1050 layers.10.mlp.down_proj.weight";
"1051 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1";
"1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/to_0";
"1055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0";
"1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1";
"1057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/pow_0";
"1058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/mean_0";
"1059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__add___0";
"1060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1061 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_2";
"1063 layers.11.input_layernorm.weight";
"1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/to_0";
"1066 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"1067 layers.11.self_attn.q_proj.weight";
"1068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1069 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"1071 layers.11.self_attn.k_proj.weight";
"1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"1075 layers.11.self_attn.v_proj.weight";
"1076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_0";
"1079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0";
"1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_1";
"1081 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1";
"1082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_2";
"1083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2";
"1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1085 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0";
"1092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1094 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___0";
"1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0";
"1096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1";
"1097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0";
"1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2";
"1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___1";
"1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1";
"1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3";
"1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1";
"1105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_0";
"1107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_0";
"1108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_1";
"1110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_1";
"1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_3";
"1116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_3";
"1118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1119 layers.11.self_attn.o_proj.weight";
"1120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0";
"1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1131 layers.11.post_attention_layernorm.weight";
"1132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/to_0";
"1134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1135 layers.11.mlp.gate_proj.weight";
"1136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1140 layers.11.mlp.up_proj.weight";
"1141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0";
"1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"1145 layers.11.mlp.down_proj.weight";
"1146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1";
"1149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/to_0";
"1150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0";
"1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1";
"1152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/pow_0";
"1153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/mean_0";
"1154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__add___0";
"1155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_2";
"1158 layers.12.input_layernorm.weight";
"1159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/to_0";
"1161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"1162 layers.12.self_attn.q_proj.weight";
"1163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"1166 layers.12.self_attn.k_proj.weight";
"1167 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"1170 layers.12.self_attn.v_proj.weight";
"1171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_0";
"1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0";
"1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_1";
"1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1";
"1177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_2";
"1178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2";
"1179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0";
"1187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___0";
"1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0";
"1191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1";
"1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0";
"1193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2";
"1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___1";
"1197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1";
"1198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3";
"1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1";
"1200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_0";
"1202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_0";
"1203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_1";
"1205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_1";
"1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_3";
"1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_3";
"1213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1214 layers.12.self_attn.o_proj.weight";
"1215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0";
"1218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1226 layers.12.post_attention_layernorm.weight";
"1227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/to_0";
"1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1230 layers.12.mlp.gate_proj.weight";
"1231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1235 layers.12.mlp.up_proj.weight";
"1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0";
"1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"1240 layers.12.mlp.down_proj.weight";
"1241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1";
"1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/to_0";
"1245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0";
"1246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1";
"1247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/pow_0";
"1248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/mean_0";
"1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__add___0";
"1250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_2";
"1253 layers.13.input_layernorm.weight";
"1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/to_0";
"1256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"1257 layers.13.self_attn.q_proj.weight";
"1258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"1261 layers.13.self_attn.k_proj.weight";
"1262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"1265 layers.13.self_attn.v_proj.weight";
"1266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_0";
"1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0";
"1270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_1";
"1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1";
"1272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_2";
"1273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2";
"1274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0";
"1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___0";
"1285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0";
"1286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1";
"1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0";
"1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2";
"1289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1291 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___1";
"1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1";
"1293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3";
"1294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1";
"1295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_0";
"1297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_0";
"1298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_1";
"1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_1";
"1301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_3";
"1306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1307 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_3";
"1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1309 layers.13.self_attn.o_proj.weight";
"1310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1311 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0";
"1313 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1321 layers.13.post_attention_layernorm.weight";
"1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/to_0";
"1324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1325 layers.13.mlp.gate_proj.weight";
"1326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1330 layers.13.mlp.up_proj.weight";
"1331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0";
"1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"1335 layers.13.mlp.down_proj.weight";
"1336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1";
"1339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/to_0";
"1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0";
"1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1";
"1342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/pow_0";
"1343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/mean_0";
"1344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__add___0";
"1345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_2";
"1348 layers.14.input_layernorm.weight";
"1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/to_0";
"1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"1352 layers.14.self_attn.q_proj.weight";
"1353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"1356 layers.14.self_attn.k_proj.weight";
"1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"1360 layers.14.self_attn.v_proj.weight";
"1361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_0";
"1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0";
"1365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_1";
"1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1";
"1367 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_2";
"1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2";
"1369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0";
"1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___0";
"1380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0";
"1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1";
"1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0";
"1383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2";
"1384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___1";
"1387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1";
"1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3";
"1389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1";
"1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_0";
"1392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_0";
"1393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_1";
"1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_1";
"1396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_3";
"1401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_3";
"1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1404 layers.14.self_attn.o_proj.weight";
"1405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0";
"1408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1416 layers.14.post_attention_layernorm.weight";
"1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/to_0";
"1419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1420 layers.14.mlp.gate_proj.weight";
"1421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1425 layers.14.mlp.up_proj.weight";
"1426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0";
"1429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"1430 layers.14.mlp.down_proj.weight";
"1431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1";
"1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/to_0";
"1435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0";
"1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1";
"1437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/pow_0";
"1438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/mean_0";
"1439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__add___0";
"1440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_2";
"1443 layers.15.input_layernorm.weight";
"1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/to_0";
"1446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"1447 layers.15.self_attn.q_proj.weight";
"1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"1451 layers.15.self_attn.k_proj.weight";
"1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"1455 layers.15.self_attn.v_proj.weight";
"1456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_0";
"1459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0";
"1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_1";
"1461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1";
"1462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_2";
"1463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2";
"1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0";
"1472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___0";
"1475 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0";
"1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1";
"1477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0";
"1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2";
"1479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___1";
"1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1";
"1483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3";
"1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1";
"1485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_0";
"1487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_0";
"1488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_1";
"1490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_1";
"1491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_3";
"1496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_3";
"1498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1499 layers.15.self_attn.o_proj.weight";
"1500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0";
"1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1511 layers.15.post_attention_layernorm.weight";
"1512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/to_0";
"1514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1515 layers.15.mlp.gate_proj.weight";
"1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1520 layers.15.mlp.up_proj.weight";
"1521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0";
"1524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"1525 layers.15.mlp.down_proj.weight";
"1526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1";
"1529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/to_0";
"1530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0";
"1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1";
"1532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/pow_0";
"1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/mean_0";
"1534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__add___0";
"1535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_2";
"1538 layers.16.input_layernorm.weight";
"1539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/to_0";
"1541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"1542 layers.16.self_attn.q_proj.weight";
"1543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"1546 layers.16.self_attn.k_proj.weight";
"1547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"1550 layers.16.self_attn.v_proj.weight";
"1551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_0";
"1554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0";
"1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_1";
"1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1";
"1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_2";
"1558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2";
"1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0";
"1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___0";
"1570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0";
"1571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1";
"1572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0";
"1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2";
"1574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___1";
"1577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1";
"1578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3";
"1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1";
"1580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_0";
"1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_0";
"1583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_1";
"1585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_1";
"1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_3";
"1591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_3";
"1593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1594 layers.16.self_attn.o_proj.weight";
"1595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0";
"1598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1605 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1606 layers.16.post_attention_layernorm.weight";
"1607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/to_0";
"1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1610 layers.16.mlp.gate_proj.weight";
"1611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1615 layers.16.mlp.up_proj.weight";
"1616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0";
"1619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"1620 layers.16.mlp.down_proj.weight";
"1621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1";
"1624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/to_0";
"1625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0";
"1626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1";
"1627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/pow_0";
"1628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/mean_0";
"1629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__add___0";
"1630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_2";
"1633 layers.17.input_layernorm.weight";
"1634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/to_0";
"1636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"1637 layers.17.self_attn.q_proj.weight";
"1638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"1641 layers.17.self_attn.k_proj.weight";
"1642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"1645 layers.17.self_attn.v_proj.weight";
"1646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_0";
"1649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0";
"1650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_1";
"1651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1";
"1652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_2";
"1653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2";
"1654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0";
"1662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___0";
"1665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0";
"1666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1";
"1667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0";
"1668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2";
"1669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1670 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___1";
"1672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1";
"1673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3";
"1674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1";
"1675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_0";
"1677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_0";
"1678 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_1";
"1680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_1";
"1681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1683 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_3";
"1686 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1687 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_3";
"1688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1689 layers.17.self_attn.o_proj.weight";
"1690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0";
"1693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1695 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1701 layers.17.post_attention_layernorm.weight";
"1702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1703 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/to_0";
"1704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1705 layers.17.mlp.gate_proj.weight";
"1706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1710 layers.17.mlp.up_proj.weight";
"1711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0";
"1714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"1715 layers.17.mlp.down_proj.weight";
"1716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1";
"1719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/to_0";
"1720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0";
"1721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1";
"1722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/pow_0";
"1723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/mean_0";
"1724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__add___0";
"1725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1727 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_2";
"1728 layers.18.input_layernorm.weight";
"1729 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/to_0";
"1731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"1732 layers.18.self_attn.q_proj.weight";
"1733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1735 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"1736 layers.18.self_attn.k_proj.weight";
"1737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1739 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"1740 layers.18.self_attn.v_proj.weight";
"1741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_0";
"1744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0";
"1745 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_1";
"1746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1";
"1747 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_2";
"1748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2";
"1749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1751 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0";
"1757 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1759 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___0";
"1760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0";
"1761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1";
"1762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0";
"1763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2";
"1764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1765 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___1";
"1767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1";
"1768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3";
"1769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1";
"1770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_0";
"1772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_0";
"1773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_1";
"1775 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_1";
"1776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1777 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_3";
"1781 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1782 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_3";
"1783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1784 layers.18.self_attn.o_proj.weight";
"1785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0";
"1788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1790 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1796 layers.18.post_attention_layernorm.weight";
"1797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/to_0";
"1799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1800 layers.18.mlp.gate_proj.weight";
"1801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1805 layers.18.mlp.up_proj.weight";
"1806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0";
"1809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"1810 layers.18.mlp.down_proj.weight";
"1811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1";
"1814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/to_0";
"1815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0";
"1816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1";
"1817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/pow_0";
"1818 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/mean_0";
"1819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__add___0";
"1820 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1822 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_2";
"1823 layers.19.input_layernorm.weight";
"1824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/to_0";
"1826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"1827 layers.19.self_attn.q_proj.weight";
"1828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"1831 layers.19.self_attn.k_proj.weight";
"1832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1833 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"1835 layers.19.self_attn.v_proj.weight";
"1836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1838 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_0";
"1839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0";
"1840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_1";
"1841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1";
"1842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_2";
"1843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2";
"1844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1846 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0";
"1852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1853 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___0";
"1855 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0";
"1856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1";
"1857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0";
"1858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2";
"1859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___1";
"1862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1";
"1863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3";
"1864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1";
"1865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_0";
"1867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_0";
"1868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_1";
"1870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_1";
"1871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1873 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1875 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_3";
"1876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1877 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_3";
"1878 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1879 layers.19.self_attn.o_proj.weight";
"1880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1881 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0";
"1883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1891 layers.19.post_attention_layernorm.weight";
"1892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1893 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/to_0";
"1894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1895 layers.19.mlp.gate_proj.weight";
"1896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1897 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1899 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1900 layers.19.mlp.up_proj.weight";
"1901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0";
"1904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"1905 layers.19.mlp.down_proj.weight";
"1906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"1908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1";
"1909 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/to_0";
"1910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0";
"1911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1";
"1912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/pow_0";
"1913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/mean_0";
"1914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__add___0";
"1915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"1916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0";
"1917 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_2";
"1918 layers.20.input_layernorm.weight";
"1919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1";
"1920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/to_0";
"1921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"1922 layers.20.self_attn.q_proj.weight";
"1923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"1925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"1926 layers.20.self_attn.k_proj.weight";
"1927 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"1929 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"1930 layers.20.self_attn.v_proj.weight";
"1931 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"1933 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_0";
"1934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0";
"1935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_1";
"1936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1";
"1937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_2";
"1938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2";
"1939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"1940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"1941 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"1942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"1943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"1944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"1945 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"1946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0";
"1947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___0";
"1948 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___1";
"1949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___0";
"1950 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0";
"1951 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1";
"1952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0";
"1953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2";
"1954 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___2";
"1955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___3";
"1956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___1";
"1957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1";
"1958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3";
"1959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1";
"1960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___4";
"1961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_0";
"1962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_0";
"1963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___5";
"1964 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_1";
"1965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_1";
"1966 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_0";
"1967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_1";
"1968 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_2";
"1969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"1970 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_3";
"1971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_3";
"1972 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_3";
"1973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"1974 layers.20.self_attn.o_proj.weight";
"1975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"1977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0";
"1978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"1979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"1980 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"1981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"1982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"1983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"1984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"1985 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"1986 layers.20.post_attention_layernorm.weight";
"1987 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"1988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/to_0";
"1989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"1990 layers.20.mlp.gate_proj.weight";
"1991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"1993 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"1994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"1995 layers.20.mlp.up_proj.weight";
"1996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"1997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"1998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0";
"1999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"2000 layers.20.mlp.down_proj.weight";
"2001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"2002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"2003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1";
"2004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/to_0";
"2005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0";
"2006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1";
"2007 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/pow_0";
"2008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/mean_0";
"2009 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__add___0";
"2010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/rsqrt_0";
"2011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0";
"2012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_2";
"2013 layers.21.input_layernorm.weight";
"2014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1";
"2015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/to_0";
"2016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0";
"2017 layers.21.self_attn.q_proj.weight";
"2018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"2019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0";
"2020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0";
"2021 layers.21.self_attn.k_proj.weight";
"2022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"2023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0";
"2024 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0";
"2025 layers.21.self_attn.v_proj.weight";
"2026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"2027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0";
"2028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_0";
"2029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0";
"2030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_1";
"2031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1";
"2032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_2";
"2033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2";
"2034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0";
"2035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0";
"2036 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0";
"2037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0";
"2038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1";
"2039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0";
"2040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1";
"2041 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0";
"2042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___0";
"2043 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___1";
"2044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___0";
"2045 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0";
"2046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1";
"2047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0";
"2048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2";
"2049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___2";
"2050 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___3";
"2051 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___1";
"2052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1";
"2053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3";
"2054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1";
"2055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___4";
"2056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_0";
"2057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_0";
"2058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___5";
"2059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_1";
"2060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_1";
"2061 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_0";
"2062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_1";
"2063 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_2";
"2064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0";
"2065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_3";
"2066 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_3";
"2067 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_3";
"2068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0";
"2069 layers.21.self_attn.o_proj.weight";
"2070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"2071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0";
"2072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0";
"2073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0";
"2074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1";
"2075 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/pow_0";
"2076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/mean_0";
"2077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__add___0";
"2078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0";
"2079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0";
"2080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_2";
"2081 layers.21.post_attention_layernorm.weight";
"2082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1";
"2083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/to_0";
"2084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/to_0";
"2085 layers.21.mlp.gate_proj.weight";
"2086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"2087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0";
"2088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0";
"2089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/to_0";
"2090 layers.21.mlp.up_proj.weight";
"2091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"2092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0";
"2093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0";
"2094 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/to_0";
"2095 layers.21.mlp.down_proj.weight";
"2096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0";
"2097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0";
"2098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1";
"2099 LlamaModel/LlamaRMSNorm[norm]/to_0";
"2100 LlamaModel/LlamaRMSNorm[norm]/to_1";
"2101 LlamaModel/LlamaRMSNorm[norm]/pow_0";
"2102 LlamaModel/LlamaRMSNorm[norm]/mean_0";
"2103 LlamaModel/LlamaRMSNorm[norm]/__add___0";
"2104 LlamaModel/LlamaRMSNorm[norm]/rsqrt_0";
"2105 LlamaModel/LlamaRMSNorm[norm]/__mul___0";
"2106 LlamaModel/LlamaRMSNorm[norm]/to_2";
"2107 norm.weight";
"2108 LlamaModel/LlamaRMSNorm[norm]/__mul___1";
"2109 /nncf_model_output_0";
"2110 /nncf_model_output_1";
"2111 /nncf_model_output_2";
"2112 /nncf_model_output_3";
"2113 /nncf_model_output_4";
"2114 /nncf_model_output_5";
"2115 /nncf_model_output_6";
"2116 /nncf_model_output_7";
"2117 /nncf_model_output_8";
"2118 /nncf_model_output_9";
"2119 /nncf_model_output_10";
"2120 /nncf_model_output_11";
"2121 /nncf_model_output_12";
"2122 /nncf_model_output_13";
"2123 /nncf_model_output_14";
"2124 /nncf_model_output_15";
"2125 /nncf_model_output_16";
"2126 /nncf_model_output_17";
"2127 /nncf_model_output_18";
"2128 /nncf_model_output_19";
"2129 /nncf_model_output_20";
"2130 /nncf_model_output_21";
"2131 /nncf_model_output_22";
"2132 /nncf_model_output_23";
"2133 /nncf_model_output_24";
"2134 /nncf_model_output_25";
"2135 /nncf_model_output_26";
"2136 /nncf_model_output_27";
"2137 /nncf_model_output_28";
"2138 /nncf_model_output_29";
"2139 /nncf_model_output_30";
"2140 /nncf_model_output_31";
"2141 /nncf_model_output_32";
"2142 /nncf_model_output_33";
"2143 /nncf_model_output_34";
"2144 /nncf_model_output_35";
"2145 /nncf_model_output_36";
"2146 /nncf_model_output_37";
"2147 /nncf_model_output_38";
"2148 /nncf_model_output_39";
"2149 /nncf_model_output_40";
"2150 /nncf_model_output_41";
"2151 /nncf_model_output_42";
"2152 /nncf_model_output_43";
"2153 /nncf_model_output_44";
"0 /nncf_model_input_0" -> "2 LlamaModel/Embedding[embed_tokens]/to_0"  [label="(1, 21) \n0 -> 0", style=dashed];
"1 /nncf_model_input_1" -> "5 LlamaModel/__getitem___0"  [label="(1, 21) \n0 -> 0", style=dashed];
"1 /nncf_model_input_1" -> "8 LlamaModel/__ne___0"  [label="(1, 21) \n0 -> 0", style=dashed];
"2 LlamaModel/Embedding[embed_tokens]/to_0" -> "4 LlamaModel/Embedding[embed_tokens]/embedding_0"  [label="(1, 21) \n0 -> 0", style=dashed];
"3 embed_tokens.weight" -> "4 LlamaModel/Embedding[embed_tokens]/embedding_0"  [label="(32000, 2048) \n0 -> 1", style=solid];
"4 LlamaModel/Embedding[embed_tokens]/embedding_0" -> "9 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"5 LlamaModel/__getitem___0" -> "6 LlamaModel/eq_0"  [label="(1, 1, 1, 21) \n0 -> 0", style=dashed];
"6 LlamaModel/eq_0" -> "7 LlamaModel/__rmul___0"  [label="(1, 1, 1, 21) \n0 -> 0", style=dashed];
"9 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/to_0" -> "10 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"10 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_0" -> "11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1" -> "12 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1" -> "16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"11 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_1" -> "77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"12 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/pow_0" -> "13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"13 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/mean_0" -> "14 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"14 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__add___0" -> "15 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"15 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"16 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "17 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"17 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/to_2" -> "19 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"18 layers.0.input_layernorm.weight" -> "19 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"19 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"20 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/to_0" -> "21 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"21 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"21 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "25 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"22 layers.0.self_attn.q_proj.weight" -> "23 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"23 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"24 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "33 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"25 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"25 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "29 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"26 layers.0.self_attn.k_proj.weight" -> "27 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"27 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"28 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"29 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "32 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"30 layers.0.self_attn.v_proj.weight" -> "31 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"31 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "32 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"32 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "37 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"33 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_0" -> "34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0" -> "46 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0" -> "47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"34 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_0" -> "48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"35 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_1" -> "36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1" -> "53 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1" -> "54 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"36 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_1" -> "55 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"37 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_2" -> "38 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"38 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2" -> "63 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"38 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2110 /nncf_model_output_1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"39 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"39 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"40 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"41 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"42 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "44 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"43 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "45 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"44 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "46 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"44 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "53 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"45 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "51 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"45 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "58 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"46 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___0" -> "52 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"47 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "50 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"48 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"49 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___0" -> "50 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"50 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_0" -> "51 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"51 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___1" -> "52 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"52 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___0" -> "66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"53 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___2" -> "59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"54 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "57 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"55 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"56 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__neg___1" -> "57 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"57 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/cat_1" -> "58 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"58 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__mul___3" -> "59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1" -> "60 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"59 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__add___1" -> "2109 /nncf_model_output_0"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"60 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "61 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"61 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_0" -> "62 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"62 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_0" -> "67 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"63 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "64 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"64 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/expand_1" -> "65 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"65 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/reshape_1" -> "68 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"66 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"67 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"68 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"69 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "70 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"70 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/transpose_3" -> "71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"71 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "72 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"72 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/view_3" -> "73 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"73 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"74 layers.0.self_attn.o_proj.weight" -> "75 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"75 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"76 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"77 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___0" -> "78 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"78 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"79 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"80 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "81 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"81 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "82 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"82 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "83 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"83 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"84 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "85 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"85 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "87 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"86 layers.0.post_attention_layernorm.weight" -> "87 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"87 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "88 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"88 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/to_0" -> "89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "92 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"89 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "94 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"90 layers.0.mlp.gate_proj.weight" -> "91 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"91 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "92 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"92 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "93 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"93 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"94 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "97 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"95 layers.0.mlp.up_proj.weight" -> "96 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"96 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "97 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"97 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"98 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/__mul___0" -> "99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"99 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"100 layers.0.mlp.down_proj.weight" -> "101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[0]/__add___1" -> "104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/to_0" -> "105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_0" -> "106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1" -> "107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1" -> "111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_1" -> "172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/pow_0" -> "108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/mean_0" -> "109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__add___0" -> "110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/to_2" -> "114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"113 layers.1.input_layernorm.weight" -> "114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/to_0" -> "116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"117 layers.1.self_attn.q_proj.weight" -> "118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"119 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"121 layers.1.self_attn.k_proj.weight" -> "122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"125 layers.1.self_attn.v_proj.weight" -> "126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_0" -> "129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0" -> "141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0" -> "142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_0" -> "143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_1" -> "131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1" -> "148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1" -> "149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"131 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_1" -> "150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_2" -> "133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2" -> "158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2112 /nncf_model_output_3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"135 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"140 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___0" -> "147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "145 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___0" -> "145 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"145 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_0" -> "146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___1" -> "147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___0" -> "161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___2" -> "154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__neg___1" -> "152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/cat_1" -> "153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__mul___3" -> "154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1" -> "155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__add___1" -> "2111 /nncf_model_output_2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_0" -> "157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_0" -> "162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"158 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/expand_1" -> "160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/reshape_1" -> "163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"162 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/transpose_3" -> "166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"166 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "167 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"167 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/view_3" -> "168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"169 layers.1.self_attn.o_proj.weight" -> "170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"170 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___0" -> "173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"181 layers.1.post_attention_layernorm.weight" -> "182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/to_0" -> "184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"185 layers.1.mlp.gate_proj.weight" -> "186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"190 layers.1.mlp.up_proj.weight" -> "191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/__mul___0" -> "194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"195 layers.1.mlp.down_proj.weight" -> "196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[1]/__add___1" -> "199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/to_0" -> "200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_0" -> "201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1" -> "202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1" -> "206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_1" -> "267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/pow_0" -> "203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/mean_0" -> "204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__add___0" -> "205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/to_2" -> "209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"208 layers.2.input_layernorm.weight" -> "209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/to_0" -> "211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"212 layers.2.self_attn.q_proj.weight" -> "213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"214 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"216 layers.2.self_attn.k_proj.weight" -> "217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"220 layers.2.self_attn.v_proj.weight" -> "221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_0" -> "224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0" -> "236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0" -> "237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_0" -> "238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_1" -> "226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1" -> "243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1" -> "244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"226 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_1" -> "245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_2" -> "228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2" -> "253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2114 /nncf_model_output_5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"230 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"235 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___0" -> "242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___0" -> "240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"240 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_0" -> "241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___1" -> "242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___0" -> "256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___2" -> "249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__neg___1" -> "247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/cat_1" -> "248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__mul___3" -> "249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1" -> "250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__add___1" -> "2113 /nncf_model_output_4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_0" -> "252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_0" -> "257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"253 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/expand_1" -> "255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/reshape_1" -> "258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"257 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/transpose_3" -> "261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"261 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/view_3" -> "263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"264 layers.2.self_attn.o_proj.weight" -> "265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"265 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___0" -> "268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"276 layers.2.post_attention_layernorm.weight" -> "277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/to_0" -> "279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"280 layers.2.mlp.gate_proj.weight" -> "281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"285 layers.2.mlp.up_proj.weight" -> "286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/__mul___0" -> "289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"290 layers.2.mlp.down_proj.weight" -> "291 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"291 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[2]/__add___1" -> "294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/to_0" -> "295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_0" -> "296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1" -> "297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1" -> "301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_1" -> "362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/pow_0" -> "298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/mean_0" -> "299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__add___0" -> "300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/to_2" -> "304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"303 layers.3.input_layernorm.weight" -> "304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/to_0" -> "306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "309 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"307 layers.3.self_attn.q_proj.weight" -> "308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "309 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"309 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "313 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"311 layers.3.self_attn.k_proj.weight" -> "312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "313 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"313 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"315 layers.3.self_attn.v_proj.weight" -> "316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_0" -> "319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0" -> "331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0" -> "332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_0" -> "333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_1" -> "321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1" -> "338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1" -> "339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"321 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_1" -> "340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_2" -> "323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2" -> "348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2116 /nncf_model_output_7"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"325 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"330 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___0" -> "337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___0" -> "335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"335 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_0" -> "336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___1" -> "337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___0" -> "351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___2" -> "344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__neg___1" -> "342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/cat_1" -> "343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__mul___3" -> "344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1" -> "345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__add___1" -> "2115 /nncf_model_output_6"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_0" -> "347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_0" -> "352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"348 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/expand_1" -> "350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/reshape_1" -> "353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"352 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/transpose_3" -> "356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"356 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/view_3" -> "358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"359 layers.3.self_attn.o_proj.weight" -> "360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"360 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___0" -> "363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "367 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"367 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"371 layers.3.post_attention_layernorm.weight" -> "372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/to_0" -> "374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"375 layers.3.mlp.gate_proj.weight" -> "376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"380 layers.3.mlp.up_proj.weight" -> "381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/__mul___0" -> "384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"385 layers.3.mlp.down_proj.weight" -> "386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[3]/__add___1" -> "389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/to_0" -> "390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_0" -> "391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1" -> "392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1" -> "396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_1" -> "457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/pow_0" -> "393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/mean_0" -> "394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__add___0" -> "395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/to_2" -> "399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"398 layers.4.input_layernorm.weight" -> "399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/to_0" -> "401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"402 layers.4.self_attn.q_proj.weight" -> "403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"404 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"406 layers.4.self_attn.k_proj.weight" -> "407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"410 layers.4.self_attn.v_proj.weight" -> "411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_0" -> "414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0" -> "426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0" -> "427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_0" -> "428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_1" -> "416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1" -> "433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1" -> "434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"416 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_1" -> "435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_2" -> "418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2" -> "443 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2118 /nncf_model_output_9"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"420 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"425 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___0" -> "432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___0" -> "430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"430 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_0" -> "431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___1" -> "432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___0" -> "446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___2" -> "439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__neg___1" -> "437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/cat_1" -> "438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__mul___3" -> "439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1" -> "440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__add___1" -> "2117 /nncf_model_output_8"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_0" -> "442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_0" -> "447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"443 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/expand_1" -> "445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/reshape_1" -> "448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"447 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/transpose_3" -> "451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"451 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/view_3" -> "453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"454 layers.4.self_attn.o_proj.weight" -> "455 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"455 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___0" -> "458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"466 layers.4.post_attention_layernorm.weight" -> "467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/to_0" -> "469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"470 layers.4.mlp.gate_proj.weight" -> "471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"475 layers.4.mlp.up_proj.weight" -> "476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/__mul___0" -> "479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"480 layers.4.mlp.down_proj.weight" -> "481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[4]/__add___1" -> "484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/to_0" -> "485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_0" -> "486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1" -> "487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1" -> "491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_1" -> "552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/pow_0" -> "488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/mean_0" -> "489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__add___0" -> "490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/to_2" -> "494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"493 layers.5.input_layernorm.weight" -> "494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/to_0" -> "496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"497 layers.5.self_attn.q_proj.weight" -> "498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"499 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"501 layers.5.self_attn.k_proj.weight" -> "502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"505 layers.5.self_attn.v_proj.weight" -> "506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_0" -> "509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0" -> "521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0" -> "522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_0" -> "523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_1" -> "511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1" -> "528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1" -> "529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"511 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_1" -> "530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_2" -> "513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2" -> "538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2120 /nncf_model_output_11"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"515 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"520 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___0" -> "527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___0" -> "525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"525 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_0" -> "526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___1" -> "527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___0" -> "541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___2" -> "534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__neg___1" -> "532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/cat_1" -> "533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__mul___3" -> "534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1" -> "535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__add___1" -> "2119 /nncf_model_output_10"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_0" -> "537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_0" -> "542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"538 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/expand_1" -> "540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/reshape_1" -> "543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"542 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/transpose_3" -> "546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"546 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/view_3" -> "548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"549 layers.5.self_attn.o_proj.weight" -> "550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"550 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___0" -> "553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"561 layers.5.post_attention_layernorm.weight" -> "562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/to_0" -> "564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"565 layers.5.mlp.gate_proj.weight" -> "566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"570 layers.5.mlp.up_proj.weight" -> "571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/__mul___0" -> "574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"575 layers.5.mlp.down_proj.weight" -> "576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[5]/__add___1" -> "579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/to_0" -> "580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_0" -> "581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1" -> "582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1" -> "586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_1" -> "647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/pow_0" -> "583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/mean_0" -> "584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__add___0" -> "585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/to_2" -> "589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"588 layers.6.input_layernorm.weight" -> "589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/to_0" -> "591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"592 layers.6.self_attn.q_proj.weight" -> "593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"594 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"596 layers.6.self_attn.k_proj.weight" -> "597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "605 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"600 layers.6.self_attn.v_proj.weight" -> "601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_0" -> "604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0" -> "616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0" -> "617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_0" -> "618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"605 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_1" -> "606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1" -> "623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1" -> "624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"606 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_1" -> "625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_2" -> "608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2" -> "633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2122 /nncf_model_output_13"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"610 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"615 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___0" -> "622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___0" -> "620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"620 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_0" -> "621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___1" -> "622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___0" -> "636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___2" -> "629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__neg___1" -> "627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/cat_1" -> "628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__mul___3" -> "629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1" -> "630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__add___1" -> "2121 /nncf_model_output_12"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_0" -> "632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_0" -> "637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"633 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/expand_1" -> "635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/reshape_1" -> "638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"637 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/transpose_3" -> "641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"641 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/view_3" -> "643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"644 layers.6.self_attn.o_proj.weight" -> "645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"645 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___0" -> "648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"656 layers.6.post_attention_layernorm.weight" -> "657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/to_0" -> "659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"660 layers.6.mlp.gate_proj.weight" -> "661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"665 layers.6.mlp.up_proj.weight" -> "666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/__mul___0" -> "669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"670 layers.6.mlp.down_proj.weight" -> "671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[6]/__add___1" -> "674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/to_0" -> "675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_0" -> "676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1" -> "677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1" -> "681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_1" -> "742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/pow_0" -> "678 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"678 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/mean_0" -> "679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__add___0" -> "680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/to_2" -> "684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"683 layers.7.input_layernorm.weight" -> "684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/to_0" -> "686 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"686 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "689 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"686 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"687 layers.7.self_attn.q_proj.weight" -> "688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "689 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"689 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"691 layers.7.self_attn.k_proj.weight" -> "692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"695 layers.7.self_attn.v_proj.weight" -> "696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_0" -> "699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0" -> "711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0" -> "712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_0" -> "713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_1" -> "701 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"701 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1" -> "718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"701 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1" -> "719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"701 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_1" -> "720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_2" -> "703 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"703 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2" -> "728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"703 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2124 /nncf_model_output_15"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "705 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"705 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"710 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___0" -> "717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "715 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___0" -> "715 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"715 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_0" -> "716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___1" -> "717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___0" -> "731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___2" -> "724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__neg___1" -> "722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/cat_1" -> "723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__mul___3" -> "724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1" -> "725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__add___1" -> "2123 /nncf_model_output_14"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_0" -> "727 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"727 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_0" -> "732 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"728 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "729 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"729 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/expand_1" -> "730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/reshape_1" -> "733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"732 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "735 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"735 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/transpose_3" -> "736 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"736 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/view_3" -> "738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"739 layers.7.self_attn.o_proj.weight" -> "740 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"740 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___0" -> "743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "745 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"745 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "747 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"747 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"751 layers.7.post_attention_layernorm.weight" -> "752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/to_0" -> "754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "757 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "759 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"755 layers.7.mlp.gate_proj.weight" -> "756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "757 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"757 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"759 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"760 layers.7.mlp.up_proj.weight" -> "761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/__mul___0" -> "764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"765 layers.7.mlp.down_proj.weight" -> "766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[7]/__add___1" -> "769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/to_0" -> "770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_0" -> "771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1" -> "772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1" -> "776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_1" -> "837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/pow_0" -> "773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/mean_0" -> "774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__add___0" -> "775 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"775 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "777 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"777 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/to_2" -> "779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"778 layers.8.input_layernorm.weight" -> "779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/to_0" -> "781 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"781 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"781 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"782 layers.8.self_attn.q_proj.weight" -> "783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"784 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"786 layers.8.self_attn.k_proj.weight" -> "787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"790 layers.8.self_attn.v_proj.weight" -> "791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_0" -> "794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0" -> "806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0" -> "807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_0" -> "808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_1" -> "796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1" -> "813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1" -> "814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"796 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_1" -> "815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_2" -> "798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2" -> "823 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2126 /nncf_model_output_17"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "800 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"800 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "805 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"805 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"805 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "818 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___0" -> "812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___0" -> "810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"810 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_0" -> "811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___1" -> "812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___0" -> "826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___2" -> "819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__neg___1" -> "817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/cat_1" -> "818 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"818 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__mul___3" -> "819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1" -> "820 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__add___1" -> "2125 /nncf_model_output_16"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"820 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_0" -> "822 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"822 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_0" -> "827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"823 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/expand_1" -> "825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/reshape_1" -> "828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"827 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/transpose_3" -> "831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"831 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/view_3" -> "833 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"833 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"834 layers.8.self_attn.o_proj.weight" -> "835 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"835 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___0" -> "838 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"838 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"846 layers.8.post_attention_layernorm.weight" -> "847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/to_0" -> "849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"850 layers.8.mlp.gate_proj.weight" -> "851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "853 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"853 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"855 layers.8.mlp.up_proj.weight" -> "856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/__mul___0" -> "859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"860 layers.8.mlp.down_proj.weight" -> "861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[8]/__add___1" -> "864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/to_0" -> "865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_0" -> "866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1" -> "867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1" -> "871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_1" -> "932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/pow_0" -> "868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/mean_0" -> "869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__add___0" -> "870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/to_2" -> "874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"873 layers.9.input_layernorm.weight" -> "874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "875 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"875 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/to_0" -> "876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"877 layers.9.self_attn.q_proj.weight" -> "878 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"878 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"879 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"881 layers.9.self_attn.k_proj.weight" -> "882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"885 layers.9.self_attn.v_proj.weight" -> "886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_0" -> "889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0" -> "901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0" -> "902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_0" -> "903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_1" -> "891 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"891 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1" -> "908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"891 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1" -> "909 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"891 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_1" -> "910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_2" -> "893 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"893 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2" -> "918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"893 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2128 /nncf_model_output_19"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "895 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"895 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "897 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"897 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "899 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"899 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"899 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"900 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___0" -> "907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "905 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___0" -> "905 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"905 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_0" -> "906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___1" -> "907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___0" -> "921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___2" -> "914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"909 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__neg___1" -> "912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/cat_1" -> "913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__mul___3" -> "914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1" -> "915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__add___1" -> "2127 /nncf_model_output_18"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_0" -> "917 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"917 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_0" -> "922 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"918 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/expand_1" -> "920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/reshape_1" -> "923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"922 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/transpose_3" -> "926 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"926 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "927 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"927 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/view_3" -> "928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "931 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"929 layers.9.self_attn.o_proj.weight" -> "930 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"930 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "931 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"931 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___0" -> "933 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"933 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"941 layers.9.post_attention_layernorm.weight" -> "942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/to_0" -> "944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"945 layers.9.mlp.gate_proj.weight" -> "946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "948 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"948 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"950 layers.9.mlp.up_proj.weight" -> "951 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"951 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/__mul___0" -> "954 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"954 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"955 layers.9.mlp.down_proj.weight" -> "956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[9]/__add___1" -> "959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/to_0" -> "960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_0" -> "961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1" -> "962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1" -> "966 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_1" -> "1027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/pow_0" -> "963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/mean_0" -> "964 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"964 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__add___0" -> "965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "966 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"966 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/to_2" -> "969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"968 layers.10.input_layernorm.weight" -> "969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "970 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"970 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/to_0" -> "971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "974 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"972 layers.10.self_attn.q_proj.weight" -> "973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "974 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"974 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"976 layers.10.self_attn.k_proj.weight" -> "977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "985 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"980 layers.10.self_attn.v_proj.weight" -> "981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "987 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_0" -> "984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0" -> "996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0" -> "997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_0" -> "998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"985 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_1" -> "986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"986 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"987 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_2" -> "988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1013 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2130 /nncf_model_output_21"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "990 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"990 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "993 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"993 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"995 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1000 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1000 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1000 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_0" -> "1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___0" -> "1016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1009 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1007 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1007 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1007 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/cat_1" -> "1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1009 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1009 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1" -> "1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1009 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__add___1" -> "2129 /nncf_model_output_20"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_0" -> "1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1017 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1013 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/expand_1" -> "1015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1017 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1021 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1021 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/view_3" -> "1023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1024 layers.10.self_attn.o_proj.weight" -> "1025 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1025 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___0" -> "1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1036 layers.10.post_attention_layernorm.weight" -> "1037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/to_0" -> "1039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1040 layers.10.mlp.gate_proj.weight" -> "1041 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1041 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1043 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1043 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1045 layers.10.mlp.up_proj.weight" -> "1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/__mul___0" -> "1049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1050 layers.10.mlp.down_proj.weight" -> "1051 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"1051 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[10]/__add___1" -> "1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/to_0" -> "1055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_0" -> "1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1" -> "1057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1" -> "1061 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_1" -> "1122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1061 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1061 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/to_2" -> "1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1063 layers.11.input_layernorm.weight" -> "1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/to_0" -> "1066 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1066 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1069 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1066 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1067 layers.11.self_attn.q_proj.weight" -> "1068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1069 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1069 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1071 layers.11.self_attn.k_proj.weight" -> "1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1075 layers.11.self_attn.v_proj.weight" -> "1076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_0" -> "1079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_1" -> "1081 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1081 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1081 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1081 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_2" -> "1083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2132 /nncf_model_output_23"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1085 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1085 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1090 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1094 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1094 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1095 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_0" -> "1096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___0" -> "1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1099 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1100 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1101 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1102 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/cat_1" -> "1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1103 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1" -> "1105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1104 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__add___1" -> "2131 /nncf_model_output_22"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1105 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1106 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_0" -> "1107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1107 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1108 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1109 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/expand_1" -> "1110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1110 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1111 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1112 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1113 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1114 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1115 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1116 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1117 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/view_3" -> "1118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1118 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1119 layers.11.self_attn.o_proj.weight" -> "1120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1120 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1121 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1122 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___0" -> "1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1123 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1124 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1125 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1126 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1127 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1128 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1129 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1130 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1131 layers.11.post_attention_layernorm.weight" -> "1132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1132 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1133 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/to_0" -> "1134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1134 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1135 layers.11.mlp.gate_proj.weight" -> "1136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1136 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1137 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1138 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1139 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1140 layers.11.mlp.up_proj.weight" -> "1141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1141 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1142 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1143 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/__mul___0" -> "1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1144 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1145 layers.11.mlp.down_proj.weight" -> "1146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"1146 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1147 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1148 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[11]/__add___1" -> "1149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1149 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/to_0" -> "1150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1150 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_0" -> "1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1" -> "1152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1" -> "1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1151 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_1" -> "1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1152 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1153 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1154 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1155 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1156 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1157 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/to_2" -> "1159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1158 layers.12.input_layernorm.weight" -> "1159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1159 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1160 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/to_0" -> "1161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1161 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1162 layers.12.self_attn.q_proj.weight" -> "1163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1163 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1164 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1165 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1166 layers.12.self_attn.k_proj.weight" -> "1167 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1167 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1168 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1169 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1170 layers.12.self_attn.v_proj.weight" -> "1171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1171 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1172 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1173 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_0" -> "1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1174 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1175 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_1" -> "1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1176 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1177 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_2" -> "1178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1178 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2134 /nncf_model_output_25"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1179 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1180 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1181 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1182 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1183 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1184 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1185 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1186 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1187 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1188 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1189 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1190 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_0" -> "1191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1191 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1192 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___0" -> "1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1193 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1194 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1195 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1196 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1197 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/cat_1" -> "1198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1198 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1" -> "1200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1199 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__add___1" -> "2133 /nncf_model_output_24"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1200 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1201 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_0" -> "1202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1202 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1203 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1204 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/expand_1" -> "1205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1205 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1206 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1207 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1208 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1209 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1210 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1211 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1212 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/view_3" -> "1213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1213 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1214 layers.12.self_attn.o_proj.weight" -> "1215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1215 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1216 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1217 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___0" -> "1218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1218 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1219 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1220 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1221 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1222 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1223 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1224 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1225 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1226 layers.12.post_attention_layernorm.weight" -> "1227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1227 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1228 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/to_0" -> "1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1229 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1230 layers.12.mlp.gate_proj.weight" -> "1231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1231 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1232 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1233 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1234 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1235 layers.12.mlp.up_proj.weight" -> "1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1236 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1237 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1238 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/__mul___0" -> "1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1239 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1240 layers.12.mlp.down_proj.weight" -> "1241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"1241 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1242 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1243 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[12]/__add___1" -> "1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1244 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/to_0" -> "1245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1245 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_0" -> "1246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1" -> "1247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1" -> "1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1246 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_1" -> "1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1247 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1248 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1249 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1250 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1251 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1252 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/to_2" -> "1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1253 layers.13.input_layernorm.weight" -> "1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1254 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1255 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/to_0" -> "1256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1256 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1257 layers.13.self_attn.q_proj.weight" -> "1258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1258 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1259 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1260 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1261 layers.13.self_attn.k_proj.weight" -> "1262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1262 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1263 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1264 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1265 layers.13.self_attn.v_proj.weight" -> "1266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1266 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1267 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1268 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_0" -> "1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1269 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1270 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_1" -> "1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1271 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1272 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_2" -> "1273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1273 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2136 /nncf_model_output_27"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1274 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1275 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1276 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1277 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1278 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1279 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1280 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1281 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1282 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1283 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1284 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1285 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_0" -> "1286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1286 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1287 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___0" -> "1301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1288 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1289 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1290 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1291 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1291 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1292 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/cat_1" -> "1293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1293 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1" -> "1295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1294 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__add___1" -> "2135 /nncf_model_output_26"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1295 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1296 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_0" -> "1297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1297 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1298 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1299 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/expand_1" -> "1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1300 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1301 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1302 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1303 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1304 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1305 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1306 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1307 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1307 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/view_3" -> "1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1308 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1311 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1309 layers.13.self_attn.o_proj.weight" -> "1310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1310 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1311 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1311 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1312 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___0" -> "1313 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1313 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1314 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1315 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1316 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1317 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1318 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1319 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1320 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1321 layers.13.post_attention_layernorm.weight" -> "1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1322 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1323 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/to_0" -> "1324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1324 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1325 layers.13.mlp.gate_proj.weight" -> "1326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1326 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1327 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1328 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1329 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1330 layers.13.mlp.up_proj.weight" -> "1331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1331 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1332 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1333 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/__mul___0" -> "1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1334 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "1337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1335 layers.13.mlp.down_proj.weight" -> "1336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"1336 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1337 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1338 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[13]/__add___1" -> "1339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1339 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/to_0" -> "1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1340 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_0" -> "1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1" -> "1342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1" -> "1346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1341 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_1" -> "1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1342 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1343 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1344 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1345 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1346 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1347 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/to_2" -> "1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1348 layers.14.input_layernorm.weight" -> "1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1349 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1350 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/to_0" -> "1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1351 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1352 layers.14.self_attn.q_proj.weight" -> "1353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1353 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1354 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1355 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1356 layers.14.self_attn.k_proj.weight" -> "1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1357 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1358 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1359 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1360 layers.14.self_attn.v_proj.weight" -> "1361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1361 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1362 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1367 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1363 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_0" -> "1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1364 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1365 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_1" -> "1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1366 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1367 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_2" -> "1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1368 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2138 /nncf_model_output_29"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1369 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1370 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1371 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1372 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1373 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1374 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1375 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1376 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1377 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1378 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1379 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1380 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_0" -> "1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1381 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1382 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___0" -> "1396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1383 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1384 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1385 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1386 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1387 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/cat_1" -> "1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1388 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1" -> "1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1389 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__add___1" -> "2137 /nncf_model_output_28"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1390 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1391 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_0" -> "1392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1392 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1393 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1394 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/expand_1" -> "1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1395 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1396 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1397 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1398 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1399 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1400 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1401 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1402 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/view_3" -> "1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1403 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1404 layers.14.self_attn.o_proj.weight" -> "1405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1405 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1406 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1407 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___0" -> "1408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1408 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1409 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1410 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1411 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1412 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1413 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1414 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1415 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1416 layers.14.post_attention_layernorm.weight" -> "1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1417 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1418 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/to_0" -> "1419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1419 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1420 layers.14.mlp.gate_proj.weight" -> "1421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1421 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1422 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1423 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1424 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1425 layers.14.mlp.up_proj.weight" -> "1426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1426 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1427 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1428 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/__mul___0" -> "1429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1429 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1430 layers.14.mlp.down_proj.weight" -> "1431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"1431 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1432 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1433 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[14]/__add___1" -> "1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1434 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/to_0" -> "1435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1435 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_0" -> "1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1" -> "1437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1" -> "1441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1436 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_1" -> "1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1437 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1438 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1439 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1440 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1441 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1442 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/to_2" -> "1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1443 layers.15.input_layernorm.weight" -> "1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1444 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1445 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/to_0" -> "1446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1446 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1447 layers.15.self_attn.q_proj.weight" -> "1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1448 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1449 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1450 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1451 layers.15.self_attn.k_proj.weight" -> "1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1452 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1453 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1454 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "1457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1455 layers.15.self_attn.v_proj.weight" -> "1456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1456 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1457 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1458 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_0" -> "1459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1459 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1460 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_1" -> "1461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1461 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1462 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_2" -> "1463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1463 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2140 /nncf_model_output_31"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1464 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1465 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1466 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1467 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1468 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1469 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1470 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1471 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1472 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1475 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1473 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1474 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1475 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1475 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_0" -> "1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1476 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1477 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___0" -> "1491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1478 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1479 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1480 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1481 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1482 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/cat_1" -> "1483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1483 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1" -> "1485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1484 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__add___1" -> "2139 /nncf_model_output_30"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1485 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1486 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_0" -> "1487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1487 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1488 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1489 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/expand_1" -> "1490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1490 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1491 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1492 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1493 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1494 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1495 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1496 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1497 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/view_3" -> "1498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1498 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1499 layers.15.self_attn.o_proj.weight" -> "1500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1500 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1501 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1502 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___0" -> "1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1503 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1504 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1505 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1506 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1507 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1508 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1509 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1510 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1511 layers.15.post_attention_layernorm.weight" -> "1512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1512 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1513 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/to_0" -> "1514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1514 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1515 layers.15.mlp.gate_proj.weight" -> "1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1516 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1517 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1518 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1519 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1520 layers.15.mlp.up_proj.weight" -> "1521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1521 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1522 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1523 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/__mul___0" -> "1524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1524 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1525 layers.15.mlp.down_proj.weight" -> "1526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"1526 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1527 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1528 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[15]/__add___1" -> "1529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1529 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/to_0" -> "1530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1530 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_0" -> "1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1" -> "1532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1" -> "1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1531 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_1" -> "1597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1532 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1533 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1534 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1535 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1536 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1537 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/to_2" -> "1539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1538 layers.16.input_layernorm.weight" -> "1539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1539 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1540 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/to_0" -> "1541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1541 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1542 layers.16.self_attn.q_proj.weight" -> "1543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1543 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1544 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1545 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1546 layers.16.self_attn.k_proj.weight" -> "1547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1547 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1548 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1549 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "1552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1550 layers.16.self_attn.v_proj.weight" -> "1551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1551 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1552 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1553 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_0" -> "1554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1554 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1555 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_1" -> "1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1556 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1557 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_2" -> "1558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1558 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2142 /nncf_model_output_33"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1559 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1560 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1561 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1562 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1563 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1564 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1565 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1566 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1567 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1568 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1569 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1570 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_0" -> "1571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1571 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1572 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___0" -> "1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1573 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1574 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1575 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1576 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1577 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/cat_1" -> "1578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1578 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1" -> "1580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1579 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__add___1" -> "2141 /nncf_model_output_32"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1580 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1581 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_0" -> "1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1582 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1583 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1584 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/expand_1" -> "1585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1585 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1586 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1587 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1588 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1589 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1590 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1591 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1592 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/view_3" -> "1593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1593 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1594 layers.16.self_attn.o_proj.weight" -> "1595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1595 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1596 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1597 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___0" -> "1598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1598 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1599 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1600 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1601 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1602 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1603 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1604 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1605 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1605 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1606 layers.16.post_attention_layernorm.weight" -> "1607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1607 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1608 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/to_0" -> "1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1609 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1610 layers.16.mlp.gate_proj.weight" -> "1611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1611 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1612 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1613 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1614 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1615 layers.16.mlp.up_proj.weight" -> "1616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1616 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1617 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1618 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/__mul___0" -> "1619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1619 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "1622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1620 layers.16.mlp.down_proj.weight" -> "1621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"1621 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1622 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1623 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[16]/__add___1" -> "1624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1624 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/to_0" -> "1625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1625 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_0" -> "1626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1" -> "1627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1" -> "1631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1626 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_1" -> "1692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1627 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1628 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1629 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1630 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1631 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1632 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/to_2" -> "1634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1633 layers.17.input_layernorm.weight" -> "1634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1634 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1635 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/to_0" -> "1636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1636 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1637 layers.17.self_attn.q_proj.weight" -> "1638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1638 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1639 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1640 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1641 layers.17.self_attn.k_proj.weight" -> "1642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1642 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1643 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1644 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "1647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1645 layers.17.self_attn.v_proj.weight" -> "1646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1646 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1647 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1648 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_0" -> "1649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1649 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1650 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_1" -> "1651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1651 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1670 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1652 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_2" -> "1653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1678 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1653 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2144 /nncf_model_output_35"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1654 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1655 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1656 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1657 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1658 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1659 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1660 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1661 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1662 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1663 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1664 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1665 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_0" -> "1666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1666 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1667 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___0" -> "1681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1668 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1669 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1670 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1671 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1672 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/cat_1" -> "1673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1673 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1" -> "1675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1674 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__add___1" -> "2143 /nncf_model_output_34"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1675 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1676 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_0" -> "1677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1677 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1678 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1679 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/expand_1" -> "1680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1680 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1683 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1681 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1682 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1683 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1684 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1685 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1686 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1686 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1687 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1687 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/view_3" -> "1688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1688 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1689 layers.17.self_attn.o_proj.weight" -> "1690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1690 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1691 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1692 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___0" -> "1693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1693 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1695 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1694 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1695 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1696 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1697 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1698 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1699 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1700 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1701 layers.17.post_attention_layernorm.weight" -> "1702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1702 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1703 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1703 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/to_0" -> "1704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1704 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1705 layers.17.mlp.gate_proj.weight" -> "1706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1706 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1707 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1708 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1709 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1710 layers.17.mlp.up_proj.weight" -> "1711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1711 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1712 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1713 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/__mul___0" -> "1714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1714 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "1717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1715 layers.17.mlp.down_proj.weight" -> "1716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"1716 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1717 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1718 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[17]/__add___1" -> "1719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1719 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/to_0" -> "1720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1720 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_0" -> "1721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1" -> "1722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1" -> "1726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1721 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_1" -> "1787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1722 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1723 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1724 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1725 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1726 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1727 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1727 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/to_2" -> "1729 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1728 layers.18.input_layernorm.weight" -> "1729 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1729 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1730 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/to_0" -> "1731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1731 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1735 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1732 layers.18.self_attn.q_proj.weight" -> "1733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1733 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1734 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1735 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1735 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1739 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1736 layers.18.self_attn.k_proj.weight" -> "1737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1737 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1738 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1745 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1739 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "1742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1740 layers.18.self_attn.v_proj.weight" -> "1741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1741 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1742 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1747 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1743 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_0" -> "1744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1757 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1744 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1745 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_1" -> "1746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1746 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1765 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1747 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_2" -> "1748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1748 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2146 /nncf_model_output_37"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1749 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1751 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1750 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1751 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1752 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1753 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1754 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1755 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1756 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1757 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1758 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1759 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1759 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1760 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_0" -> "1761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1761 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1762 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___0" -> "1776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1763 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1764 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1765 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1766 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1767 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/cat_1" -> "1768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1768 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1" -> "1770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1769 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__add___1" -> "2145 /nncf_model_output_36"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1770 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1771 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_0" -> "1772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1772 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1777 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1773 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1774 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/expand_1" -> "1775 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1775 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1776 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1777 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1778 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1779 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1780 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1781 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1781 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1782 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1782 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/view_3" -> "1783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1783 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1784 layers.18.self_attn.o_proj.weight" -> "1785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1785 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1786 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1787 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___0" -> "1788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1788 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1790 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1789 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1790 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1791 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1792 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1793 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1794 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1795 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1796 layers.18.post_attention_layernorm.weight" -> "1797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1797 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1798 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/to_0" -> "1799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1799 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1800 layers.18.mlp.gate_proj.weight" -> "1801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1801 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1802 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1803 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1804 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1805 layers.18.mlp.up_proj.weight" -> "1806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1806 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1807 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1808 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/__mul___0" -> "1809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1809 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "1812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1810 layers.18.mlp.down_proj.weight" -> "1811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"1811 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1812 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1813 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[18]/__add___1" -> "1814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1814 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/to_0" -> "1815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1815 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_0" -> "1816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1" -> "1817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1" -> "1821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1816 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_1" -> "1882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1817 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1818 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1818 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1819 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1820 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1820 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1821 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1822 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1822 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/to_2" -> "1824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1823 layers.19.input_layernorm.weight" -> "1824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1824 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1825 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/to_0" -> "1826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1826 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1827 layers.19.self_attn.q_proj.weight" -> "1828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1828 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1829 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1838 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1833 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1830 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1831 layers.19.self_attn.k_proj.weight" -> "1832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1832 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1833 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1833 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1834 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "1837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1835 layers.19.self_attn.v_proj.weight" -> "1836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1836 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1837 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1838 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_0" -> "1839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1839 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1853 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1840 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_1" -> "1841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1841 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1842 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_2" -> "1843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1843 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2148 /nncf_model_output_39"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1844 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1846 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1845 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1846 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1847 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1848 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1849 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1850 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1851 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1852 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1855 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1853 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1854 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1855 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1855 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_0" -> "1856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1856 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1857 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___0" -> "1871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1858 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1859 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1860 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1861 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1862 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/cat_1" -> "1863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1863 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1" -> "1865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1864 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__add___1" -> "2147 /nncf_model_output_38"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1865 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1866 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_0" -> "1867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1867 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1868 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1869 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/expand_1" -> "1870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1870 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1873 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1871 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1872 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1873 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1874 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1875 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1875 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1876 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1877 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1877 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/view_3" -> "1878 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1878 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1881 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1879 layers.19.self_attn.o_proj.weight" -> "1880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1880 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1881 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1881 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1882 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___0" -> "1883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1883 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1884 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1885 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1886 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1887 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1888 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1889 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1890 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1891 layers.19.post_attention_layernorm.weight" -> "1892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1892 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1893 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1893 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/to_0" -> "1894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1897 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1894 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1899 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1895 layers.19.mlp.gate_proj.weight" -> "1896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1896 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1897 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1897 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1898 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1899 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1900 layers.19.mlp.up_proj.weight" -> "1901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1901 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1902 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1903 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/__mul___0" -> "1904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1904 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "1907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1905 layers.19.mlp.down_proj.weight" -> "1906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"1906 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"1907 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "1908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1908 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[19]/__add___1" -> "1909 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1909 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/to_0" -> "1910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1910 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_0" -> "1911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1" -> "1912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1" -> "1916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1911 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_1" -> "1977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1912 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/pow_0" -> "1913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1913 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/mean_0" -> "1914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1914 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__add___0" -> "1915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1915 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "1916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1916 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "1917 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1917 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/to_2" -> "1919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1918 layers.20.input_layernorm.weight" -> "1919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1919 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "1920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1920 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/to_0" -> "1921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1921 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "1925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1922 layers.20.self_attn.q_proj.weight" -> "1923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1923 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1924 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "1933 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1925 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "1929 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1926 layers.20.self_attn.k_proj.weight" -> "1927 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1927 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1928 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "1935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1929 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "1932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1930 layers.20.self_attn.v_proj.weight" -> "1931 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"1931 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"1932 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "1937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"1933 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_0" -> "1934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1934 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_0" -> "1948 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1935 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_1" -> "1936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1954 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1936 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_1" -> "1955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1937 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_2" -> "1938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"1938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2" -> "1963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1938 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2150 /nncf_model_output_41"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1939 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "1941 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1940 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "1942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1941 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "1943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1942 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "1944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1943 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "1945 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"1944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1944 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "1953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1945 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1951 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1945 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "1958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"1946 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___0" -> "1952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1947 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "1950 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"1948 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "1949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1949 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___0" -> "1950 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"1950 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_0" -> "1951 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1951 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___1" -> "1952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1952 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___0" -> "1966 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1953 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___2" -> "1959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1954 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "1957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"1955 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "1956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1956 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__neg___1" -> "1957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"1957 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/cat_1" -> "1958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1958 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__mul___3" -> "1959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"1959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1" -> "1960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1959 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__add___1" -> "2149 /nncf_model_output_40"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"1960 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "1961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1961 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_0" -> "1962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1962 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_0" -> "1967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1963 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "1964 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"1964 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/expand_1" -> "1965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"1965 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/reshape_1" -> "1968 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1966 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "1969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1967 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "1969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"1968 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "1969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"1969 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "1970 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"1970 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/transpose_3" -> "1971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1971 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "1972 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"1972 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/view_3" -> "1973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1973 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "1976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1974 layers.20.self_attn.o_proj.weight" -> "1975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"1975 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"1976 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "1977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1977 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___0" -> "1978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1978 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "1979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1980 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "1984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1979 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "2003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1980 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "1981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1981 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "1982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1982 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "1983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"1983 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "1984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"1984 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "1985 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1985 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "1987 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"1986 layers.20.post_attention_layernorm.weight" -> "1987 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"1987 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "1988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1988 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/to_0" -> "1989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1989 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "1994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1990 layers.20.mlp.gate_proj.weight" -> "1991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1991 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1992 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "1993 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1993 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "1998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1994 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "1997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"1995 layers.20.mlp.up_proj.weight" -> "1996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"1996 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "1997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"1997 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "1998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"1998 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/__mul___0" -> "1999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"1999 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "2002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"2000 layers.20.mlp.down_proj.weight" -> "2001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"2001 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "2002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"2002 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "2003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"2003 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[20]/__add___1" -> "2004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2004 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/to_0" -> "2005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2005 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_0" -> "2006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1" -> "2007 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1" -> "2011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2006 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_1" -> "2072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2007 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/pow_0" -> "2008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2008 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/mean_0" -> "2009 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"2009 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__add___0" -> "2010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"2010 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/rsqrt_0" -> "2011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"2011 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___0" -> "2012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2012 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/to_2" -> "2014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"2013 layers.21.input_layernorm.weight" -> "2014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"2014 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[input_layernorm]/__mul___1" -> "2015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2015 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/to_0" -> "2016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "2019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2016 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/to_0" -> "2020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2017 layers.21.self_attn.q_proj.weight" -> "2018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"2018 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "2019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"2019 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[q_proj]/linear_0" -> "2028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "2023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2020 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/to_0" -> "2024 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2021 layers.21.self_attn.k_proj.weight" -> "2022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"2022 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "2023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"2023 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[k_proj]/linear_0" -> "2030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_1"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"2024 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/to_0" -> "2027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2025 layers.21.self_attn.v_proj.weight" -> "2026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(256, 2048) \n0 -> 0", style=solid];
"2026 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "2027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0"  [label="(256, 2048) \n0 -> 1", style=solid];
"2027 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[v_proj]/linear_0" -> "2032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_2"  [label="(1, 21, 256) \n0 -> 0", style=solid];
"2028 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_0" -> "2029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"2029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0" -> "2041 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"2029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0" -> "2042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"2029 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_0" -> "2043 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"2030 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_1" -> "2031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"2031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1" -> "2048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"2031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1" -> "2049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___2"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"2031 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_1" -> "2050 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"2032 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_2" -> "2033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2"  [label="(1, 21, 4, 64) \n0 -> 0", style=solid];
"2033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___5"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"2033 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_2" -> "2152 /nncf_model_output_43"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"2034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "2035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"2034 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cat_0" -> "2036 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"2035 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/cos_0" -> "2037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"2036 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/sin_0" -> "2038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"2037 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_0" -> "2039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"2038 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/LlamaRotaryEmbedding[rotary_emb]/to_1" -> "2040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1"  [label="(1, 21, 64) \n0 -> 0", style=solid];
"2039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "2041 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"2039 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_0" -> "2048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"2040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "2046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"2040 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/unsqueeze_1" -> "2053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 1, 21, 64) \n0 -> 1", style=solid];
"2041 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___0" -> "2047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"2042 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___0" -> "2045 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 1", style=solid];
"2043 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___1" -> "2044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"2044 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___0" -> "2045 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0"  [label="(1, 32, 21, 32) \n0 -> 0", style=solid];
"2045 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_0" -> "2046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"2046 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___1" -> "2047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"2047 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___0" -> "2061 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"2048 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___2" -> "2054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"2049 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___2" -> "2052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 1", style=solid];
"2050 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___3" -> "2051 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"2051 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__neg___1" -> "2052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1"  [label="(1, 4, 21, 32) \n0 -> 0", style=solid];
"2052 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/cat_1" -> "2053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"2053 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__mul___3" -> "2054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1"  [label="(1, 4, 21, 64) \n0 -> 1", style=solid];
"2054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1" -> "2055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___4"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"2054 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__add___1" -> "2151 /nncf_model_output_42"  [label="(1, 4, 21, 64) \n0 -> 0", style=solid];
"2055 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___4" -> "2056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_0"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"2056 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_0" -> "2057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_0"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"2057 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_0" -> "2062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_1"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"2058 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/__getitem___5" -> "2059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_1"  [label="(1, 4, 1, 21, 64) \n0 -> 0", style=solid];
"2059 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/expand_1" -> "2060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_1"  [label="(1, 4, 8, 21, 64) \n0 -> 0", style=solid];
"2060 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/reshape_1" -> "2063 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_2"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"2061 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_0" -> "2064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"2062 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_1" -> "2064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 1", style=solid];
"2063 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_2" -> "2064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0"  [label="(1, 32, 21, 64) \n0 -> 2", style=solid];
"2064 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/scaled_dot_product_attention_0" -> "2065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_3"  [label="(1, 32, 21, 64) \n0 -> 0", style=solid];
"2065 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/transpose_3" -> "2066 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"2066 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/contiguous_3" -> "2067 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_3"  [label="(1, 21, 32, 64) \n0 -> 0", style=solid];
"2067 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/view_3" -> "2068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2068 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/to_0" -> "2071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2069 layers.21.self_attn.o_proj.weight" -> "2070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 2048) \n0 -> 0", style=solid];
"2070 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "2071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0"  [label="(2048, 2048) \n0 -> 1", style=solid];
"2071 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaSdpaAttention[self_attn]/Linear[o_proj]/linear_0" -> "2072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"2072 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___0" -> "2073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2073 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_0" -> "2074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "2075 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "2079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2074 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_1" -> "2098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2075 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/pow_0" -> "2076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2076 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/mean_0" -> "2077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"2077 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__add___0" -> "2078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"2078 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/rsqrt_0" -> "2079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"2079 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___0" -> "2080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2080 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/to_2" -> "2082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"2081 layers.21.post_attention_layernorm.weight" -> "2082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"2082 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaRMSNorm[post_attention_layernorm]/__mul___1" -> "2083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2083 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/to_0" -> "2084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "2087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2084 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/to_0" -> "2089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2085 layers.21.mlp.gate_proj.weight" -> "2086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"2086 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "2087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"2087 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[gate_proj]/linear_0" -> "2088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"2088 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/SiLU[act_fn]/silu_0" -> "2093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"2089 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/to_0" -> "2092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2090 layers.21.mlp.up_proj.weight" -> "2091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(5632, 2048) \n0 -> 0", style=solid];
"2091 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "2092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0"  [label="(5632, 2048) \n0 -> 1", style=solid];
"2092 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[up_proj]/linear_0" -> "2093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0"  [label="(1, 21, 5632) \n0 -> 1", style=solid];
"2093 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/__mul___0" -> "2094 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/to_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"2094 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/to_0" -> "2097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(1, 21, 5632) \n0 -> 0", style=solid];
"2095 layers.21.mlp.down_proj.weight" -> "2096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0"  [label="(2048, 5632) \n0 -> 0", style=solid];
"2096 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/AsymmetricQuantizer/asymmetric_quantize_0" -> "2097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0"  [label="(2048, 5632) \n0 -> 1", style=solid];
"2097 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/LlamaMLP[mlp]/Linear[down_proj]/linear_0" -> "2098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"2098 LlamaModel/ModuleList[layers]/LlamaDecoderLayer[21]/__add___1" -> "2099 LlamaModel/LlamaRMSNorm[norm]/to_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2099 LlamaModel/LlamaRMSNorm[norm]/to_0" -> "2100 LlamaModel/LlamaRMSNorm[norm]/to_1"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2100 LlamaModel/LlamaRMSNorm[norm]/to_1" -> "2101 LlamaModel/LlamaRMSNorm[norm]/pow_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2100 LlamaModel/LlamaRMSNorm[norm]/to_1" -> "2105 LlamaModel/LlamaRMSNorm[norm]/__mul___0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2101 LlamaModel/LlamaRMSNorm[norm]/pow_0" -> "2102 LlamaModel/LlamaRMSNorm[norm]/mean_0"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2102 LlamaModel/LlamaRMSNorm[norm]/mean_0" -> "2103 LlamaModel/LlamaRMSNorm[norm]/__add___0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"2103 LlamaModel/LlamaRMSNorm[norm]/__add___0" -> "2104 LlamaModel/LlamaRMSNorm[norm]/rsqrt_0"  [label="(1, 21, 1) \n0 -> 0", style=solid];
"2104 LlamaModel/LlamaRMSNorm[norm]/rsqrt_0" -> "2105 LlamaModel/LlamaRMSNorm[norm]/__mul___0"  [label="(1, 21, 1) \n0 -> 1", style=solid];
"2105 LlamaModel/LlamaRMSNorm[norm]/__mul___0" -> "2106 LlamaModel/LlamaRMSNorm[norm]/to_2"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
"2106 LlamaModel/LlamaRMSNorm[norm]/to_2" -> "2108 LlamaModel/LlamaRMSNorm[norm]/__mul___1"  [label="(1, 21, 2048) \n0 -> 1", style=solid];
"2107 norm.weight" -> "2108 LlamaModel/LlamaRMSNorm[norm]/__mul___1"  [label="(2048,) \n0 -> 0", style=solid];
"2108 LlamaModel/LlamaRMSNorm[norm]/__mul___1" -> "2153 /nncf_model_output_44"  [label="(1, 21, 2048) \n0 -> 0", style=solid];
}
