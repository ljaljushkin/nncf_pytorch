{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NNCF Quantization PyTorch Demo (tiny-imagenet/resnet-18",
   "provenance": [],
   "collapsed_sections": [
    "K5HPrY_d-7cV",
    "E01dMaR2_AFL",
    "qMnYsGo9_MA8",
    "L0tH9KdwtHhV"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "git68adWeq4l"
   },
   "source": [
    "#Intro\n",
    "\n",
    "This notebook is based on 'ImageNet training in PyTorch' [example](https://github.com/pytorch/examples/blob/master/imagenet/main.py).\n",
    "\n",
    "The goal of this notebook is not to reach the best possible baselines for the quantization, but to demonstrate simple use cases of [NNCF](https://github.com/openvinotoolkit/nncf) with Pytorch. For more advanced usage refer to these [examples](https://github.com/openvinotoolkit/nncf/tree/develop/examples)\n",
    "\n",
    "To make downloading and training fast, we suggest to use resnet-18 model with tiny-imagenet dataset. But it is possible to change it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M1xndNu-z_2"
   },
   "source": [
    "#Install pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtaM_i2mEB0z"
   },
   "source": [
    "Create a separate Python* virtual environment and install the following prerequisites into it:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EIo5S145S0Ug",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9a2db892-eb38-4863-dfdb-560aa12c8232"
   },
   "source": [
    "!pip install nncf[torch]\n",
    "!pip install openvino openvino-dev"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nncf[torch] in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
      "Requirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (1.4.2)\n",
      "Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (7.1.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (4.62.0)\n",
      "Requirement already satisfied: wheel>=0.36.1 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (0.36.2)\n",
      "Requirement already satisfied: jsonschema==3.2.0 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (3.2.0)\n",
      "Requirement already satisfied: jstyleson>=0.0.2 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (0.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (3.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (0.24.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (1.5.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (1.1.5)\n",
      "Requirement already satisfied: ninja>=1.10.0.post2 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (1.10.2)\n",
      "Requirement already satisfied: addict>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (2.4.0)\n",
      "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (2.5.1)\n",
      "Requirement already satisfied: texttable>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (1.6.4)\n",
      "Requirement already satisfied: torch!=1.8.0,<=1.8.1,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from nncf[torch]) (1.8.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema==3.2.0->nncf[torch]) (0.18.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from jsonschema==3.2.0->nncf[torch]) (57.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema==3.2.0->nncf[torch]) (1.15.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema==3.2.0->nncf[torch]) (21.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema==3.2.0->nncf[torch]) (4.6.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->nncf[torch]) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->nncf[torch]) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->nncf[torch]) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->nncf[torch]) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->nncf[torch]) (8.3.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.5->nncf[torch]) (4.4.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->nncf[torch]) (2018.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->nncf[torch]) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->nncf[torch]) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch!=1.8.0,<=1.8.1,>=1.5.0->nncf[torch]) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema==3.2.0->nncf[torch]) (3.5.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
      "Requirement already satisfied: openvino in /usr/local/lib/python3.7/dist-packages (2021.4.0)\n",
      "Requirement already satisfied: openvino-dev in /usr/local/lib/python3.7/dist-packages (2021.4.0)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from openvino) (1.19.5)\n",
      "Requirement already satisfied: texttable~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.6.4)\n",
      "Requirement already satisfied: networkx~=2.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (2.5.1)\n",
      "Requirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.7.1)\n",
      "Requirement already satisfied: hyperopt~=0.1.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.1.2)\n",
      "Requirement already satisfied: addict>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (2.4.0)\n",
      "Requirement already satisfied: yamlloader>=0.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.1.0)\n",
      "Requirement already satisfied: pandas~=1.1.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.1.5)\n",
      "Requirement already satisfied: nibabel>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (3.2.1)\n",
      "Requirement already satisfied: editdistance>=0.5.3 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.5.3)\n",
      "Requirement already satisfied: pydicom>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (2.1.2)\n",
      "Requirement already satisfied: jstyleson~=0.0.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.0.2)\n",
      "Requirement already satisfied: py-cpuinfo>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (8.0.0)\n",
      "Requirement already satisfied: progress>=1.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.6)\n",
      "Requirement already satisfied: opencv-python==4.5.* in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (4.5.3.56)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.24.2)\n",
      "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (4.62.0)\n",
      "Requirement already satisfied: scikit-image~=0.17.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.17.2)\n",
      "Requirement already satisfied: scipy~=1.5.4 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.5.4)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.7.1)\n",
      "Requirement already satisfied: fast-ctc-decode>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.3.0)\n",
      "Requirement already satisfied: tokenizers>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.10.3)\n",
      "Requirement already satisfied: parasail>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.2.4)\n",
      "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (2.26.0)\n",
      "Requirement already satisfied: pillow>=8.1.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (8.3.1)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (5.4.1)\n",
      "Requirement already satisfied: rawpy>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.16.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.95 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.1.96)\n",
      "Requirement already satisfied: nltk>=3.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (3.6.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt~=0.1.2->openvino-dev) (0.16.0)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt~=0.1.2->openvino-dev) (3.11.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt~=0.1.2->openvino-dev) (1.15.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx~=2.5->openvino-dev) (4.4.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from nibabel>=3.2.1->openvino-dev) (21.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.5->openvino-dev) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.5->openvino-dev) (7.1.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk>=3.5->openvino-dev) (2019.12.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->nibabel>=3.2.1->openvino-dev) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas~=1.1.5->openvino-dev) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas~=1.1.5->openvino-dev) (2.8.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->openvino-dev) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->openvino-dev) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->openvino-dev) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->openvino-dev) (1.24.3)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image~=0.17.2->openvino-dev) (2.4.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image~=0.17.2->openvino-dev) (1.1.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image~=0.17.2->openvino-dev) (2021.7.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image~=0.17.2->openvino-dev) (3.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image~=0.17.2->openvino-dev) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image~=0.17.2->openvino-dev) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->openvino-dev) (2.2.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HxsU71bEbLS"
   },
   "source": [
    "Import NNCF from your Python* code"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ya9XgA87DPoM"
   },
   "source": [
    "import os\n",
    "import time\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import torch\n",
    "import nncf  # Important - should be imported directly after torch\n",
    "from nncf import NNCFConfig\n",
    "from nncf.torch import create_compressed_model\n",
    "from nncf.torch import register_default_init_args\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ],
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fx4lCGIGNp28"
   },
   "source": [
    "Download Tiny ImageNet dataset\n",
    "* 100k images of shape 3x64x64\n",
    "* 200 different classes: snakes, spaiders, cats, trucks, grasshopper, gull, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gubHWwZELIRn"
   },
   "source": [
    "def download_tiny_imagenet_200(path,\n",
    "                        url='http://cs231n.stanford.edu/tiny-imagenet-200.zip',\n",
    "                        tarname='tiny-imagenet-200.zip'):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    archive_path = os.path.join(path, tarname)\n",
    "    urlretrieve(url, archive_path)\n",
    "    print(archive_path)\n",
    "    import zipfile\n",
    "    zip_ref = zipfile.ZipFile(archive_path, 'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()\n",
    "  \n",
    "DATASET_DIR = 'tiny-imagenet-200'\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    download_tiny_imagenet_200('.')"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4H9sI_uJOrT"
   },
   "source": [
    "Connect to google drive to save and get access to the pretrained model on tiny-imagenet dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q58vQSLWIwBp",
    "outputId": "ce194ed6-af92-48a4-90fe-18bd070b33ea"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eZX2GAh3W7ZT"
   },
   "source": [
    "# path to the saved model checkpoint\n",
    "# can be any\n",
    "PATH = '/content/drive/MyDrive/Colab Notebooks/nncf/'"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E01dMaR2_AFL"
   },
   "source": [
    "##Train function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "940rcAIyiXml"
   },
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':6.3f')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do opt step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print_frequency = 50\n",
    "        if i % print_frequency == 0:\n",
    "            progress.display(i)\n"
   ],
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoNr8qwm_El2"
   },
   "source": [
    "##Validate function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KgnugrWgicWC"
   },
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':6.3f')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            print_frequency = 10\n",
    "            if i % print_frequency == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "    return top1.avg"
   ],
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMnYsGo9_MA8"
   },
   "source": [
    "##Helpers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R724tbxcidQE"
   },
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ],
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kcSjyLBwiqBx"
   },
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = init_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ],
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "118rlV22_PB5"
   },
   "source": [
    "#Pre-train floating-point model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avCsioUYIaL7",
    "outputId": "183bdbb6-4016-463c-8d76-636a6b3a9778"
   },
   "source": [
    "num_classes = 200  # 200 is for tiny-imagenet, default is 1000 for imagenet\n",
    "init_lr = 1e-4\n",
    "batch_size = 256\n",
    "image_size = 64\n",
    "epochs = 4\n",
    "\n",
    "# create model\n",
    "model = models.resnet18(pretrained=True)\n",
    "# update the last FC layer for tiny-imagenet number of classes\n",
    "model.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "model.cuda()\n",
    "\n",
    "# Data loading code\n",
    "train_dir = os.path.join(DATASET_DIR, 'train')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "dataset = datasets.ImageFolder(\n",
    "    train_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=4, pin_memory=True, sampler=None)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=4, pin_memory=True)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "\n",
    "acc1 = 0\n",
    "# Training loop\n",
    "for epoch in range(0, epochs):\n",
    "    # run a single training epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion)\n",
    "print(f'Accuracy of FP32 model: {acc1:.3f}')"
   ],
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch: [0][  0/313]\tTime  0.945 ( 0.945)\tData  0.844 ( 0.844)\tLoss  5.665 ( 5.665)\tAcc@1   0.00 (  0.00)\tAcc@5   3.12 (  3.12)\n",
      "Epoch: [0][ 50/313]\tTime  0.133 ( 0.157)\tData  0.000 ( 0.048)\tLoss  4.283 ( 5.002)\tAcc@1  17.97 (  6.40)\tAcc@5  36.33 ( 15.91)\n",
      "Epoch: [0][100/313]\tTime  0.200 ( 0.153)\tData  0.140 ( 0.044)\tLoss  3.594 ( 4.453)\tAcc@1  21.88 ( 14.05)\tAcc@5  52.34 ( 30.28)\n",
      "Epoch: [0][150/313]\tTime  0.137 ( 0.153)\tData  0.004 ( 0.043)\tLoss  2.797 ( 4.047)\tAcc@1  37.89 ( 20.00)\tAcc@5  68.36 ( 39.62)\n",
      "Epoch: [0][200/313]\tTime  0.237 ( 0.152)\tData  0.182 ( 0.042)\tLoss  2.670 ( 3.747)\tAcc@1  39.06 ( 24.30)\tAcc@5  69.92 ( 46.04)\n",
      "Epoch: [0][250/313]\tTime  0.127 ( 0.150)\tData  0.006 ( 0.040)\tLoss  2.664 ( 3.526)\tAcc@1  38.28 ( 27.51)\tAcc@5  67.19 ( 50.38)\n",
      "Epoch: [0][300/313]\tTime  0.129 ( 0.148)\tData  0.010 ( 0.039)\tLoss  2.309 ( 3.348)\tAcc@1  48.05 ( 30.19)\tAcc@5  72.27 ( 53.77)\n",
      "Test: [ 0/79]\tTime  0.862 ( 0.862)\tLoss  2.188 ( 2.188)\tAcc@1  55.47 ( 55.47)\tAcc@5  73.83 ( 73.83)\n",
      "Test: [10/79]\tTime  0.041 ( 0.173)\tLoss  2.192 ( 2.296)\tAcc@1  51.56 ( 47.12)\tAcc@5  73.83 ( 73.58)\n",
      "Test: [20/79]\tTime  0.388 ( 0.161)\tLoss  2.276 ( 2.332)\tAcc@1  45.70 ( 45.18)\tAcc@5  73.44 ( 72.88)\n",
      "Test: [30/79]\tTime  0.037 ( 0.143)\tLoss  2.357 ( 2.344)\tAcc@1  45.70 ( 45.44)\tAcc@5  72.27 ( 72.58)\n",
      "Test: [40/79]\tTime  0.356 ( 0.142)\tLoss  2.572 ( 2.346)\tAcc@1  39.06 ( 45.41)\tAcc@5  66.41 ( 72.37)\n",
      "Test: [50/79]\tTime  0.034 ( 0.136)\tLoss  2.349 ( 2.356)\tAcc@1  44.53 ( 44.94)\tAcc@5  75.39 ( 72.42)\n",
      "Test: [60/79]\tTime  0.368 ( 0.136)\tLoss  2.356 ( 2.349)\tAcc@1  44.53 ( 45.04)\tAcc@5  76.95 ( 72.52)\n",
      "Test: [70/79]\tTime  0.037 ( 0.132)\tLoss  2.403 ( 2.354)\tAcc@1  45.70 ( 44.93)\tAcc@5  72.27 ( 72.37)\n",
      " * Acc@1 45.040 Acc@5 72.415\n",
      "Epoch: [1][  0/313]\tTime  0.816 ( 0.816)\tData  0.754 ( 0.754)\tLoss  2.129 ( 2.129)\tAcc@1  51.95 ( 51.95)\tAcc@5  79.30 ( 79.30)\n",
      "Epoch: [1][ 50/313]\tTime  0.132 ( 0.156)\tData  0.000 ( 0.041)\tLoss  2.255 ( 2.098)\tAcc@1  49.22 ( 51.13)\tAcc@5  73.83 ( 77.37)\n",
      "Epoch: [1][100/313]\tTime  0.133 ( 0.148)\tData  0.075 ( 0.032)\tLoss  2.009 ( 2.076)\tAcc@1  51.56 ( 51.25)\tAcc@5  76.17 ( 77.52)\n",
      "Epoch: [1][150/313]\tTime  0.142 ( 0.147)\tData  0.000 ( 0.032)\tLoss  1.991 ( 2.051)\tAcc@1  53.91 ( 51.76)\tAcc@5  75.78 ( 77.72)\n",
      "Epoch: [1][200/313]\tTime  0.154 ( 0.147)\tData  0.099 ( 0.033)\tLoss  1.941 ( 2.023)\tAcc@1  53.12 ( 52.22)\tAcc@5  79.30 ( 78.05)\n",
      "Epoch: [1][250/313]\tTime  0.148 ( 0.148)\tData  0.001 ( 0.034)\tLoss  2.031 ( 2.005)\tAcc@1  52.73 ( 52.54)\tAcc@5  77.34 ( 78.22)\n",
      "Epoch: [1][300/313]\tTime  0.176 ( 0.148)\tData  0.129 ( 0.034)\tLoss  1.949 ( 1.990)\tAcc@1  54.69 ( 52.74)\tAcc@5  78.12 ( 78.39)\n",
      "Test: [ 0/79]\tTime  0.757 ( 0.757)\tLoss  1.953 ( 1.953)\tAcc@1  55.08 ( 55.08)\tAcc@5  78.52 ( 78.52)\n",
      "Test: [10/79]\tTime  0.031 ( 0.166)\tLoss  1.885 ( 1.961)\tAcc@1  53.91 ( 51.46)\tAcc@5  77.73 ( 78.27)\n",
      "Test: [20/79]\tTime  0.369 ( 0.155)\tLoss  1.933 ( 1.991)\tAcc@1  53.12 ( 51.00)\tAcc@5  75.00 ( 77.10)\n",
      "Test: [30/79]\tTime  0.032 ( 0.141)\tLoss  1.999 ( 2.008)\tAcc@1  52.34 ( 51.03)\tAcc@5  78.52 ( 76.92)\n",
      "Test: [40/79]\tTime  0.341 ( 0.142)\tLoss  2.238 ( 2.008)\tAcc@1  47.66 ( 51.27)\tAcc@5  75.00 ( 76.93)\n",
      "Test: [50/79]\tTime  0.032 ( 0.137)\tLoss  1.974 ( 2.015)\tAcc@1  52.34 ( 51.05)\tAcc@5  78.91 ( 77.14)\n",
      "Test: [60/79]\tTime  0.404 ( 0.137)\tLoss  1.988 ( 2.010)\tAcc@1  51.17 ( 51.10)\tAcc@5  78.91 ( 77.15)\n",
      "Test: [70/79]\tTime  0.033 ( 0.134)\tLoss  2.046 ( 2.017)\tAcc@1  51.56 ( 51.02)\tAcc@5  77.73 ( 77.03)\n",
      " * Acc@1 51.175 Acc@5 77.065\n",
      "Epoch: [2][  0/313]\tTime  0.947 ( 0.947)\tData  0.881 ( 0.881)\tLoss  1.687 ( 1.687)\tAcc@1  59.77 ( 59.77)\tAcc@5  82.81 ( 82.81)\n",
      "Epoch: [2][ 50/313]\tTime  0.129 ( 0.168)\tData  0.002 ( 0.056)\tLoss  1.559 ( 1.561)\tAcc@1  64.06 ( 62.12)\tAcc@5  85.55 ( 85.03)\n",
      "Epoch: [2][100/313]\tTime  0.226 ( 0.159)\tData  0.174 ( 0.046)\tLoss  1.645 ( 1.547)\tAcc@1  62.11 ( 62.24)\tAcc@5  83.59 ( 85.14)\n",
      "Epoch: [2][150/313]\tTime  0.138 ( 0.155)\tData  0.005 ( 0.042)\tLoss  1.438 ( 1.543)\tAcc@1  67.58 ( 62.23)\tAcc@5  85.55 ( 85.20)\n",
      "Epoch: [2][200/313]\tTime  0.133 ( 0.151)\tData  0.045 ( 0.034)\tLoss  1.433 ( 1.539)\tAcc@1  60.16 ( 62.19)\tAcc@5  87.11 ( 85.18)\n",
      "Epoch: [2][250/313]\tTime  0.132 ( 0.149)\tData  0.004 ( 0.032)\tLoss  1.385 ( 1.531)\tAcc@1  66.80 ( 62.40)\tAcc@5  85.94 ( 85.16)\n",
      "Epoch: [2][300/313]\tTime  0.140 ( 0.147)\tData  0.079 ( 0.029)\tLoss  1.667 ( 1.536)\tAcc@1  60.16 ( 62.21)\tAcc@5  80.86 ( 85.06)\n",
      "Test: [ 0/79]\tTime  0.781 ( 0.781)\tLoss  1.871 ( 1.871)\tAcc@1  57.42 ( 57.42)\tAcc@5  78.12 ( 78.12)\n",
      "Test: [10/79]\tTime  0.033 ( 0.164)\tLoss  1.783 ( 1.868)\tAcc@1  56.64 ( 53.98)\tAcc@5  79.30 ( 79.05)\n",
      "Test: [20/79]\tTime  0.209 ( 0.146)\tLoss  1.874 ( 1.899)\tAcc@1  51.56 ( 53.12)\tAcc@5  76.95 ( 78.33)\n",
      "Test: [30/79]\tTime  0.037 ( 0.138)\tLoss  1.958 ( 1.923)\tAcc@1  53.12 ( 52.75)\tAcc@5  78.52 ( 78.07)\n",
      "Test: [40/79]\tTime  0.388 ( 0.138)\tLoss  2.058 ( 1.918)\tAcc@1  48.44 ( 52.95)\tAcc@5  75.78 ( 78.13)\n",
      "Test: [50/79]\tTime  0.034 ( 0.133)\tLoss  1.899 ( 1.924)\tAcc@1  55.08 ( 52.81)\tAcc@5  80.08 ( 78.26)\n",
      "Test: [60/79]\tTime  0.383 ( 0.133)\tLoss  1.845 ( 1.917)\tAcc@1  56.25 ( 52.99)\tAcc@5  80.86 ( 78.50)\n",
      "Test: [70/79]\tTime  0.037 ( 0.128)\tLoss  1.877 ( 1.919)\tAcc@1  54.30 ( 53.00)\tAcc@5  80.08 ( 78.48)\n",
      " * Acc@1 53.135 Acc@5 78.580\n",
      "Epoch: [3][  0/313]\tTime  0.837 ( 0.837)\tData  0.762 ( 0.762)\tLoss  1.133 ( 1.133)\tAcc@1  74.22 ( 74.22)\tAcc@5  90.23 ( 90.23)\n",
      "Epoch: [3][ 50/313]\tTime  0.135 ( 0.159)\tData  0.005 ( 0.044)\tLoss  1.170 ( 1.159)\tAcc@1  71.88 ( 72.17)\tAcc@5  90.62 ( 90.39)\n",
      "Epoch: [3][100/313]\tTime  0.240 ( 0.156)\tData  0.188 ( 0.042)\tLoss  1.001 ( 1.182)\tAcc@1  78.12 ( 71.31)\tAcc@5  91.02 ( 90.06)\n",
      "Epoch: [3][150/313]\tTime  0.126 ( 0.153)\tData  0.030 ( 0.041)\tLoss  1.066 ( 1.188)\tAcc@1  73.83 ( 70.96)\tAcc@5  93.36 ( 89.94)\n",
      "Epoch: [3][200/313]\tTime  0.130 ( 0.151)\tData  0.070 ( 0.041)\tLoss  1.133 ( 1.196)\tAcc@1  72.66 ( 70.66)\tAcc@5  90.62 ( 89.80)\n",
      "Epoch: [3][250/313]\tTime  0.145 ( 0.149)\tData  0.003 ( 0.038)\tLoss  1.273 ( 1.201)\tAcc@1  66.02 ( 70.45)\tAcc@5  86.72 ( 89.59)\n",
      "Epoch: [3][300/313]\tTime  0.119 ( 0.148)\tData  0.005 ( 0.036)\tLoss  1.138 ( 1.209)\tAcc@1  72.27 ( 70.10)\tAcc@5  89.45 ( 89.48)\n",
      "Test: [ 0/79]\tTime  0.877 ( 0.877)\tLoss  1.819 ( 1.819)\tAcc@1  59.77 ( 59.77)\tAcc@5  80.86 ( 80.86)\n",
      "Test: [10/79]\tTime  0.065 ( 0.164)\tLoss  1.810 ( 1.845)\tAcc@1  56.64 ( 54.90)\tAcc@5  78.52 ( 79.65)\n",
      "Test: [20/79]\tTime  0.319 ( 0.153)\tLoss  1.897 ( 1.866)\tAcc@1  53.91 ( 54.41)\tAcc@5  76.95 ( 78.85)\n",
      "Test: [30/79]\tTime  0.042 ( 0.141)\tLoss  1.918 ( 1.894)\tAcc@1  54.69 ( 53.87)\tAcc@5  80.47 ( 78.44)\n",
      "Test: [40/79]\tTime  0.101 ( 0.136)\tLoss  2.012 ( 1.896)\tAcc@1  52.34 ( 53.92)\tAcc@5  76.56 ( 78.45)\n",
      "Test: [50/79]\tTime  0.263 ( 0.133)\tLoss  1.782 ( 1.895)\tAcc@1  56.64 ( 53.92)\tAcc@5  80.86 ( 78.75)\n",
      "Test: [60/79]\tTime  0.032 ( 0.130)\tLoss  1.899 ( 1.889)\tAcc@1  49.22 ( 53.78)\tAcc@5  81.25 ( 79.04)\n",
      "Test: [70/79]\tTime  0.333 ( 0.131)\tLoss  1.956 ( 1.889)\tAcc@1  52.73 ( 53.86)\tAcc@5  78.91 ( 78.97)\n",
      " * Acc@1 53.930 Acc@5 79.040\n",
      "Accuracy of FP32 model: 53.930\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0tH9KdwtHhV"
   },
   "source": [
    "# Create and initialize quantization of the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pt_xNDDrJKsy",
    "outputId": "0925c801-0585-4431-98c9-de0decc4ad27"
   },
   "source": [
    "nncf_config_dict = {\n",
    "    \"input_info\": {\n",
    "        \"sample_size\": [1, 3, image_size, image_size]\n",
    "    },\n",
    "    \"compression\": {\n",
    "        \"algorithm\": \"quantization\",  # specify the algorithm here\n",
    "    }\n",
    "}\n",
    "# Load a configuration file to specify compression\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n",
    "# Provide data loaders for compression algorithm initialization, if necessary\n",
    "nncf_config = register_default_init_args(nncf_config, train_loader)\n",
    "\n",
    "compression_ctrl, model = create_compressed_model(model, nncf_config)\n",
    "\n",
    "# evaluate on validation set after initialization of quantization\n",
    "acc1 = validate(val_loader, model, criterion)\n",
    "print(f'Accuracy of initialized INT8 model: {acc1:.3f}')\n"
   ],
   "execution_count": 46,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:nncf:Please, provide execution parameters for optimal model initialization\n",
      "WARNING:nncf:Graphviz is not installed - only the .dot model visualization format will be used. Install pygraphviz into your Python environment and graphviz system-wide to enable PNG rendering.\n",
      "INFO:nncf:Wrapping module ResNet/Conv2d[conv1] by ResNet/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv1] by ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv2] by ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv1] by ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv2] by ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv1] by ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv2] by ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/Conv2d[0] by ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv1] by ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv2] by ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv1] by ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv2] by ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/Conv2d[0] by ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv1] by ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv2] by ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv1] by ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv2] by ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/Conv2d[0] by ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv1] by ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv2] by ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Linear[fc] by ResNet/NNCFLinear[fc]\n",
      "WARNING:nncf:Enabling quantization range initialization with default parameters.\n",
      "WARNING:nncf:NNCFNetwork(\n",
      "  (nncf_module): ResNet(\n",
      "    (conv1): NNCFConv2d(\n",
      "      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "      (pre_ops): ModuleDict()\n",
      "      (post_ops): ModuleDict()\n",
      "    )\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): NNCFConv2d(\n",
      "            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (pre_ops): ModuleDict()\n",
      "            (post_ops): ModuleDict()\n",
      "          )\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): NNCFConv2d(\n",
      "            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (pre_ops): ModuleDict()\n",
      "            (post_ops): ModuleDict()\n",
      "          )\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): NNCFConv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (pre_ops): ModuleDict()\n",
      "            (post_ops): ModuleDict()\n",
      "          )\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): NNCFLinear(\n",
      "      in_features=512, out_features=200, bias=True\n",
      "      (pre_ops): ModuleDict()\n",
      "      (post_ops): ModuleDict()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "INFO:nncf:Collecting tensor statistics ████████████████  | 1 / 1\n",
      "INFO:nncf:Set sign: True and scale: [2.6400, ] for TargetType.OPERATOR_POST_HOOK /nncf_model_input_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK /nncf_model_input_0\n",
      "INFO:nncf:Set sign: False and scale: [2.4446, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [3.5122, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [2.6191, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [6.1023, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [2.3259, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [4.4591, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: True and scale: [3.2266, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [2.8361, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [3.5787, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [2.6001, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [3.6699, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: True and scale: [1.4854, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [2.3495, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [5.1378, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [1.9740, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [3.7819, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: True and scale: [1.9265, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [2.0858, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [11.7168, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [13.5531, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [12.6463, ] for TargetType.OPERATOR_POST_HOOK ResNet/AdaptiveAvgPool2d[avgpool]/adaptive_avg_pool2d_0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/AdaptiveAvgPool2d[avgpool]/adaptive_avg_pool2d_0\n",
      "INFO:nncf:Set sign: False and scale: [3.8070, ] for TargetType.OPERATOR_POST_HOOK ResNet/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: False and scale: [4.5497, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [6.6620, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [5.8350, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [5.8199, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [3.9538, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [4.4879, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [4.1007, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "WARNING:nncf:The saturation issue fix will be applied. Now all weight quantizers will effectively use only 7 bits out of 8 bits. This resolves the saturation issue problem on AVX2 and AVX-512 machines. Please take a look at the documentation for a detailed information.\n",
      "INFO:nncf:Set sign: True and scale: [0.7715, 0.3202, 0.1000, 0.1694, 0.1000, 0.4187, 0.5818, 0.1000, 0.7550, 0.1000, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.8024, 0.2467, 0.5588, 0.3642, 0.3538, 0.3231, 0.3718, 0.1829, 0.4615, 0.4258, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.3296, 0.1514, 0.1000, 0.2359, 0.1334, 0.2363, 0.2472, 0.1812, 0.2780, 0.2597, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2308, 0.4309, 0.2273, 0.1677, 0.3031, 0.3174, 0.1968, 0.6534, 0.3465, 0.3622, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2000, 0.3185, 0.1862, 0.1865, 0.1713, 0.3466, 0.2899, 0.2909, 0.2819, 0.2150, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1750, 0.1541, 0.2139, 0.2145, 0.1859, 0.2401, 0.1229, 0.2474, 0.1381, 0.1733, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2308, 0.1469, 0.2544, 0.1084, 0.2259, 0.1972, 0.1824, 0.1725, 0.1802, 0.1596, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.4994, 0.1144, 0.1714, 0.3215, 0.1703, 0.1495, 0.1684, 0.3959, 0.1661, 0.3872, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2626, 0.1481, 0.1619, 0.1723, 0.2041, 0.2112, 0.1188, 0.2597, 0.2241, 0.2945, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1387, 0.1000, 0.1598, 0.1821, 0.1466, 0.1288, 0.1335, 0.1306, 0.1105, 0.1739, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1693, 0.2864, 0.1298, 0.1744, 0.1082, 0.1421, 0.1201, 0.1212, 0.1250, 0.1148, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1269, 0.1127, 0.1000, 0.1795, 0.1296, 0.1970, 0.1611, 0.1042, 0.1608, 0.1242, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1216, 0.1132, 0.1000, 0.1586, 0.1057, 0.1378, 0.1000, 0.1000, 0.1000, 0.1000, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1046, 0.2031, 0.1154, 0.2019, 0.1000, 0.1031, 0.2288, 0.1528, 0.1000, 0.1275, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1284, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1083, 0.1000, 0.1156, 0.1000, 0.1000, 0.1000, 0.1000, 0.1332, 0.1000, 0.1370, ... (first 10/512 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1051, 0.1028, 0.1000, 0.1000, 0.1008, 0.1000, 0.1390, 0.1074, 0.1000, 0.1300, ... (first 10/512 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1293, 0.1027, 0.2737, 0.1000, 0.1109, 0.1385, 0.2113, 0.1000, 0.1000, ... (first 10/512 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1003, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, ... (first 10/512 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1305, 0.1000, 0.1000, ... (first 10/512 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, ... (first 10/200 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/NNCFLinear[fc]/linear_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/NNCFLinear[fc]/linear_0\n",
      "INFO:nncf:BatchNorm statistics adaptation ██                | 1 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ████              | 2 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ██████            | 3 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ████████          | 4 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ██████████        | 5 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ████████████      | 6 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ██████████████    | 7 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ████████████████  | 8 / 8\n",
      "WARNING:nncf:Graphviz is not installed - only the .dot model visualization format will be used. Install pygraphviz into your Python environment and graphviz system-wide to enable PNG rendering.\n",
      "Test: [ 0/79]\tTime  1.001 ( 1.001)\tLoss  1.764 ( 1.764)\tAcc@1  60.16 ( 60.16)\tAcc@5  79.30 ( 79.30)\n",
      "Test: [10/79]\tTime  0.121 ( 0.265)\tLoss  1.821 ( 1.841)\tAcc@1  54.69 ( 54.44)\tAcc@5  77.34 ( 79.62)\n",
      "Test: [20/79]\tTime  0.187 ( 0.220)\tLoss  1.937 ( 1.867)\tAcc@1  53.52 ( 53.92)\tAcc@5  76.56 ( 78.98)\n",
      "Test: [30/79]\tTime  0.161 ( 0.202)\tLoss  1.907 ( 1.896)\tAcc@1  54.30 ( 53.60)\tAcc@5  78.12 ( 78.54)\n",
      "Test: [40/79]\tTime  0.222 ( 0.198)\tLoss  2.035 ( 1.896)\tAcc@1  50.00 ( 53.78)\tAcc@5  76.56 ( 78.43)\n",
      "Test: [50/79]\tTime  0.171 ( 0.195)\tLoss  1.825 ( 1.896)\tAcc@1  57.42 ( 53.87)\tAcc@5  80.86 ( 78.74)\n",
      "Test: [60/79]\tTime  0.134 ( 0.193)\tLoss  1.910 ( 1.887)\tAcc@1  51.95 ( 53.87)\tAcc@5  79.69 ( 79.05)\n",
      "Test: [70/79]\tTime  0.168 ( 0.191)\tLoss  1.977 ( 1.891)\tAcc@1  52.34 ( 53.94)\tAcc@5  77.73 ( 78.99)\n",
      " * Acc@1 53.975 Acc@5 79.125\n",
      "Accuracy of initialized INT8 model: 53.975\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d8LOmKut36x"
   },
   "source": [
    "# Fine-tune the model - Quantization-Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qk0Nub3KiqZK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a438341a-8211-4750-be34-440ae483a511"
   },
   "source": [
    "# train for one epoch with NNCF\n",
    "train(train_loader, model, criterion, optimizer, epoch=epochs)\n",
    "\n",
    "# evaluate on validation set after Quantization-Aware Training (QAT case)\n",
    "acc1 = validate(val_loader, model, criterion)\n",
    "\n",
    "print(f'Accuracy of tuned INT8 model: {acc1:.3f}')"
   ],
   "execution_count": 47,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch: [4][  0/313]\tTime  1.240 ( 1.240)\tData  0.901 ( 0.901)\tLoss  0.849 ( 0.849)\tAcc@1  80.08 ( 80.08)\tAcc@5  94.14 ( 94.14)\n",
      "Epoch: [4][ 50/313]\tTime  0.234 ( 0.246)\tData  0.001 ( 0.023)\tLoss  0.879 ( 0.905)\tAcc@1  78.12 ( 78.23)\tAcc@5  92.97 ( 93.60)\n",
      "Epoch: [4][100/313]\tTime  0.202 ( 0.230)\tData  0.008 ( 0.015)\tLoss  0.922 ( 0.914)\tAcc@1  77.34 ( 78.01)\tAcc@5  93.75 ( 93.24)\n",
      "Epoch: [4][150/313]\tTime  0.208 ( 0.224)\tData  0.000 ( 0.012)\tLoss  1.070 ( 0.926)\tAcc@1  71.48 ( 77.72)\tAcc@5  90.62 ( 93.00)\n",
      "Epoch: [4][200/313]\tTime  0.207 ( 0.220)\tData  0.000 ( 0.010)\tLoss  0.870 ( 0.926)\tAcc@1  78.52 ( 77.56)\tAcc@5  93.75 ( 92.96)\n",
      "Epoch: [4][250/313]\tTime  0.205 ( 0.218)\tData  0.007 ( 0.009)\tLoss  0.990 ( 0.930)\tAcc@1  76.95 ( 77.24)\tAcc@5  92.19 ( 92.89)\n",
      "Epoch: [4][300/313]\tTime  0.226 ( 0.218)\tData  0.001 ( 0.009)\tLoss  1.003 ( 0.935)\tAcc@1  76.17 ( 76.99)\tAcc@5  92.19 ( 92.84)\n",
      "Test: [ 0/79]\tTime  1.148 ( 1.148)\tLoss  1.815 ( 1.815)\tAcc@1  58.98 ( 58.98)\tAcc@5  80.47 ( 80.47)\n",
      "Test: [10/79]\tTime  0.185 ( 0.296)\tLoss  1.789 ( 1.843)\tAcc@1  54.30 ( 55.08)\tAcc@5  78.52 ( 79.76)\n",
      "Test: [20/79]\tTime  0.235 ( 0.246)\tLoss  1.855 ( 1.883)\tAcc@1  54.69 ( 54.30)\tAcc@5  78.91 ( 79.07)\n",
      "Test: [30/79]\tTime  0.217 ( 0.226)\tLoss  1.941 ( 1.907)\tAcc@1  55.86 ( 54.21)\tAcc@5  78.52 ( 78.57)\n",
      "Test: [40/79]\tTime  0.226 ( 0.216)\tLoss  2.000 ( 1.905)\tAcc@1  52.73 ( 54.39)\tAcc@5  79.69 ( 78.77)\n",
      "Test: [50/79]\tTime  0.220 ( 0.210)\tLoss  1.824 ( 1.904)\tAcc@1  57.81 ( 54.35)\tAcc@5  78.91 ( 78.92)\n",
      "Test: [60/79]\tTime  0.117 ( 0.203)\tLoss  1.919 ( 1.901)\tAcc@1  57.81 ( 54.40)\tAcc@5  79.69 ( 79.12)\n",
      "Test: [70/79]\tTime  0.130 ( 0.202)\tLoss  1.911 ( 1.902)\tAcc@1  54.30 ( 54.37)\tAcc@5  78.12 ( 79.08)\n",
      " * Acc@1 54.430 Acc@5 79.135\n",
      "Accuracy of tuned INT8 model: 54.430\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pobVoHEoKcYp"
   },
   "source": [
    "# Export model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ENAbqFpdWSlE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cd2701e3-e4a2-4a19-86cd-ae37f45cd64a"
   },
   "source": [
    "# Export to ONNX that is supported by the OpenVINO™ toolkit\n",
    "compression_ctrl.export_model(\"model_compressed.onnx\")"
   ],
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/onnx/utils.py:305: UserWarning: It is recommended that constant folding be turned off ('do_constant_folding=False') when exporting the model in training-amenable mode, i.e. with 'training=TrainingMode.TRAIN' or 'training=TrainingMode.PRESERVE' (when model is in training mode). Otherwise, some learnable model parameters may not translate correctly in the exported ONNX model because constant folding mutates model parameters. Please consider turning off constant folding or setting the training=TrainingMode.EVAL.\n",
      "  warnings.warn(\"It is recommended that constant folding be turned off ('do_constant_folding=False') \"\n",
      "/usr/local/lib/python3.7/dist-packages/nncf/torch/dynamic_graph/trace_tensor.py:36: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self.shape = tuple(int(dim) for dim in shape)  # Handle cases when shape is a tuple of Tensors\n",
      "/usr/local/lib/python3.7/dist-packages/nncf/torch/quantization/layers.py:148: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.is_enabled_quantization():\n",
      "/usr/local/lib/python3.7/dist-packages/nncf/torch/quantization/layers.py:204: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return self._num_bits.item()\n",
      "/usr/local/lib/python3.7/dist-packages/nncf/torch/quantization/layers.py:414: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return self.signed_tensor.item() == 1\n",
      "/usr/local/lib/python3.7/dist-packages/nncf/torch/layers.py:100: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self.get_padding_value_ref().data.fill_(padding_value.item())\n",
      "/usr/local/lib/python3.7/dist-packages/nncf/torch/layers.py:106: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.get_padding_value_ref():\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I_G-g9TPWkl"
   },
   "source": [
    "#Export to OpenVINO™ Intermediate Representation (IR)\n",
    "To export a model to the OpenVINO IR and run it using the Intel® Deep Learning Deployment Toolkit, refer to this [tutorial](https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html)."
   ]
  }
 ]
}